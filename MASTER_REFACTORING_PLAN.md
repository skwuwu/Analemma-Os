# Analemma OS ì¢…í•© ë¦¬íŒ©í† ë§ ë§ˆìŠ¤í„°í”Œëœ
## "Fat State â†’ Merkle DAG + Lean Manifest" ì „í™˜

> **ëª©í‘œ:** í˜„ëŒ€ì  ë¶„ì‚° OSì˜ ì •ì„ì„ ë”°ë¥´ëŠ” ê°€ë²„ë„ŒìŠ¤ ëŸ°íƒ€ì„ êµ¬ì¶•  
> **ê¸°ê°„:** 8-12ì£¼ (Phase 0-7)  
> **ì˜ˆìƒ íš¨ê³¼:** í˜ì´ë¡œë“œ 67% ì ˆê°, ë°ì´í„° ì¤‘ë³µ 90% â†’ 10%, íšŒê·€ ì†ë„ ì¦‰ì‹œ ì „í™˜

---

## ğŸ“Š í˜„ì¬ ìƒíƒœ ì§„ë‹¨

### ì•„í‚¤í…ì²˜ ë¬¸ì œì 
```
í˜„ì¬ (V2 - Fat State):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Initialize Lambda                                   â”‚
â”‚ â”œâ”€ workflow_config: 200KB (ì „ì²´ ê·¸ë˜í”„)            â”‚
â”‚ â”œâ”€ partition_map: 50KB (ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸)             â”‚
â”‚ â””â”€ StateBagì— ì˜êµ¬ ì €ì¥ âŒ                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì „ë‹¬
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Execute Segment 0                                   â”‚
â”‚ â”œâ”€ workflow_config: 200KB â† ë¶ˆí•„ìš”                 â”‚
â”‚ â”œâ”€ partition_map: 50KB â† ë¶ˆí•„ìš”                    â”‚
â”‚ â”œâ”€ current_state: 100KB                            â”‚
â”‚ â””â”€ segment_config: ë™ì  ìƒì„± (ëŠë¦¼)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ 100ê°œ ì„¸ê·¸ë¨¼íŠ¸ = 37MB ë‚­ë¹„

ë¬¸ì œ 1: ì´ˆê¸°í™” ë°ì´í„°ì™€ ëŸ°íƒ€ì„ ìƒíƒœì˜ í˜¼ì¬
ë¬¸ì œ 2: ì„¸ê·¸ë¨¼íŠ¸ê°€ ì „ì²´ ì›Œí¬í”Œë¡œìš° êµ¬ì¡°ë¥¼ ì•Œ ìˆ˜ ìˆìŒ (ë³´ì•ˆ ìœ„ë°˜)
ë¬¸ì œ 3: ìƒíƒœ ë³€ê²½ ì‹œ ì „ì²´ ë³µì‚¬ (ë°ì´í„° ì¤‘ë³µ 90%)
ë¬¸ì œ 4: íšŒê·€(Rollback) ì‹œ ì •í™•í•œ ì‹œì  ì¬í˜„ ë¶ˆê°€
ë¬¸ì œ 5: S3 256KB ì œí•œ ìš°íšŒ ë¶ˆê°€ëŠ¥
```

### ëª©í‘œ ì•„í‚¤í…ì²˜
```
ëª©í‘œ (V3 - Merkle DAG + Lean Manifest):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Initialize Lambda                                   â”‚
â”‚ â”œâ”€ workflow_config: 200KB â†’ S3 (ì°¸ì¡°ìš©)            â”‚
â”‚ â”‚  â””â”€ config_hash: sha256(...) â†’ Manifest Root     â”‚
â”‚ â”œâ”€ partition_map: ë¡œì»¬ ë³€ìˆ˜ (íê¸°)                 â”‚
â”‚ â”œâ”€ segment_manifest: S3 ì €ì¥                       â”‚
â”‚ â”‚  â””â”€ manifest_id: uuid â†’ DynamoDB Pointer         â”‚
â”‚ â””â”€ StateBag: manifest_id + hashë§Œ ì €ì¥ âœ…          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ í¬ì¸í„°ë§Œ ì „ë‹¬ (100 bytes)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Execute Segment 0                                   â”‚
â”‚ â”œâ”€ segment_config: ASL ì§ì ‘ ì£¼ì… (10KB)            â”‚
â”‚ â”‚  â””â”€ ë˜ëŠ” manifest[0] Lazy Load                   â”‚
â”‚ â”œâ”€ current_state: S3 Select (í•„ìš”í•œ í•„ë“œë§Œ)        â”‚
â”‚ â””â”€ manifest_hash: ê²€ì¦ í›„ ì‹¤í–‰ âœ…                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ 100ê°œ ì„¸ê·¸ë¨¼íŠ¸ = 13MB (-65%)

í•´ê²° 1: ì´ˆê¸°í™” ë°ì´í„°ëŠ” S3 ì°¸ì¡°, ëŸ°íƒ€ì„ì€ í¬ì¸í„°ë§Œ
í•´ê²° 2: ì„¸ê·¸ë¨¼íŠ¸ëŠ” ìì‹ ì˜ configë§Œ ì•Œ ìˆ˜ ìˆìŒ (ìµœì†Œ ê¶Œí•œ)
í•´ê²° 3: Merkle DAGë¡œ ë¸íƒ€ë§Œ ì €ì¥ (10% ì¤‘ë³µ)
í•´ê²° 4: Pointer Manifestë¡œ ì¦‰ì‹œ íšŒê·€ ê°€ëŠ¥
í•´ê²° 5: S3 Selectë¡œ í•„ë“œë³„ ì„ íƒì  ë¡œë“œ
```

---

## ğŸ¯ Phase 0: ì‚¬ì „ ì¤€ë¹„ (Week 1, P0 - Critical)

> **í•µì‹¬:** ê¸°ì¡´ ì‹œìŠ¤í…œì„ ê¹¨ì§€ ì•Šê³  ìƒˆë¡œìš´ ë¡œë”© ë©”ì»¤ë‹ˆì¦˜ ë¨¼ì € ë°°í¬  
> **ìˆœì„œê°€ ì¤‘ìš”:** Fallback ë¡œì§ ë°°í¬ â†’ ë°ì´í„° ì œê±° â†’ ASL ìµœì í™”

### 0.1 Fallback ë¡œë”© ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„

**íŒŒì¼:** [segment_runner_service.py](c:\\Users\\gimgy\\OneDrive\\ë°”íƒ•%20í™”ë©´\\Analemma-Os\\analemma-workflow-os\\backend\\src\\services\\execution\\segment_runner_service.py)

```python
def _load_segment_config_from_manifest(
    self, 
    manifest_s3_path: str, 
    segment_index: int,
    cache_ttl: int = 300  # 5ë¶„ ìºì‹œ
) -> dict:
    """
    S3ì—ì„œ segment_manifestë¥¼ ë¡œë“œí•˜ê³  íŠ¹ì • segment_configë¥¼ ì¶”ì¶œ
    
    ìƒˆ ê¸°ëŠ¥:
    - Size-based routing: ì‘ì€ manifestëŠ” ì „ì²´ ë¡œë“œ, í° ê²ƒì€ S3 Select
    - In-memory cache: ê°™ì€ manifest ì¬ì‚¬ìš©
    - Checksum verification: manifest_hash ê²€ì¦
    """
    cache_key = f"{manifest_s3_path}:{segment_index}"
    
    # 1. ìºì‹œ í™•ì¸ (Lambda warm start ì‹œ ì¬ì‚¬ìš©)
    if hasattr(self, '_manifest_cache'):
        cached = self._manifest_cache.get(cache_key)
        if cached and time.time() - cached['timestamp'] < cache_ttl:
            logger.info(f"Cache hit for segment_config: {cache_key}")
            return cached['config']
    
    # 2. S3 ê²½ë¡œ íŒŒì‹±
    bucket_name = manifest_s3_path.replace("s3://", "").split("/")[0]
    key_name = "/".join(manifest_s3_path.replace("s3://", "").split("/")[1:])
    
    # 3. Size-based routing (í”¼ë“œë°± ë°˜ì˜)
    s3 = boto3.client('s3')
    head_obj = s3.head_object(Bucket=bucket_name, Key=key_name)
    object_size = head_obj['ContentLength']
    
    if object_size < 10 * 1024:  # 10KB ë¯¸ë§Œ
        # ì „ì²´ ë¡œë“œê°€ ë” íš¨ìœ¨ì 
        logger.info(f"Small manifest ({object_size}B), using GetObject")
        obj = s3.get_object(Bucket=bucket_name, Key=key_name)
        content = obj['Body'].read().decode('utf-8')
        manifest = self._safe_json_load(content)
    else:
        # S3 Selectë¡œ íŠ¹ì • ì„¸ê·¸ë¨¼íŠ¸ë§Œ ì¶”ì¶œ
        logger.info(f"Large manifest ({object_size}B), using S3 Select")
        response = s3.select_object_content(
            Bucket=bucket_name,
            Key=key_name,
            ExpressionType='SQL',
            Expression=f"SELECT * FROM s3object[*][{segment_index}]",
            InputSerialization={'JSON': {'Type': 'DOCUMENT'}},
            OutputSerialization={'JSON': {}}
        )
        # S3 Select ì‘ë‹µ íŒŒì‹±
        result = []
        for event in response['Payload']:
            if 'Records' in event:
                result.append(event['Records']['Payload'].decode('utf-8'))
        segment_entry = json.loads(''.join(result))
    
    # 4. segment_config ì¶”ì¶œ
    if object_size < 10 * 1024:
        if not isinstance(manifest, list):
            raise ValueError(f"Invalid manifest: expected list, got {type(manifest)}")
        if not (0 <= segment_index < len(manifest)):
            raise ValueError(f"Index {segment_index} out of range (manifest has {len(manifest)} segments)")
        segment_entry = manifest[segment_index]
    
    # 5. Nested êµ¬ì¡° ì²˜ë¦¬
    if 'segment_config' in segment_entry:
        segment_config = segment_entry['segment_config']
    else:
        segment_config = segment_entry
    
    # 6. ìºì‹œ ì €ì¥
    if not hasattr(self, '_manifest_cache'):
        self._manifest_cache = {}
    self._manifest_cache[cache_key] = {
        'config': segment_config,
        'timestamp': time.time()
    }
    
    logger.info(f"Loaded segment_config: type={segment_config.get('type')}, "
               f"nodes={len(segment_config.get('nodes', []))}")
    
    return segment_config
```

**ë°°í¬ ìš°ì„ ìˆœìœ„:** âš ï¸ **CRITICAL - Phase 0ì˜ ìµœìš°ì„  ì‘ì—…**  
ì´ ë©”ì„œë“œë¥¼ ë¨¼ì € ë°°í¬í•˜ë©´ ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ë˜ë©´ì„œ ìƒˆ ê²½ë¡œë„ ì§€ì›

**ì„±ëŠ¥ ìµœì í™” (í”¼ë“œë°± ë°˜ì˜):**
- **Lambda ìºì‹±ì´ ì‹¤ì œ ì£¼ ê²½ë¡œ**: ASL Direct Injectionì€ 256KB ì œì•½ìœ¼ë¡œ ì „ì²´ íŠ¸ë˜í”½ì˜ 20% ë¯¸ë§Œë§Œ ì²˜ë¦¬
- **Warm Start ìµœì í™”**: `_manifest_cache`ë¥¼ Lambda ì¸ìŠ¤í„´ìŠ¤ ë ˆë²¨ì—ì„œ ìœ ì§€í•˜ì—¬ ì¬ì‚¬ìš©
- **ì˜ˆìƒ ìºì‹œ íˆíŠ¸ìœ¨**: 80% ì´ìƒ (ê°™ì€ ì›Œí¬í”Œë¡œìš°ì˜ ì—°ì† ì„¸ê·¸ë¨¼íŠ¸ ì‹¤í–‰ ì‹œ)

```python
# Lambda ì´ˆê¸°í™” ì‹œ ìºì‹œ í¬ê¸° ì œí•œ ì„¤ì •
if not hasattr(self, '_manifest_cache'):
    self._manifest_cache = {}  # LRUë¡œ êµì²´ ê¶Œì¥ (ìµœëŒ€ 100ê°œ í•­ëª©)
```

---

### 0.2 Hybrid Loading ë¡œì§ (ASL 256KB ëŒ€ì‘)

**íŒŒì¼:** [segment_runner_service.py](c:\\Users\\gimgy\\OneDrive\\ë°”íƒ•%20í™”ë©´\\Analemma-Os\\analemma-workflow-os\\backend\\src\\services\\execution\\segment_runner_service.py) Line 2856-2919

```python
# âœ… Hybrid Loading: ASL ì§ì ‘ ì£¼ì… ë˜ëŠ” Fallback
segment_config = event.get('segment_config')  # ASLì—ì„œ ì£¼ì… (ì‘ì€ manifest)

if not segment_config:
    # Fallback: Lambdaê°€ S3ì—ì„œ ì§ì ‘ ë¡œë“œ (í° manifest)
    manifest_s3_path = event.get('segment_manifest_s3_path')
    segment_index = event.get('segment_index', segment_id)
    
    if manifest_s3_path:
        segment_config = self._load_segment_config_from_manifest(
            manifest_s3_path,
            segment_index
        )
    else:
        # Legacy fallback: workflow_config + partition_map (í˜¸í™˜ì„±)
        workflow_config = _safe_get_from_bag(event, 'workflow_config')
        partition_map = _safe_get_from_bag(event, 'partition_map')
        
        if workflow_config or partition_map:
            logger.warning("[Legacy Mode] Using workflow_config/partition_map fallback")
            segment_config = self._resolve_segment_config(
                workflow_config, partition_map, segment_id
            )
        else:
            raise ValueError("No segment_config source available")

# workflow_configì™€ partition_mapì€ ë” ì´ìƒ ì§ì ‘ ì‚¬ìš© ì•ˆ í•¨
# (statebagì—ì„œ ì œê±° ì˜ˆì •)
```

**ë°°í¬ íš¨ê³¼:** 3ë‹¨ê³„ Fallbackìœ¼ë¡œ ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ëŠ¥

---

## ğŸ¯ Phase 1: Merkle DAG ì¸í”„ë¼ êµ¬ì¶• (Week 2-3, P0)

### 1.1 DynamoDB í…Œì´ë¸” ìƒì„±

**í…Œì´ë¸”:** `WorkflowManifestsV3`

```python
{
    "TableName": "WorkflowManifestsV3",
    "KeySchema": [
        {"AttributeName": "manifest_id", "KeyType": "HASH"},
        {"AttributeName": "version", "KeyType": "RANGE"}
    ],
    "AttributeDefinitions": [
        {"AttributeName": "manifest_id", "AttributeType": "S"},
        {"AttributeName": "version", "AttributeType": "N"},
        {"AttributeName": "workflow_id", "AttributeType": "S"},
        {"AttributeName": "parent_hash", "AttributeType": "S"}
    ],
    "GlobalSecondaryIndexes": [
        {
            "IndexName": "WorkflowIndex",
            "KeySchema": [
                {"AttributeName": "workflow_id", "KeyType": "HASH"},
                {"AttributeName": "version", "KeyType": "RANGE"}
            ],
            "Projection": {"ProjectionType": "ALL"}
        },
        {
            "IndexName": "ParentHashIndex",
            "KeySchema": [
                {"AttributeName": "parent_hash", "KeyType": "HASH"}
            ],
            "Projection": {"ProjectionType": "KEYS_ONLY"}
        }
    ],
    "StreamSpecification": {
        "StreamEnabled": true,
        "StreamViewType": "NEW_AND_OLD_IMAGES"
    }
}
```

**í•­ëª© êµ¬ì¡°:**
```python
{
    "manifest_id": "uuid",
    "version": 1,
    "workflow_id": "workflow_123",
    "parent_hash": "sha256(...)",  # ì´ì „ ë²„ì „ì˜ í•´ì‹œ (Merkle ì²´ì¸)
    "manifest_hash": "sha256(...)",  # í˜„ì¬ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ì˜ í•´ì‹œ
    "config_hash": "sha256(...)",    # workflow_configì˜ í•´ì‹œ (ë¶ˆë³€ ì°¸ì¡°)
    "s3_pointers": {
        "manifest": "s3://bucket/manifests/uuid.json",
        "config": "s3://bucket/configs/workflow_123.json",  # ì°¸ì¡°ìš©
        "state_blocks": [
            "s3://bucket/states/block_abc.json",  # ë¸íƒ€ ë¸”ë¡
            "s3://bucket/states/block_def.json"
        ]
    },
    "metadata": {
        "created_at": "2026-02-18T10:00:00Z",
        "segment_count": 10,
        "total_size": 150000,
        "compression": "gzip"
    },
    "ttl": 1708678800  # 30ì¼ í›„ ì‚­ì œ (GCìš©)
}
```

---

### 1.2 StateVersioningService êµ¬í˜„

**íŒŒì¼:** `backend/src/services/state/state_versioning_service.py` (NEW)

```python
import hashlib
import json
from typing import Dict, List, Optional
from dataclasses import dataclass
import boto3

@dataclass
class ContentBlock:
    """Merkle DAGì˜ ì»¨í…ì¸  ë¸”ë¡"""
    block_id: str  # sha256 í•´ì‹œ
    s3_path: str
    size: int
    fields: List[str]  # ì´ ë¸”ë¡ì— í¬í•¨ëœ í•„ë“œ ëª©ë¡
    checksum: str

@dataclass
class ManifestPointer:
    """Pointer Manifest êµ¬ì¡°"""
    manifest_id: str
    version: int
    parent_hash: Optional[str]
    manifest_hash: str
    config_hash: str  # workflow_config ê²€ì¦ìš©
    blocks: List[ContentBlock]
    metadata: Dict

class StateVersioningService:
    """
    Merkle DAG ê¸°ë°˜ ìƒíƒœ ë²„ì €ë‹ ì„œë¹„ìŠ¤
    
    í•µì‹¬ ê¸°ëŠ¥:
    1. ìƒíƒœ ë³€ê²½ ì‹œ ë¸íƒ€ë§Œ ì €ì¥ (Content-Addressable Storage)
    2. Merkle Rootë¡œ ë¬´ê²°ì„± ê²€ì¦
    3. Pointer Manifestë¡œ ì¦‰ì‹œ íšŒê·€ ê°€ëŠ¥
    """
    
    def __init__(self, dynamodb_table: str, s3_bucket: str):
        self.dynamodb = boto3.resource('dynamodb')
        self.table = self.dynamodb.Table(dynamodb_table)
        self.s3 = boto3.client('s3')
        self.bucket = s3_bucket
    
    def create_manifest(
        self,
        workflow_id: str,
        workflow_config: dict,
        segment_manifest: List[dict],
        parent_manifest_id: Optional[str] = None
    ) -> ManifestPointer:
        """
        ìƒˆ Pointer Manifest ìƒì„±
        
        Args:
            workflow_id: ì›Œí¬í”Œë¡œìš° ID
            workflow_config: ì›Œí¬í”Œë¡œìš° ì„¤ì • (í•´ì‹œ ê³„ì‚°ìš©)
            segment_manifest: ì„¸ê·¸ë¨¼íŠ¸ ëª©ë¡
            parent_manifest_id: ì´ì „ ë²„ì „ ID (Merkle ì²´ì¸)
        
        Returns:
            ManifestPointer: ìƒì„±ëœ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ í¬ì¸í„°
        """
        import uuid
        
        manifest_id = str(uuid.uuid4())
        
        # 1. workflow_config í•´ì‹œ ê³„ì‚° (ë¶ˆë³€ ì°¸ì¡°)
        config_hash = self._compute_hash(workflow_config)
        
        # 2. workflow_configë¥¼ S3ì— ì €ì¥ (ì°¸ì¡°ìš©)
        config_s3_key = f"workflow-configs/{workflow_id}/{config_hash}.json"
        self.s3.put_object(
            Bucket=self.bucket,
            Key=config_s3_key,
            Body=json.dumps(workflow_config, default=str),
            ContentType='application/json',
            Metadata={
                'usage': 'reference_only',
                'workflow_id': workflow_id,
                'config_hash': config_hash
            }
        )
        
        # 3. segment_manifestë¥¼ Content Blocksë¡œ ë¶„í• 
        blocks = self._split_into_blocks(segment_manifest)
        
        # 3.5. Pre-computed Hash ìƒì„± (Phase 7 ê²€ì¦ ìµœì í™”ìš©)
        segment_hashes = self._compute_segment_hashes(segment_manifest)
        
        # 4. ê° ë¸”ë¡ì„ S3ì— ì €ì¥ (Content-Addressable)
        for block in blocks:
            if not self._block_exists(block.block_id):
                self.s3.put_object(
                    Bucket=self.bucket,
                    Key=block.s3_path.replace(f"s3://{self.bucket}/", ""),
                    Body=json.dumps({
                        'fields': block.fields,
                        'data': segment_manifest  # ì‹¤ì œë¡œëŠ” í•´ë‹¹ í•„ë“œë§Œ
                    }),
                    ContentType='application/json',
                    Metadata={'block_id': block.block_id}
                )
        
        # 5. Merkle Root ê³„ì‚°
        parent_hash = None
        if parent_manifest_id:
            parent = self.get_manifest(parent_manifest_id)
            parent_hash = parent.manifest_hash
        
        manifest_hash = self._compute_merkle_root(blocks, config_hash, parent_hash)
        
        # 6. DynamoDBì— í¬ì¸í„° ì €ì¥
        version = self._get_next_version(workflow_id)
        
        self.table.put_item(Item={
            'manifest_id': manifest_id,
            'version': version,
            'workflow_id': workflow_id,
            'parent_hash': parent_hash,
            'manifest_hash': manifest_hash,
            'config_hash': config_hash,
            'segment_hashes': segment_hashes,  # âœ… Pre-computed Hash ì €ì¥
            's3_pointers': {
                'manifest': f"s3://{self.bucket}/manifests/{manifest_id}.json",
                'config': f"s3://{self.bucket}/{config_s3_key}",
                'state_blocks': [block.s3_path for block in blocks]
            },
            'metadata': {
                'created_at': datetime.utcnow().isoformat(),
                'segment_count': len(segment_manifest),
                'total_size': sum(block.size for block in blocks),
                'compression': 'none'
            },
            'ttl': int(time.time()) + 30 * 24 * 3600  # 30ì¼ í›„ GC
        })
        
        return ManifestPointer(
            manifest_id=manifest_id,
            version=version,
            parent_hash=parent_hash,
            manifest_hash=manifest_hash,
            config_hash=config_hash,
            blocks=blocks,
            metadata={}
        )
    
    def verify_manifest_integrity(self, manifest_id: str) -> bool:
        """
        Merkle Root ê²€ì¦
        
        Returns:
            bool: ë¬´ê²°ì„± ê²€ì¦ í†µê³¼ ì—¬ë¶€
        """
        item = self.table.get_item(Key={'manifest_id': manifest_id})['Item']
        
        # ì €ì¥ëœ ë¸”ë¡ë“¤ë¡œ Merkle Root ì¬ê³„ì‚°
        blocks = self._load_blocks(item['s3_pointers']['state_blocks'])
        computed_hash = self._compute_merkle_root(
            blocks,
            item['config_hash'],
            item.get('parent_hash')
        )
        
        is_valid = computed_hash == item['manifest_hash']
        
        if not is_valid:
            logger.error(f"[Integrity Violation] Manifest {manifest_id} hash mismatch! "
                        f"Expected: {item['manifest_hash']}, "
                        f"Computed: {computed_hash}")
        
        return is_valid
    
    def _compute_hash(self, data: dict) -> str:
        """JSON ë°ì´í„°ì˜ SHA256 í•´ì‹œ ê³„ì‚°"""
        json_str = json.dumps(data, sort_keys=True, default=str)
        return hashlib.sha256(json_str.encode()).hexdigest()
    
    def _compute_merkle_root(
        self,
        blocks: List[ContentBlock],
        config_hash: str,
        parent_hash: Optional[str]
    ) -> str:
        """
        Merkle Root ê³„ì‚°
        
        êµ¬ì¡°:
        root_hash = sha256(
            config_hash +
            parent_hash +
            sha256(block1.checksum + block2.checksum + ...)
        )
        """
        blocks_hash = hashlib.sha256(
            ''.join(b.checksum for b in sorted(blocks, key=lambda x: x.block_id)).encode()
        ).hexdigest()
        
        combined = config_hash + (parent_hash or '') + blocks_hash
        return hashlib.sha256(combined.encode()).hexdigest()
    
    def _split_into_blocks(self, manifest: List[dict]) -> List[ContentBlock]:
        """
        segment_manifestë¥¼ Content Blocksë¡œ ë¶„í• 
        
        ì „ëµ: ê° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë³„ë„ ë¸”ë¡ìœ¼ë¡œ
        """
        blocks = []
        for idx, segment in enumerate(manifest):
            block_data = json.dumps(segment, default=str)
            block_id = hashlib.sha256(block_data.encode()).hexdigest()
            
            blocks.append(ContentBlock(
                block_id=block_id,
                s3_path=f"s3://{self.bucket}/state-blocks/{block_id}.json",
                size=len(block_data.encode()),
                fields=[f"segment_{idx}"],
                checksum=block_id
            ))
        
        return blocks
    
    def _block_exists(self, block_id: str) -> bool:
        """ë¸”ë¡ì´ S3ì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (ì¤‘ë³µ ì œê±°)"""
        try:
            self.s3.head_object(
                Bucket=self.bucket,
                Key=f"state-blocks/{block_id}.json"
            )
            return True
        except:
            return False
    
    def _compute_segment_hashes(self, manifest: List[dict]) -> Dict[int, str]:
        """
        ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ ê°œë³„ í•´ì‹œ ë¯¸ë¦¬ ê³„ì‚° (Phase 7 ìµœì í™”ìš©)
        
        í”¼ë“œë°±:
        - ë§¤ ì„¸ê·¸ë¨¼íŠ¸ë§ˆë‹¤ partition_workflow() ì¬ì‹¤í–‰ì€ ë„ˆë¬´ ë¬´ê±°ì›€
        - Pre-computed Hashë¡œ O(n) â†’ O(1) ê²€ì¦
        
        Returns:
            Dict[segment_index, hash]: ì„¸ê·¸ë¨¼íŠ¸ë³„ í•´ì‹œê°’
        """
        segment_hashes = {}
        
        for idx, segment in enumerate(manifest):
            # segment_configë§Œ ì¶”ì¶œí•˜ì—¬ í•´ì‹œ ê³„ì‚°
            segment_config = segment.get('segment_config', segment)
            segment_hash = self._compute_hash(segment_config)
            segment_hashes[idx] = segment_hash
            
            logger.debug(f"Pre-computed hash for segment {idx}: {segment_hash[:8]}...")
        
        return segment_hashes
```

---

## ğŸ¯ Phase 2: workflow_config/partition_map ì œê±° (Week 4, P0)

### 2.1 initialize_state_data.py ìˆ˜ì •

**íŒŒì¼:** [initialize_state_data.py](c:\\Users\\gimgy\\OneDrive\\ë°”íƒ•%20í™”ë©´\\Analemma-Os\\analemma-workflow-os\\backend\\src\\common\\initialize_state_data.py) Line 421-423

**Before:**
```python
bag['workflow_config'] = workflow_config  # âŒ
bag['partition_map'] = partition_map      # âŒ
```

**After:**
```python
# StateVersioningServiceë¥¼ í†µí•´ Merkle Manifest ìƒì„±
versioning_service = StateVersioningService(
    dynamodb_table=os.environ['MANIFESTS_TABLE'],
    s3_bucket=bucket
)

manifest_pointer = versioning_service.create_manifest(
    workflow_id=workflow_id,
    workflow_config=workflow_config,  # í•´ì‹œ ê³„ì‚° í›„ S3 ì €ì¥
    segment_manifest=segment_manifest,
    parent_manifest_id=None  # ì²« ì‹¤í–‰
)

# StateBagì—ëŠ” í¬ì¸í„°ë§Œ ì €ì¥
bag['manifest_id'] = manifest_pointer.manifest_id
bag['manifest_hash'] = manifest_pointer.manifest_hash
bag['config_hash'] = manifest_pointer.config_hash  # ê²€ì¦ìš©

# âŒ ì œê±°
# bag['workflow_config'] = workflow_config
# bag['partition_map'] = partition_map

logger.info(f"Created Merkle Manifest: {manifest_pointer.manifest_id}, "
           f"hash={manifest_pointer.manifest_hash[:8]}..., "
           f"blocks={len(manifest_pointer.blocks)}")
```

**ë³´ì•ˆ ê°•í™” (í”¼ë“œë°± ë°˜ì˜):**
```python
# workflow_config í•´ì‹œë¥¼ statebagì— ì €ì¥í•˜ì—¬
# ì‹¤í–‰ ì¤‘ì¸ segment_configê°€ ì›ë³¸ ì„¤ê³„ë„ì—ì„œ ìœ ë˜í–ˆìŒì„ ë³´ì¥
bag['config_hash'] = manifest_pointer.config_hash

# ê° ì„¸ê·¸ë¨¼íŠ¸ ì‹¤í–‰ ì‹œ ê²€ì¦:
# if segment_config_hash != bag['config_hash']:
#     raise SecurityError("Segment config does not match original workflow!")
```

---

### 2.2 ASL ìˆ˜ì • - Threshold-based Loading

**íŒŒì¼:** [aws_step_functions_v3.json](c:\\Users\\gimgy\\OneDrive\\ë°”íƒ•%20í™”ë©´\\Analemma-Os\\analemma-workflow-os\\backend\\src\\aws_step_functions_v3.json)

```json
{
  "Comment": "Analemma OS V3 - Merkle DAG + Lean Manifest",
  "StartAt": "CheckManifestSize",
  
  "States": {
    "CheckManifestSize": {
      "Type": "Choice",
      "Choices": [
        {
          "Variable": "$.manifest_size",
          "NumericLessThan": 256000,
          "Comment": "256KB ë¯¸ë§Œ: ASLì—ì„œ ì§ì ‘ ì²˜ë¦¬",
          "Next": "ExecuteWithDirectInjection"
        }
      ],
      "Default": "ExecuteWithS3Loading"
    },
    
    "ExecuteWithDirectInjection": {
      "Comment": "ì‘ì€ manifest: ASLì—ì„œ segment_config ì§ì ‘ ì£¼ì…",
      "Type": "Task",
      "Resource": "arn:aws:lambda:...:function:ExecuteSegment",
      "Parameters": {
        "state_data.$": "$.state_data",
        "segment_index.$": "$.segment_index",
        
        "segment_config.$": "States.JsonToString($.segment_manifest[$.segment_index].segment_config)",
        
        "manifest_hash.$": "$.manifest_hash",
        "config_hash.$": "$.config_hash"
      },
      "Next": "CheckCompletion"
    },
    
    "ExecuteWithS3Loading": {
      "Comment": "í° manifest: Lambdaê°€ S3ì—ì„œ ì§ì ‘ ë¡œë“œ",
      "Type": "Task",
      "Resource": "arn:aws:lambda:...:function:ExecuteSegment",
      "Parameters": {
        "state_data.$": "$.state_data",
        "segment_index.$": "$.segment_index",
        
        "segment_manifest_s3_path.$": "$.segment_manifest_s3_path",
        "manifest_id.$": "$.manifest_id",
        
        "manifest_hash.$": "$.manifest_hash",
        "config_hash.$": "$.config_hash"
      },
      "Next": "CheckCompletion"
    }
  }
}
```

**í”¼ë“œë°± ë°˜ì˜:**
- 256KB ê¸°ì¤€ìœ¼ë¡œ Direct Injection vs S3 Loading ì„ íƒ
- manifest_hashì™€ config_hashë¥¼ ëª¨ë“  ì„¸ê·¸ë¨¼íŠ¸ì— ì „ë‹¬í•˜ì—¬ ë¬´ê²°ì„± ê²€ì¦

**âš ï¸ ì‹¤ë¬´ ê²½ê³  (ASLì˜ í•¨ì •):**
- ASLì˜ `States.ArrayGetItem`ê³¼ `States.JsonToString` ì¡°í•©ì€ ë¬¸ë²•ì´ ê¹Œë‹¤ë¡œì›€
- ë™ì  ì¸ë±ìŠ¤(`$.segment_index`) ì‚¬ìš© ì‹œ ì‹¤ìˆ˜ ë¹ˆë²ˆ
- **ì‹¤ì œ ìš´ì˜ ì˜ˆìƒ**: Direct Injectionì€ ì „ì²´ì˜ 20% ë¯¸ë§Œ, ë‚˜ë¨¸ì§€ 80%ëŠ” Lambda Fallback ì²˜ë¦¬
- **ì „ëµ**: ASLì€ "ë¹ ë¥¸ ê²½ë¡œ(Fast Path)"ë¡œë§Œ ì‚¬ìš©, LambdaëŠ” "ì•ˆì • ê²½ë¡œ(Stable Path)"

```json
// ASL ì¸íŠ¸ë¦°ì§ í•¨ìˆ˜ ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­
// âŒ ì‘ë™ ì•ˆ í•¨: "segment_config.$": "$.segment_manifest[$.segment_index]"
// âœ… ì˜¬ë°”ë¥¸ ë°©ë²•: "segment_config.$": "States.ArrayGetItem($.segment_manifest, $.segment_index)"
```

---

## ğŸ¯ Phase 3: SegmentFieldOptimizer í†µí•© (Week 5, P1)

### 3.1 Capability-based Filtering ì¶”ê°€ (í”¼ë“œë°± ë°˜ì˜)

**íŒŒì¼:** `backend/src/services/execution/segment_field_optimizer.py`

**Before (ì •ì  í•„í„°ë§):**
```python
NODE_REQUIRED_FIELDS = {
    "llm_chat": ["node", "config"],
    "parallel_group": ["branches", "node"],
    # ...
}
```

**After (ë™ì  í•„í„°ë§):**
```python
class SegmentFieldOptimizer:
    """
    ì„¸ê·¸ë¨¼íŠ¸ í˜ì´ë¡œë“œ ìµœì í™”
    
    ìƒˆ ê¸°ëŠ¥:
    - Capability-based Filtering: ë…¸ë“œì˜ ì˜ë„(Intent)ì— ë”°ë¼ í•„ë“œ ë™ì  í•´ì œ
    - Security Ring í†µí•©: Ring 3 ë…¸ë“œëŠ” ë” ë§ì€ í•„ë“œ ì œí•œ
    """
    
    ALWAYS_EXCLUDE_FIELDS = [
        'workflow_config',   # Phase 2ì—ì„œ ì œê±°ë¨
        'partition_map',     # Phase 2ì—ì„œ ì œê±°ë¨
        'debug_info',
        'internal_cache'
    ]
    
    # ê¸°ë³¸ í•„ìˆ˜ í•„ë“œ
    BASE_REQUIRED_FIELDS = {
        "llm_chat": ["node", "config"],
        "parallel_group": ["branches", "node"],
        "aggregator": ["branch_results"],
        "trigger": ["event_config"],
        "code_interpreter": ["code", "node"],
        "http_request": ["request_config", "node"],
    }
    
    # Capability ê¸°ë°˜ ì¶”ê°€ í•„ë“œ (ììœ¨í˜• ì—ì´ì „íŠ¸ìš©)
    CAPABILITY_FIELDS = {
        "tool_use": ["tools", "tool_schemas"],  # ë„êµ¬ ì‚¬ìš© ì‹œ í•„ìš”
        "memory_access": ["memory_context"],     # ë©”ëª¨ë¦¬ ì ‘ê·¼ ì‹œ í•„ìš”
        "state_mutation": ["state_schema"],      # ìƒíƒœ ë³€ê²½ ì‹œ í•„ìš”
    }
    
    # Security Ringë³„ ì œì•½
    RING_RESTRICTIONS = {
        "ring_0": [],  # ì œì•½ ì—†ìŒ (ì‹ ë¢°ëœ ì½”ë“œ)
        "ring_1": ["internal_state"],
        "ring_2": ["internal_state", "credentials"],
        "ring_3": ["internal_state", "credentials", "workflow_metadata"]  # ìµœì†Œ ê¶Œí•œ
    }
    
    def filter_event_payload(
        self,
        event: dict,
        segment_config: dict,
        security_ring: str = "ring_3"  # ê¸°ë³¸ê°’: ìµœì†Œ ê¶Œí•œ
    ) -> dict:
        """
        ë™ì  í•„ë“œ í•„í„°ë§
        
        Args:
            event: ì›ë³¸ ì´ë²¤íŠ¸
            segment_config: ì„¸ê·¸ë¨¼íŠ¸ ì„¤ì •
            security_ring: ë³´ì•ˆ ë§ ë ˆë²¨
        
        Returns:
            dict: ìµœì í™”ëœ ì´ë²¤íŠ¸
        """
        nodes = segment_config.get('nodes', [])
        if not nodes:
            return event
        
        # 1. ë…¸ë“œ íƒ€ì…ë³„ í•„ìˆ˜ í•„ë“œ
        required_fields = set()
        for node in nodes:
            node_type = node.get('type')
            base_fields = self.BASE_REQUIRED_FIELDS.get(node_type, [])
            required_fields.update(base_fields)
            
            # 2. Capability ê¸°ë°˜ ì¶”ê°€ í•„ë“œ
            node_capabilities = node.get('capabilities', [])
            for cap in node_capabilities:
                cap_fields = self.CAPABILITY_FIELDS.get(cap, [])
                required_fields.update(cap_fields)
        
        # 3. Security Ring ì œì•½ ì ìš©
        restricted_fields = set(self.RING_RESTRICTIONS.get(security_ring, []))
        
        # 4. í•„í„°ë§
        filtered = {}
        for key, value in event.items():
            # í•­ìƒ ì œì™¸
            if key in self.ALWAYS_EXCLUDE_FIELDS:
                continue
            
            # Ring ì œì•½ìœ¼ë¡œ ì œì™¸
            if key in restricted_fields:
                logger.info(f"Field '{key}' excluded by {security_ring} restriction")
                continue
            
            # í•„ìˆ˜ í•„ë“œ ë˜ëŠ” ë©”íƒ€ë°ì´í„°ëŠ” í¬í•¨
            if key in required_fields or key.startswith('_'):
                filtered[key] = value
        
        # 5. ë¡œê¹…
        original_size = len(json.dumps(event, default=str))
        filtered_size = len(json.dumps(filtered, default=str))
        reduction = (1 - filtered_size / original_size) * 100
        
        logger.info(f"Payload optimized: {original_size}B â†’ {filtered_size}B "
                   f"(-{reduction:.1f}%), ring={security_ring}")
        
        return filtered
```

**ì‚¬ìš© ì˜ˆì‹œ:**
```python
# segment_runner_service.pyì—ì„œ
optimizer = SegmentFieldOptimizer()

# ììœ¨í˜• ì—ì´ì „íŠ¸ (Manus): Ring 3 + tool_use capability
filtered_event = optimizer.filter_event_payload(
    event,
    segment_config,
    security_ring="ring_3"
)

# ì‹ ë¢°ëœ ì‹œìŠ¤í…œ ë…¸ë“œ: Ring 0
filtered_event = optimizer.filter_event_payload(
    event,
    segment_config,
    security_ring="ring_0"
)
```

---

## ğŸ¯ Phase 4: S3 Select ìµœì í™” (Week 6, P1)

### 4.1 StateHydrator ê°œì„  - Size-based Routing

**íŒŒì¼:** [state_hydrator.py](c:\\Users\\gimgy\\OneDrive\\ë°”íƒ•%20í™”ë©´\\Analemma-Os\\analemma-workflow-os\\backend\\src\\services\\state\\state_hydrator.py)

```python
class StateHydrator:
    """
    ìƒíƒœ ì§ë ¬í™”/ì—­ì§ë ¬í™” with S3 Select ìµœì í™”
    
    ìƒˆ ê¸°ëŠ¥:
    - CloudWatch ê¸°ë°˜ ë™ì  threshold íŠœë‹ (ë ˆì´í„´ì‹œ ì§€í„° ëŒ€ì‘)
    """
    
    # âš ï¸ ì´ˆê¸°ê°’: ìš´ì˜ ì¤‘ CloudWatch Metric ê¸°ë°˜ìœ¼ë¡œ ì¡°ì •
    FIELD_SIZE_THRESHOLD = 10 * 1024  # 10KB (ë™ì  ì¡°ì • ê°€ëŠ¥)
    S3_SELECT_OVERHEAD = 100  # ms (ì‹¤ì œ ì¸¡ì •ê°’ìœ¼ë¡œ êµì²´ í•„ìš”)
    GET_OBJECT_OVERHEAD = 50  # ms (ì‹¤ì œ ì¸¡ì •ê°’ìœ¼ë¡œ êµì²´ í•„ìš”)
    
    # CloudWatch ê¸°ë°˜ ë™ì  ìµœì í™”
    _threshold_cache = None
    _threshold_last_update = 0
    THRESHOLD_UPDATE_INTERVAL = 3600  # 1ì‹œê°„ë§ˆë‹¤ ê°±ì‹ 
    
    def load_fields_selective(
        self,
        s3_path: str,
        field_names: List[str],
        auto_routing: bool = True
    ) -> dict:
        """
        S3 Selectë¥¼ ì´ìš©í•œ ì„ íƒì  í•„ë“œ ë¡œë”©
        
        ìƒˆ ê¸°ëŠ¥:
        - Size-based routing: ì‘ì€ ê°ì²´ëŠ” GetObject, í° ê²ƒì€ Select
        - Cost optimization: í•„ë“œ í¬ê¸° ê¸°ë°˜ ìµœì  ê²½ë¡œ ì„ íƒ
        
        Args:
            s3_path: S3 ê²½ë¡œ
            field_names: ë¡œë“œí•  í•„ë“œ ëª©ë¡
            auto_routing: ìë™ ê²½ë¡œ ì„ íƒ í™œì„±í™”
        
        Returns:
            dict: ë¡œë“œëœ í•„ë“œ
        """
        bucket, key = self._parse_s3_path(s3_path)
        
        # 1. ê°ì²´ í¬ê¸° í™•ì¸
        head = self.s3_client.head_object(Bucket=bucket, Key=key)
        object_size = head['ContentLength']
        
        # 2. Size-based routing (í”¼ë“œë°± ë°˜ì˜)
        if auto_routing:
            use_select = self._should_use_select(
                object_size,
                len(field_names),
                field_names
            )
        else:
            use_select = object_size >= self.FIELD_SIZE_THRESHOLD
        
        # 3. ë¡œë”©
        if not use_select:
            # GetObject: ì „ì²´ ë¡œë“œ í›„ í•„í„°ë§
            logger.info(f"Using GetObject for {object_size}B object")
            obj = self.s3_client.get_object(Bucket=bucket, Key=key)
            data = json.loads(obj['Body'].read().decode('utf-8'))
            return {k: v for k, v in data.items() if k in field_names}
        else:
            # S3 Select: SQL ì¿¼ë¦¬ë¡œ í•„ë“œ ì„ íƒ
            logger.info(f"Using S3 Select for {object_size}B object, "
                       f"fields={field_names}")
            
            sql_fields = ', '.join(f's.{f}' for f in field_names)
            expression = f"SELECT {sql_fields} FROM s3object s"
            
            response = self.s3_client.select_object_content(
                Bucket=bucket,
                Key=key,
                ExpressionType='SQL',
                Expression=expression,
                InputSerialization={'JSON': {'Type': 'DOCUMENT'}},
                OutputSerialization={'JSON': {}}
            )
            
            # ì‘ë‹µ íŒŒì‹±
            result = []
            for event in response['Payload']:
                if 'Records' in event:
                    result.append(event['Records']['Payload'].decode('utf-8'))
            
            return json.loads(''.join(result))
    
    def _get_dynamic_threshold(self) -> int:
        """
        CloudWatch ê¸°ë°˜ ë™ì  threshold (ë ˆì´í„´ì‹œ ì§€í„° ëŒ€ì‘)
        
        í”¼ë“œë°± ë°˜ì˜:
        - S3 Selectì˜ ì¿¼ë¦¬ íŒŒì‹± ì˜¤ë²„í—¤ë“œëŠ” ë°ì´í„° ì–‘ì— ë”°ë¼ ë³€ë™
        - ì‹¤ì œ ë ˆì´í„´ì‹œë¥¼ ì¸¡ì •í•˜ì—¬ threshold ìë™ ì¡°ì •
        """
        import time
        
        # ìºì‹œ í™•ì¸ (1ì‹œê°„ ìœ íš¨)
        if (self._threshold_cache and 
            time.time() - self._threshold_last_update < self.THRESHOLD_UPDATE_INTERVAL):
            return self._threshold_cache
        
        try:
            # CloudWatchì—ì„œ ìµœê·¼ 1ì¼ê°„ ë ˆì´í„´ì‹œ ë©”íŠ¸ë¦­ ì¡°íšŒ
            cloudwatch = boto3.client('cloudwatch')
            
            # S3 Select í‰ê·  ë ˆì´í„´ì‹œ
            select_latency = cloudwatch.get_metric_statistics(
                Namespace='Analemma/StateHydrator',
                MetricName='S3SelectLatency',
                StartTime=datetime.utcnow() - timedelta(days=1),
                EndTime=datetime.utcnow(),
                Period=3600,
                Statistics=['Average']
            )
            
            # GetObject í‰ê·  ë ˆì´í„´ì‹œ
            get_latency = cloudwatch.get_metric_statistics(
                Namespace='Analemma/StateHydrator',
                MetricName='GetObjectLatency',
                StartTime=datetime.utcnow() - timedelta(days=1),
                EndTime=datetime.utcnow(),
                Period=3600,
                Statistics=['Average']
            )
            
            # ë ˆì´í„´ì‹œ ë¹„êµí•˜ì—¬ ìµœì  threshold ê³„ì‚°
            if select_latency['Datapoints'] and get_latency['Datapoints']:
                select_avg = select_latency['Datapoints'][0]['Average']
                get_avg = get_latency['Datapoints'][0]['Average']
                
                # Selectê°€ Getë³´ë‹¤ 2ë°° ì´ìƒ ëŠë¦¬ë©´ threshold ì¦ê°€
                if select_avg > get_avg * 2:
                    new_threshold = 50 * 1024  # 50KBë¡œ ì¦ê°€
                    logger.warning(f"S3 Select latency high, increasing threshold to {new_threshold}B")
                else:
                    new_threshold = 10 * 1024  # ê¸°ë³¸ê°’ ìœ ì§€
                
                self._threshold_cache = new_threshold
                self._threshold_last_update = time.time()
                return new_threshold
        
        except Exception as e:
            logger.warning(f"Failed to get dynamic threshold from CloudWatch: {e}")
        
        # Fallback: ê¸°ë³¸ê°’
        return self.FIELD_SIZE_THRESHOLD
    
    def _should_use_select(
        self,
        object_size: int,
        field_count: int,
        field_names: List[str]
    ) -> bool:
        """
        S3 Select vs GetObject ë¹„ìš©/ì„±ëŠ¥ ë¹„êµ
        
        Decision Tree:
        - ê°ì²´ < dynamic_threshold: GetObject (ë ˆì´í„´ì‹œ ì§€í„° ëŒ€ì‘)
        - í•„ë“œ > 80%: GetObject (ëŒ€ë¶€ë¶„ í•„ìš”í•˜ë©´ ì „ì²´ ë¡œë“œê°€ íš¨ìœ¨ì )
        - í•„ë“œ < 20%: S3 Select (ëŒ€ë¶€ë¶„ ë¶ˆí•„ìš”í•˜ë©´ ì„ íƒì  ë¡œë“œ)
        - ë‚˜ë¨¸ì§€: ê°ì²´ í¬ê¸° ê¸°ë°˜ (> 50KBë©´ Select)
        """
        # ë™ì  threshold ì‚¬ìš©
        dynamic_threshold = self._get_dynamic_threshold()
        
        if object_size < dynamic_threshold:
            return False  # ì‘ì€ ê°ì²´ëŠ” GetObject
        
        # ì „ì²´ í•„ë“œ ìˆ˜ ì¶”ì • (ë©”íƒ€ë°ì´í„°ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’)
        total_fields = head.get('Metadata', {}).get('field_count', 10)
        field_ratio = field_count / total_fields
        
        if field_ratio > 0.8:
            return False  # ëŒ€ë¶€ë¶„ í•„ìš”í•˜ë©´ GetObject
        
        if field_ratio < 0.2:
            return True  # ì¼ë¶€ë§Œ í•„ìš”í•˜ë©´ Select
        
        # ì¤‘ê°„ ì˜ì—­: í¬ê¸° ê¸°ë°˜
        return object_size >= 50 * 1024  # 50KB ì´ìƒ
```

---

## ğŸ¯ Phase 5: ë¹„ë™ê¸° ì»¤ë°‹ + Redis ìºì‹œ (Week 7-8, P2)

### 5.1 Read-After-Write Consistency ë³´ì¥ (í”¼ë“œë°± ë°˜ì˜)

**íŒŒì¼:** `backend/src/services/state/async_state_checkpointer.py` (NEW)

```python
class AsyncStateCheckpointer:
    """
    ë¹„ë™ê¸° ìƒíƒœ ì²´í¬í¬ì¸íŠ¸ with Read-After-Write ì¼ê´€ì„± ë³´ì¥
    
    í•µì‹¬:
    - ë™ê¸°ì‹ ë²„ì „ í† í° ë°œí–‰
    - ë¹„ë™ê¸° S3/DynamoDB ê¸°ë¡
    - ì½ê¸° ì‹œ ë²„ì „ í† í° ëŒ€ê¸°
    """
    
    def __init__(self, sns_topic_arn: str, redis_host: str):
        self.sns = boto3.client('sns')
        self.topic_arn = sns_topic_arn
        self.redis = redis.Redis(host=redis_host, decode_responses=True)
    
    def checkpoint_async(
        self,
        manifest_id: str,
        state_delta: dict,
        wait_for_commit: bool = False
    ) -> str:
        """
        ë¹„ë™ê¸° ì²´í¬í¬ì¸íŠ¸ with ë²„ì „ í† í°
        
        Args:
            manifest_id: í˜„ì¬ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ID
            state_delta: ë³€ê²½ëœ ìƒíƒœ
            wait_for_commit: ì»¤ë°‹ ì™„ë£Œê¹Œì§€ ëŒ€ê¸° (HITL ì „ìš©)
        
        Returns:
            str: ë²„ì „ í† í° (manifest_id:version)
        """
        # 1. ë²„ì „ í† í° ë°œí–‰ (ë™ê¸°)
        version = self._get_next_version(manifest_id)
        version_token = f"{manifest_id}:{version}"
        
        # 2. Redisì— "pending" ìƒíƒœ ê¸°ë¡ (ë™ê¸°)
        self.redis.setex(
            f"version:{version_token}",
            300,  # 5ë¶„ TTL
            "pending"
        )
        
        # 3. SNSë¡œ ë¹„ë™ê¸° ì»¤ë°‹ ìš”ì²­
        self.sns.publish(
            TopicArn=self.topic_arn,
            Message=json.dumps({
                'version_token': version_token,
                'manifest_id': manifest_id,
                'state_delta': state_delta,
                'timestamp': datetime.utcnow().isoformat()
            })
        )
        
        logger.info(f"Async checkpoint initiated: {version_token}")
        
        # 4. ëŒ€ê¸° ëª¨ë“œ (HITL ì „ìš©)
        if wait_for_commit:
            self._wait_for_commit(version_token, timeout=30)
        
        return version_token
    
    def load_state_with_consistency(
        self,
        version_token: str,
        timeout: int = 10
    ) -> dict:
        """
        ë²„ì „ í† í°ì„ ê¸°ë‹¤ë¦¬ë©° ìƒíƒœ ë¡œë“œ (Read-After-Write ì¼ê´€ì„±)
        
        í”¼ë“œë°± ë°˜ì˜:
        - TOCTOU ë¦¬ìŠ¤í¬ ì™„í™”: S3 Eventual Consistency ëŒ€ì‘
        - Exponential Backoff ì¬ì‹œë„ ì¶”ê°€
        
        Args:
            version_token: ê¸°ë‹¤ë¦´ ë²„ì „ (ì˜ˆ: "uuid:5")
            timeout: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        
        Returns:
            dict: ë¡œë“œëœ ìƒíƒœ
        """
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            # Redisì—ì„œ ë²„ì „ ìƒíƒœ í™•ì¸
            status = self.redis.get(f"version:{version_token}")
            
            if status == "committed":
                # ì»¤ë°‹ ì™„ë£Œ: S3ì—ì„œ ë¡œë“œ (TOCTOU ëŒ€ì‘)
                logger.info(f"Version {version_token} committed, loading from S3")
                return self._load_from_s3_with_retry(version_token)
            
            elif status == "pending":
                # ì•„ì§ ì»¤ë°‹ ì¤‘: ëŒ€ê¸°
                logger.debug(f"Version {version_token} pending, waiting...")
                time.sleep(0.1)
            
            else:
                # Redisì— ì—†ìŒ: ì•„ì§ ë°œí–‰ë˜ì§€ ì•Šì•˜ê±°ë‚˜ TTL ì´ˆê³¼
                raise ConsistencyError(f"Version {version_token} not found")
        
        # Timeout
        raise TimeoutError(f"Version {version_token} not committed within {timeout}s")
    
    def _load_from_s3_with_retry(
        self,
        version_token: str,
        max_retries: int = 5,
        base_delay: float = 0.1
    ) -> dict:
        """
        S3ì—ì„œ ìƒíƒœ ë¡œë“œ with Exponential Backoff (TOCTOU ì™„í™”)
        
        í”¼ë“œë°±:
        - Redis ìƒíƒœê°€ 'committed'ë¡œ ë°”ë€ ì§í›„ S3 ê°ì²´ê°€ ì•„ì§ ê°€ìš©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
        - S3 Eventual Consistencyë¡œ ì¸í•œ ì§§ì€ ê°­ ì¡´ì¬
        
        Args:
            version_token: ë²„ì „ í† í°
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            base_delay: ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        
        Returns:
            dict: ë¡œë“œëœ ìƒíƒœ
        """
        manifest_id, version = version_token.split(':')
        s3_key = f"manifests/{manifest_id}/v{version}.json"
        
        for attempt in range(max_retries):
            try:
                obj = self.s3.get_object(
                    Bucket=self.bucket,
                    Key=s3_key
                )
                data = json.loads(obj['Body'].read().decode('utf-8'))
                logger.info(f"Successfully loaded state from S3: {version_token}")
                return data
            
            except self.s3.exceptions.NoSuchKey:
                # S3 Eventual Consistency ëŒ€ê¸°
                delay = base_delay * (2 ** attempt)  # Exponential backoff
                logger.warning(
                    f"S3 object not yet available (attempt {attempt+1}/{max_retries}), "
                    f"retrying in {delay}s... (TOCTOU gap)"
                )
                time.sleep(delay)
            
            except Exception as e:
                logger.error(f"Unexpected error loading from S3: {e}")
                raise
        
        # ì¬ì‹œë„ ì‹¤íŒ¨
        raise ConsistencyError(
            f"S3 object not available after {max_retries} retries. "
            f"TOCTOU gap exceeded expected window. version_token={version_token}"
        )
    
    def _wait_for_commit(self, version_token: str, timeout: int):
        """ì»¤ë°‹ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            status = self.redis.get(f"version:{version_token}")
            if status == "committed":
                logger.info(f"Commit completed: {version_token}")
                return
            time.sleep(0.5)
        
        raise TimeoutError(f"Commit timeout: {version_token}")
```

**Lambda Handler (ë¹„ë™ê¸° ì»¤ë°‹ ì›Œì»¤):**
```python
def async_commit_handler(event, context):
    """
    SNSì—ì„œ íŠ¸ë¦¬ê±°ë˜ëŠ” ë¹„ë™ê¸° ì»¤ë°‹ Lambda
    """
    for record in event['Records']:
        message = json.loads(record['Sns']['Message'])
        version_token = message['version_token']
        
        try:
            # 1. S3/DynamoDBì— ê¸°ë¡
            versioning_service.commit_checkpoint(
                manifest_id=message['manifest_id'],
                state_delta=message['state_delta']
            )
            
            # 2. Redis ìƒíƒœ ì—…ë°ì´íŠ¸
            redis_client.setex(
                f"version:{version_token}",
                300,
                "committed"
            )
            
            logger.info(f"Async commit completed: {version_token}")
            
        except Exception as e:
            logger.error(f"Async commit failed: {version_token}, error={e}")
            redis_client.setex(
                f"version:{version_token}",
                300,
                f"failed:{str(e)}"
            )
```

**ì‚¬ìš© ì „ëµ (í”¼ë“œë°± ë°˜ì˜):**
```python
# segment_runner_service.py

# 1. ììœ¨í˜• ë£¨í”„ ë‚´ë¶€: ë™ê¸° ì»¤ë°‹ (ì¼ê´€ì„± ë³´ì¥)
if execution_context == "autonomous_loop":
    # ë™ê¸°ì‹: ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ê°€ ì¦‰ì‹œ ì½ì„ ìˆ˜ ìˆì–´ì•¼ í•¨
    versioning_service.commit_checkpoint_sync(
        manifest_id=manifest_id,
        state_delta=state_delta
    )

# 2. HITL ëŒ€ê¸° ì „: ë¹„ë™ê¸° ì»¤ë°‹ + ëŒ€ê¸°
elif execution_context == "before_hitl":
    # ë¹„ë™ê¸° ì»¤ë°‹í•˜ë˜, ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
    version_token = checkpointer.checkpoint_async(
        manifest_id=manifest_id,
        state_delta=state_delta,
        wait_for_commit=True  # HITL ì „ì—ëŠ” ë°˜ë“œì‹œ ì»¤ë°‹ ì™„ë£Œ
    )
    
    # HITL ì¬ê°œ ì‹œ ë²„ì „ í† í° ì „ë‹¬
    task_token_metadata['version_token'] = version_token

# 3. ë£¨í”„ ì™„ë£Œ í›„: ì™„ì „ ë¹„ë™ê¸° (Snapshot)
elif execution_context == "loop_completed":
    # ì™„ì „ ë¹„ë™ê¸°: ì¬ê°œ ì‹œ ëŒ€ê¸°í•˜ë©´ ë¨
    version_token = checkpointer.checkpoint_async(
        manifest_id=manifest_id,
        state_delta=state_delta,
        wait_for_commit=False
    )
```

---

## ğŸ¯ Phase 6: Garbage Collection (Week 9, P2)

### 6.1 S3 Lifecycle Policy + DynamoDB TTL

**íŒŒì¼:** `infrastructure/s3_lifecycle_policy.json` (NEW)

```json
{
  "Rules": [
    {
      "Id": "ArchiveOldStateBlocks",
      "Status": "Enabled",
      "Filter": {
        "Prefix": "state-blocks/"
      },
      "Transitions": [
        {
          "Days": 30,
          "StorageClass": "GLACIER"
        }
      ],
      "Expiration": {
        "Days": 90
      }
    },
    {
      "Id": "DeleteOldConfigs",
      "Status": "Enabled",
      "Filter": {
        "Prefix": "workflow-configs/"
      },
      "Expiration": {
        "Days": 365
      }
    }
  ]
}
```

**DynamoDB TTL (ì´ë¯¸ ì„¤ì •ë¨):**
```python
# WorkflowManifestsV3 í…Œì´ë¸”
{
    "ttl": int(time.time()) + 30 * 24 * 3600  # 30ì¼ í›„ ìë™ ì‚­ì œ
}
```

### 6.2 Garbage Collector Lambda

**íŒŒì¼:** `backend/src/handlers/garbage_collector.py` (NEW)

```python
def gc_handler(event, context):
    """
    ì¼ì • ê¸°ê°„ ì°¸ì¡°ë˜ì§€ ì•Šì€ ë¸”ë¡ ì •ë¦¬
    
    íŠ¸ë¦¬ê±°: CloudWatch Events (ë§¤ì¼ ìƒˆë²½ 2ì‹œ)
    """
    dynamodb = boto3.resource('dynamodb')
    table = dynamodb.Table(os.environ['MANIFESTS_TABLE'])
    s3 = boto3.client('s3')
    bucket = os.environ['STATE_BUCKET']
    
    # 1. ì°¸ì¡°ë˜ëŠ” ëª¨ë“  ë¸”ë¡ ID ìˆ˜ì§‘
    referenced_blocks = set()
    
    scan_kwargs = {}
    while True:
        response = table.scan(**scan_kwargs)
        
        for item in response['Items']:
            # TTLì´ ì•„ì§ ìœ íš¨í•œ í•­ëª©ë§Œ
            if item.get('ttl', 0) > time.time():
                blocks = item.get('s3_pointers', {}).get('state_blocks', [])
                for block_path in blocks:
                    block_id = block_path.split('/')[-1].replace('.json', '')
                    referenced_blocks.add(block_id)
        
        if 'LastEvaluatedKey' not in response:
            break
        scan_kwargs['ExclusiveStartKey'] = response['LastEvaluatedKey']
    
    logger.info(f"Found {len(referenced_blocks)} referenced blocks")
    
    # 2. S3ì—ì„œ ëª¨ë“  ë¸”ë¡ ë‚˜ì—´
    all_blocks = set()
    paginator = s3.get_paginator('list_objects_v2')
    
    for page in paginator.paginate(Bucket=bucket, Prefix='state-blocks/'):
        for obj in page.get('Contents', []):
            block_id = obj['Key'].split('/')[-1].replace('.json', '')
            all_blocks.add(block_id)
    
    logger.info(f"Found {len(all_blocks)} total blocks in S3")
    
    # 3. ì°¸ì¡°ë˜ì§€ ì•ŠëŠ” ë¸”ë¡ ì‚­ì œ
    orphaned_blocks = all_blocks - referenced_blocks
    
    if orphaned_blocks:
        logger.info(f"Deleting {len(orphaned_blocks)} orphaned blocks")
        
        # ë°°ì¹˜ ì‚­ì œ (ìµœëŒ€ 1000ê°œì”©)
        for i in range(0, len(orphaned_blocks), 1000):
            batch = list(orphaned_blocks)[i:i+1000]
            s3.delete_objects(
                Bucket=bucket,
                Delete={
                    'Objects': [{'Key': f'state-blocks/{bid}.json'} for bid in batch]
                }
            )
        
        logger.info(f"GC completed: deleted {len(orphaned_blocks)} blocks")
    else:
        logger.info("GC completed: no orphaned blocks found")
    
    return {
        'referenced': len(referenced_blocks),
        'total': len(all_blocks),
        'deleted': len(orphaned_blocks)
    }
```

---

## ğŸ¯ Phase 7: ë³´ì•ˆ ê°•í™” (Week 10, P1)

### 7.1 Merkle Hash ê²€ì¦ ê°•ì œ

**íŒŒì¼:** [segment_runner_service.py](c:\\Users\\gimgy\\OneDrive\\ë°”íƒ•%20í™”ë©´\\Analemma-Os\\analemma-workflow-os\\backend\\src\\services\\execution\\segment_runner_service.py)

```python
def execute_segment(self, event, context):
    """
    ì„¸ê·¸ë¨¼íŠ¸ ì‹¤í–‰ with Pre-computed Hash ê²€ì¦ (ìµœì í™”)
    """
    # 1. manifest_hash ê²€ì¦
    manifest_id = event.get('manifest_id')
    segment_index = event.get('segment_index')
    
    if not manifest_id:
        raise SecurityError("Missing manifest_id - execution blocked")
    
    # 2. segment_config ë¡œë“œ
    segment_config = event.get('segment_config')
    
    # 3. Pre-computed Hashë¡œ ë¬´ê²°ì„± ê²€ì¦ (1-5ms, ê¸°ì¡´ 200-500ms ëŒ€ë¹„ 100ë°° ë¹ ë¦„)
    versioning_service = StateVersioningService(
        dynamodb_table=os.environ['MANIFESTS_TABLE'],
        s3_bucket=os.environ['STATE_BUCKET']
    )
    
    is_valid = versioning_service.verify_segment_config(
        segment_config=segment_config,
        manifest_id=manifest_id,
        segment_index=segment_index
    )
    
    if not is_valid:
        raise SecurityError(
            f"Segment config integrity violation! "
            f"manifest_id={manifest_id}, segment={segment_index}"
        )
    
    logger.info(f"âœ“ Segment {segment_index} verified (pre-computed hash)")
    
    # 4. ì‹¤í–‰
    # ...
```

**StateVersioningServiceì— ì¶”ê°€ (Pre-computed Hash Verification):**
```python
def verify_segment_config(
    self,
    segment_config: dict,
    manifest_id: str,
    segment_index: int
) -> bool:
    """
    segment_config ë¬´ê²°ì„± ê²€ì¦ (Pre-computed Hash ë°©ì‹)
    
    í”¼ë“œë°± ë°˜ì˜:
    - âŒ ê¸°ì¡´: ë§¤ë²ˆ partition_workflow() ì¬ì‹¤í–‰ (ë„ˆë¬´ ë¬´ê±°ì›€)
    - âœ… ê°œì„ : Pre-computed Hashë¡œ O(1) ê²€ì¦
    
    ë°©ë²•:
    1. DynamoDBì—ì„œ manifestì˜ segment_hashes ë¡œë“œ
    2. ì…ë ¥ëœ segment_configì˜ í•´ì‹œ ê³„ì‚°
    3. Pre-computed Hashì™€ ë¹„êµ
    
    Args:
        segment_config: ê²€ì¦í•  ì„¸ê·¸ë¨¼íŠ¸ ì„¤ì •
        manifest_id: ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ID
        segment_index: ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤
    
    Returns:
        bool: ê²€ì¦ í†µê³¼ ì—¬ë¶€
    """
    try:
        # 1. DynamoDBì—ì„œ Pre-computed Hash ë¡œë“œ
        response = self.table.get_item(
            Key={'manifest_id': manifest_id},
            ProjectionExpression='segment_hashes'
        )
        
        if 'Item' not in response:
            logger.error(f"Manifest not found: {manifest_id}")
            return False
        
        segment_hashes = response['Item'].get('segment_hashes', {})
        expected_hash = segment_hashes.get(str(segment_index))
        
        if not expected_hash:
            logger.error(f"No pre-computed hash for segment {segment_index}")
            return False
        
        # 2. ì…ë ¥ëœ segment_configì˜ í•´ì‹œ ê³„ì‚°
        actual_hash = self._compute_hash(segment_config)
        
        # 3. ë¹„êµ
        is_valid = actual_hash == expected_hash
        
        if not is_valid:
            logger.error(
                f"[Integrity Violation] Segment {segment_index} hash mismatch!\n"
                f"Expected: {expected_hash[:16]}...\n"
                f"Actual:   {actual_hash[:16]}..."
            )
        else:
            logger.info(f"âœ“ Segment {segment_index} verified: {actual_hash[:8]}...")
        
        return is_valid
    
    except Exception as e:
        logger.error(f"Verification failed: {e}", exc_info=True)
        return False
```

**ì„±ëŠ¥ ë¹„êµ:**
```
ê¸°ì¡´ (partition_workflow ì¬ì‹¤í–‰):
- ì‹œê°„: 200-500ms (workflow í¬ê¸°ì— ë”°ë¼)
- CPU: ë†’ìŒ
- ë©”ëª¨ë¦¬: ë†’ìŒ (ì „ì²´ graph ì¬êµ¬ì„±)

ê°œì„  (Pre-computed Hash):
- ì‹œê°„: 1-5ms (í•´ì‹œ ê³„ì‚°ë§Œ)
- CPU: ë‚®ìŒ
- ë©”ëª¨ë¦¬: ë‚®ìŒ (segment_configë§Œ)

â†’ 100ë°° ì´ìƒ ì„±ëŠ¥ í–¥ìƒ
```

---

## ğŸ“Š ìµœì¢… ì•„í‚¤í…ì²˜ ë¹„êµ

### Before (V2 - Fat State):
```
ë°ì´í„° íë¦„:
Initialize
â”œâ”€ workflow_config: 200KB â†’ StateBag âŒ
â”œâ”€ partition_map: 50KB â†’ StateBag âŒ
â””â”€ StateBag: 370KB

Segment 0-99
â”œâ”€ workflow_config: 200KB Ã— 100 = 20MB âŒ
â”œâ”€ partition_map: 50KB Ã— 100 = 5MB âŒ
â””â”€ ì´ ì „ì†¡: 37MB

ë³´ì•ˆ:
- ì„¸ê·¸ë¨¼íŠ¸ê°€ ì „ì²´ ì›Œí¬í”Œë¡œìš° êµ¬ì¡° ì ‘ê·¼ ê°€ëŠ¥ âŒ
- ìƒíƒœ ì¡°ì‘ ê°ì§€ ë¶ˆê°€ âŒ
- íšŒê·€ ì‹œ ì •í™•í•œ ì‹œì  ì¬í˜„ ë¶ˆê°€ âŒ

ë¹„ìš©:
- S3 ì €ì¥: ë†’ìŒ (ì¤‘ë³µ 90%)
- Lambda ë©”ëª¨ë¦¬: ë†’ìŒ
- ë„¤íŠ¸ì›Œí¬: ë†’ìŒ
```

### After (V3 - Merkle DAG + Lean Manifest):
```
ë°ì´í„° íë¦„:
Initialize
â”œâ”€ workflow_config: 200KB â†’ S3 (ì°¸ì¡°ìš©, hash ì €ì¥) âœ…
â”œâ”€ partition_map: ë¡œì»¬ íê¸° âœ…
â”œâ”€ Merkle Manifest: DynamoDB Pointer âœ…
â””â”€ StateBag: 120KB (-68%)

Segment 0-99
â”œâ”€ segment_config: 10KB (ASL ì§ì ‘ ì£¼ì… ë˜ëŠ” S3 Select) âœ…
â”œâ”€ manifest_hash: ê²€ì¦ âœ…
â””â”€ ì´ ì „ì†¡: 13MB (-65%)

ë³´ì•ˆ:
- ì„¸ê·¸ë¨¼íŠ¸ëŠ” ìì‹ ì˜ configë§Œ ì ‘ê·¼ (ìµœì†Œ ê¶Œí•œ) âœ…
- Merkle Rootë¡œ 1ë°”ì´íŠ¸ ì¡°ì‘ë„ ê°ì§€ âœ…
- Pointer Manifestë¡œ ì¦‰ì‹œ íšŒê·€ ê°€ëŠ¥ âœ…

ë¹„ìš©:
- S3 ì €ì¥: ë‚®ìŒ (ì¤‘ë³µ 10%, GC ìë™í™”)
- Lambda ë©”ëª¨ë¦¬: ë‚®ìŒ (-68%)
- ë„¤íŠ¸ì›Œí¬: ë‚®ìŒ (-65%)
```

---

## ğŸ“‹ Implementation Checklist

### Phase 0: ì‚¬ì „ ì¤€ë¹„ (Week 1)
- [ ] `_load_segment_config_from_manifest()` êµ¬í˜„
- [ ] Size-based routing ë¡œì§ ì¶”ê°€
- [ ] Hybrid loading ë°°í¬ (í˜¸í™˜ì„± í™•ë³´)
- [ ] ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° ì •ìƒ ë™ì‘ í™•ì¸

### Phase 1: Merkle DAG ì¸í”„ë¼ (Week 2-3)
- [ ] WorkflowManifestsV3 DynamoDB í…Œì´ë¸” ìƒì„±
- [ ] StateVersioningService êµ¬í˜„
- [ ] Merkle Root ê³„ì‚° ë¡œì§ ê²€ì¦
- [ ] Content-Addressable Storage í…ŒìŠ¤íŠ¸

### Phase 2: workflow_config ì œê±° (Week 4)
- [ ] initialize_state_data.py ìˆ˜ì •
- [ ] ASL Threshold-based Loading êµ¬í˜„
- [ ] manifest_hash, config_hash ì „ë‹¬ ë¡œì§
- [ ] íšŒê·€ í…ŒìŠ¤íŠ¸ (ê¸°ë³¸/parallel_group)

### Phase 3: SegmentFieldOptimizer (Week 5)
- [ ] Capability-based Filtering êµ¬í˜„
- [ ] Security Ring í†µí•©
- [ ] segment_runner_service.py í†µí•©
- [ ] í˜ì´ë¡œë“œ ì ˆê° ê²€ì¦ (67%)

### Phase 4: S3 Select ìµœì í™” (Week 6)
- [ ] StateHydrator Size-based routing
- [ ] Cost optimization ë¡œì§
- [ ] S3 Select ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### Phase 5: ë¹„ë™ê¸° ì»¤ë°‹ (Week 7-8)
- [ ] AsyncStateCheckpointer êµ¬í˜„
- [ ] Redis ìºì‹œ ë ˆì´ì–´ ì¶”ê°€
- [ ] Read-After-Write ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
- [ ] SNS + Lambda ë¹„ë™ê¸° ì›Œì»¤ ë°°í¬

### Phase 6: Garbage Collection (Week 9)
- [ ] S3 Lifecycle Policy ì ìš©
- [ ] GC Lambda êµ¬í˜„
- [ ] CloudWatch Events íŠ¸ë¦¬ê±° ì„¤ì •
- [ ] 30ì¼ í›„ ìë™ ì‚­ì œ ê²€ì¦

### Phase 7: ë³´ì•ˆ ê°•í™” (Week 10)
- [ ] Merkle Hash ê²€ì¦ ê°•ì œ
- [ ] segment_config ë¬´ê²°ì„± ê²€ì‚¬
- [ ] Security audit ì™„ë£Œ
- [ ] Penetration testing

### ìš´ì˜ ì¤€ë¹„
- [ ] CloudWatch ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ
- [ ] ì•ŒëŒ ì„¤ì • (ì¼ê´€ì„± ìœ„ë°˜, GC ì‹¤íŒ¨)
- [ ] ë¬¸ì„œí™” (API, Architecture, Migration)
- [ ] íŒ€ êµìœ¡

---

## âš ï¸ ë¦¬ìŠ¤í¬ ë° ì™„í™” ì „ëµ

### 1. Read-After-Write Consistency ìœ„ë°˜
**ë¦¬ìŠ¤í¬:** ë¹„ë™ê¸° ì»¤ë°‹ ì¤‘ ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ê°€ ê³¼ê±° ìƒíƒœ ì½ìŒ  
**ì™„í™”:**
- ììœ¨í˜• ë£¨í”„ ë‚´ë¶€ëŠ” ë™ê¸° ì»¤ë°‹ ìœ ì§€
- HITL ì „ wait_for_commit=True
- Redis ë²„ì „ í† í°ìœ¼ë¡œ ëŒ€ê¸°

### 2. S3 Select ë¹„ìš© ì¦ê°€
**ë¦¬ìŠ¤í¬:** ì‘ì€ ê°ì²´ì—ë„ Select ì‚¬ìš© ì‹œ ë¹„ìš© ìƒìŠ¹  
**ì™„í™”:**
- 10KB ë¯¸ë§Œì€ GetObject ê°•ì œ
- í•„ë“œ ë¹„ìœ¨ 80% ì´ìƒì´ë©´ GetObject
- CloudWatch Cost Explorer ëª¨ë‹ˆí„°ë§

### 3. Merkle DAG ë³µì¡ë„
**ë¦¬ìŠ¤í¬:** ë””ë²„ê¹… ì–´ë ¤ì›€, ê°œë°œì í•™ìŠµ ê³¡ì„   
**ì™„í™”:**
- ëª…í™•í•œ ë¬¸ì„œí™”
- ë””ë²„ê¹… ë„êµ¬ ì œê³µ (manifest ì‹œê°í™”)
- ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜ (V2 fallback ìœ ì§€)

### 4. GC ì˜¤ì‘ë™
**ë¦¬ìŠ¤í¬:** ì°¸ì¡° ì¤‘ì¸ ë¸”ë¡ ì‚­ì œ  
**ì™„í™”:**
- Dry-run ëª¨ë“œ ë¨¼ì € í…ŒìŠ¤íŠ¸
- ì‚­ì œ ì „ 30ì¼ Glacier ë³´ê´€
- ìˆ˜ë™ ë³µêµ¬ ì ˆì°¨ ë¬¸ì„œí™”

### 5. S3 Select ë ˆì´í„´ì‹œ ì§€í„°
**ë¦¬ìŠ¤í¬:** ì¿¼ë¦¬ íŒŒì‹± ì˜¤ë²„í—¤ë“œë¡œ ì‘ì€ ìš”ì²­ì—ì„œ GetObjectë³´ë‹¤ ëŠë¦¼  
**ì™„í™”:**
- CloudWatch ê¸°ë°˜ ë™ì  threshold íŠœë‹ (Phase 4.1)
- 10KB ë¯¸ë§Œ ê°•ì œ GetObject
- ì‹¤ì œ ë ˆì´í„´ì‹œ ì¸¡ì •í•˜ì—¬ 1ì‹œê°„ë§ˆë‹¤ threshold ìë™ ì¡°ì •

### 6. Redis TOCTOU (Time-of-Check to Time-of-Use)
**ë¦¬ìŠ¤í¬:** Redis 'committed' ìƒíƒœì™€ S3 ì‹¤ì œ ê°€ìš©ì„± ê°„ ì‹œê°„ì°¨  
**ì™„í™”:**
- S3 ë¡œë“œ ì‹œ Exponential Backoff ì¬ì‹œë„ (Phase 5.1)
- ìµœëŒ€ 5íšŒ, 0.1ì´ˆë¶€í„° ì‹œì‘í•˜ì—¬ ì§€ìˆ˜ ì¦ê°€
- S3 Eventual Consistency ëŒ€ì‘

### 7. ASL ë³µì¡ë„
**ë¦¬ìŠ¤í¬:** States.ArrayGetItem ë¬¸ë²• ì˜¤ë¥˜, ë™ì  ì¸ë±ìŠ¤ ì²˜ë¦¬ ì‹¤íŒ¨  
**ì™„í™”:**
- Lambda Fallbackì„ ì£¼ ê²½ë¡œë¡œ ì„¤ê³„ (80% ì²˜ë¦¬)
- ASL Direct Injectionì€ "ë¹ ë¥¸ ê²½ë¡œ"ë¡œë§Œ í™œìš© (20%)
- Lambda ìºì‹± ë¡œì§ì„ Phase 0ì—ì„œ ìµœìš°ì„  êµ¬í˜„

---

## ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ ëª©í‘œ

| ì§€í‘œ | í˜„ì¬ (V2) | ëª©í‘œ (V3) | ê°œì„ ìœ¨ |
|------|-----------|-----------|--------|
| í‰ê·  í˜ì´ë¡œë“œ í¬ê¸° | 400KB | 130KB | **-67%** |
| 100 ì„¸ê·¸ë¨¼íŠ¸ ì „ì†¡ëŸ‰ | 37MB | 13MB | **-65%** |
| ë°ì´í„° ì¤‘ë³µë¥  | 90% | 10% | **-89%** |
| íšŒê·€ ì†ë„ | ëŠë¦¼ (ì „ì²´ ë³µêµ¬) | ì¦‰ì‹œ (í¬ì¸í„° ì „í™˜) | **100ë°°â†‘** |
| S3 ì €ì¥ ë¹„ìš© | ê¸°ì¤€ | 30% | **-70%** |
| Lambda ë©”ëª¨ë¦¬ | 512MB | 256MB | **-50%** |
| ë¬´ê²°ì„± ê²€ì¦ | ë¶ˆê°€ëŠ¥ | Merkle Root | **100%** |

---

## ğŸ“ íŒ€ êµìœ¡ ìë£Œ

### ê°œë°œì ê°€ì´ë“œ
1. **Merkle DAG ê°œë…**
   - Gitê³¼ ë™ì¼í•œ Content-Addressable Storage
   - ìƒíƒœ ë³€ê²½ = ìƒˆ í•´ì‹œ ë¸”ë¡ ìƒì„±
   - ê³¼ê±° ë²„ì „ì€ í¬ì¸í„°ë§Œ ë°”ê¾¸ë©´ ì¦‰ì‹œ ì ‘ê·¼

2. **ë””ë²„ê¹… ë°©ë²•**
   ```bash
   # manifest_idë¡œ ìƒíƒœ ì¶”ì 
   aws dynamodb get-item \
     --table-name WorkflowManifestsV3 \
     --key '{"manifest_id": {"S": "uuid"}}'
   
   # íŠ¹ì • ë¸”ë¡ ë‚´ìš© í™•ì¸
   aws s3 cp s3://bucket/state-blocks/hash.json -
   ```

3. **íšŒê·€ ë°©ë²•**
   ```python
   # íŠ¹ì • ë²„ì „ìœ¼ë¡œ ë¡¤ë°±
   versioning_service.rollback_to_version(
       workflow_id="workflow_123",
       version=5  # 5ë²ˆ ë²„ì „ìœ¼ë¡œ
   )
   ```

---

## ğŸ“ ì°¸ê³  ë¬¸ì„œ

- [WORKFLOW_CONFIG_LIFECYCLE_FIX.md](WORKFLOW_CONFIG_LIFECYCLE_FIX.md) - workflow_config ì œê±° ìƒì„¸
- [SEGMENT_PAYLOAD_OPTIMIZATION.md](SEGMENT_PAYLOAD_OPTIMIZATION.md) - í˜ì´ë¡œë“œ ìµœì í™” ë¶„ì„
- [Git Internals - Objects](https://git-scm.com/book/en/v2/Git-Internals-Git-Objects) - Merkle DAG ì°¸ê³ 
- [S3 Select Documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html)

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** 2026-02-18  
**ë¬¸ì„œ ë²„ì „:** 1.0  
**ìŠ¹ì¸ì:** Architecture Review Board  
**ë‹¤ìŒ ê²€í† :** Phase 2 ì™„ë£Œ í›„
