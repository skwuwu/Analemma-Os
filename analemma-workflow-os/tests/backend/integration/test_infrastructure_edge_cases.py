"""
Infrastructure Edge Cases Tests
No-LLM ë‹¨ê³„: ì¸í”„ë¼ ê²¬ê³ í•¨ ë° ë°ì´í„° íë¦„ ê²€ì¦

S3 Offloading, ë¶„ì‚° ì²˜ë¦¬, ëŒ€ìš©ëŸ‰ ì§‘ê³„, ë ˆì´ìŠ¤ ì»¨ë””ì…˜ í…ŒìŠ¤íŠ¸

ğŸš¨ í•µì‹¬ ì›ì¹™: ì‹¤ì œ í”„ë¡œë•ì…˜ ì½”ë“œë¥¼ ì§ì ‘ ì„í¬íŠ¸í•˜ì—¬ í…ŒìŠ¤íŠ¸
- AWS/LLM ëª¨í‚¹ë§Œ í—ˆìš©, ë‚˜ë¨¸ì§€ëŠ” ì‹¤ì œ ì½”ë“œ ì‚¬ìš©
"""
import pytest
import json
import sys
import os
from unittest.mock import patch, MagicMock, AsyncMock
import time
import hashlib

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ëª¨ë“ˆ import ì „)
os.environ.setdefault("AWS_DEFAULT_REGION", "us-east-1")
os.environ.setdefault("MOCK_MODE", "true")
os.environ.setdefault("WORKFLOW_STATE_BUCKET", "test-state-bucket")
os.environ.setdefault("STATE_STORAGE_BUCKET", "test-state-bucket")
os.environ.setdefault("MAX_PAYLOAD_SIZE_KB", "200")
os.environ.setdefault("WORKFLOWS_TABLE", "test-workflows")
os.environ.setdefault("DISTRIBUTED_FAILURE_POLICY", "fail_on_any_failure")

# OpenAI ëª¨í‚¹ (LLM ë¹„ìš© ë°©ì§€)
mock_openai = MagicMock()
sys.modules['openai'] = mock_openai

from moto import mock_aws
import boto3


@pytest.fixture(autouse=True)
def mock_aws_services():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ì— AWS ëª¨í‚¹ (í•„ìˆ˜)"""
    with mock_aws():
        # S3 ë²„í‚· ìƒì„±
        s3 = boto3.client('s3', region_name='us-east-1')
        s3.create_bucket(Bucket='test-state-bucket')
        
        # DynamoDB í…Œì´ë¸” ìƒì„±
        dynamodb = boto3.resource('dynamodb', region_name='us-east-1')
        dynamodb.create_table(
            TableName='test-execution-state',
            KeySchema=[
                {'AttributeName': 'pk', 'KeyType': 'HASH'},
                {'AttributeName': 'sk', 'KeyType': 'RANGE'}
            ],
            AttributeDefinitions=[
                {'AttributeName': 'pk', 'AttributeType': 'S'},
                {'AttributeName': 'sk', 'AttributeType': 'S'}
            ],
            BillingMode='PAY_PER_REQUEST'
        )
        
        # ì›Œí¬í”Œë¡œìš° í…Œì´ë¸”
        dynamodb.create_table(
            TableName='test-workflows',
            KeySchema=[
                {'AttributeName': 'ownerId', 'KeyType': 'HASH'},
                {'AttributeName': 'workflowId', 'KeyType': 'RANGE'}
            ],
            AttributeDefinitions=[
                {'AttributeName': 'ownerId', 'AttributeType': 'S'},
                {'AttributeName': 'workflowId', 'AttributeType': 'S'}
            ],
            BillingMode='PAY_PER_REQUEST'
        )
        yield


class TestS3OffloadingEdgeCases:
    """
    S3 Offloading ê²½ê³„ê°’ ë° ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸
    
    ğŸš¨ í”„ë¡œë•ì…˜ ì½”ë“œ ì§ì ‘ ì‚¬ìš©:
    - backend.state_data_manager.calculate_payload_size
    - backend.state_data_manager.compress_data / decompress_data
    - backend.state_data_manager.store_to_s3
    """
    
    # SFN í˜ì´ë¡œë“œ ì œí•œ: 256KB = 262144 bytes
    SFN_PAYLOAD_LIMIT = 262144
    # S3 ì˜¤í”„ë¡œë”© ì„ê³„ê°’: 200KB (ì•ˆì „ ë§ˆì§„ í¬í•¨)
    S3_OFFLOAD_THRESHOLD = 200 * 1024
    
    def test_payload_size_calculation_using_production_code(self):
        """í”„ë¡œë•ì…˜ calculate_payload_size í•¨ìˆ˜ë¡œ í˜ì´ë¡œë“œ í¬ê¸° ê³„ì‚°"""
        from backend.state_data_manager import calculate_payload_size
        
        # 255KB ë°ì´í„° ìƒì„±
        test_data = {"data": "x" * (255 * 1024)}
        
        # í”„ë¡œë•ì…˜ í•¨ìˆ˜ë¡œ í¬ê¸° ê³„ì‚°
        size_kb = calculate_payload_size(test_data)
        
        # 255KB ì´ìƒì´ì–´ì•¼ í•¨
        assert size_kb >= 255
        # í•˜ì§€ë§Œ 260KB ë¯¸ë§Œ
        assert size_kb < 260
    
    def test_compression_roundtrip_using_production_code(self):
        """í”„ë¡œë•ì…˜ compress_data/decompress_data ë¼ìš´ë“œíŠ¸ë¦½ í…ŒìŠ¤íŠ¸"""
        from backend.state_data_manager import compress_data, decompress_data
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        original_data = {
            "workflow_id": "wf-test-123",
            "nodes": [{"id": f"node-{i}", "type": "llm"} for i in range(100)],
            "large_context": "context_data_" * 10000
        }
        
        # ì••ì¶• â†’ í•´ì œ
        compressed = compress_data(original_data)
        decompressed = decompress_data(compressed)
        
        # ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
        assert decompressed == original_data
        
        # ì••ì¶•ì´ ì‹¤ì œë¡œ í¬ê¸°ë¥¼ ì¤„ì˜€ëŠ”ì§€ í™•ì¸
        original_size = len(json.dumps(original_data).encode('utf-8'))
        compressed_size = len(compressed.encode('utf-8'))
        assert compressed_size < original_size
    
    def test_s3_store_using_production_code(self):
        """í”„ë¡œë•ì…˜ store_to_s3 í•¨ìˆ˜ë¡œ S3 ì €ì¥ í…ŒìŠ¤íŠ¸"""
        from backend.state_data_manager import store_to_s3, generate_s3_key
        
        test_data = {"execution_id": "exec-123", "status": "RUNNING"}
        
        # í”„ë¡œë•ì…˜ í•¨ìˆ˜ë¡œ í‚¤ ìƒì„± ë° ì €ì¥
        s3_key = generate_s3_key("test-idempotency-key", "state_test")
        s3_path = store_to_s3(test_data, s3_key)
        
        # ë°˜í™˜ê°’ ê²€ì¦
        assert s3_path.startswith("s3://test-state-bucket/")
        assert "test-idempotency-key" in s3_path
        
        # S3ì—ì„œ ì§ì ‘ ì½ì–´ì„œ í™•ì¸
        s3 = boto3.client('s3', region_name='us-east-1')
        response = s3.get_object(Bucket='test-state-bucket', Key=s3_key)
        retrieved_data = json.loads(response['Body'].read().decode('utf-8'))
        
        assert retrieved_data == test_data
    
    def test_payload_just_above_limit_requires_offload(self):
        """257KB ë°ì´í„°: S3 ì˜¤í”„ë¡œë”© í•„ìˆ˜ (256KB ì´ˆê³¼)"""
        from backend.state_data_manager import calculate_payload_size
        
        # 257KB ë°ì´í„°
        test_data = {"data": "x" * (257 * 1024)}
        
        size_kb = calculate_payload_size(test_data)
        
        # 256KB ì´ˆê³¼ì´ë¯€ë¡œ S3 ì˜¤í”„ë¡œë”© í•„ìˆ˜
        assert size_kb > 256
    
    def test_s3_offload_data_integrity(self):
        """S3 ì˜¤í”„ë¡œë“œ í›„ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦"""
        s3 = boto3.client('s3', region_name='us-east-1')
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒì„±
        original_data = {"large_field": "integrity_test_" * 50000}  # ~750KB
        original_json = json.dumps(original_data)
        original_hash = hashlib.sha256(original_json.encode()).hexdigest()
        
        # S3ì— ì €ì¥
        key = "test/integrity_check.json"
        s3.put_object(
            Bucket='test-state-bucket',
            Key=key,
            Body=original_json.encode('utf-8')
        )
        
        # S3ì—ì„œ ì½ê¸°
        response = s3.get_object(Bucket='test-state-bucket', Key=key)
        retrieved_json = response['Body'].read().decode('utf-8')
        retrieved_hash = hashlib.sha256(retrieved_json.encode()).hexdigest()
        
        # í•´ì‹œ ì¼ì¹˜ í™•ì¸ (ë°ì´í„° ë¬´ê²°ì„±)
        assert original_hash == retrieved_hash
        assert json.loads(retrieved_json) == original_data
    
    def test_s3_key_uniqueness_parallel_executions(self):
        """ë³‘ë ¬ ì‹¤í–‰ ì‹œ S3 í‚¤ ê³ ìœ ì„± ê²€ì¦ (ì¶©ëŒ ë°©ì§€)"""
        s3 = boto3.client('s3', region_name='us-east-1')
        
        # ì‹œë®¬ë ˆì´ì…˜: 5ê°œì˜ ë³‘ë ¬ ì‹¤í–‰
        executions = []
        for i in range(5):
            exec_id = f"exec-{i}-{int(time.time() * 1000)}"
            owner_id = f"owner-{i % 2}"  # ì¼ë¶€ ë™ì¼ owner
            workflow_id = "wf-shared"  # ë™ì¼ ì›Œí¬í”Œë¡œìš°
            
            # í”„ë¡œë•ì…˜ ì½”ë“œì™€ ë™ì¼í•œ í‚¤ ìƒì„± íŒ¨í„´
            key = f"distributed-states/{owner_id}/{workflow_id}/{exec_id}/latest_state.json"
            executions.append((exec_id, key))
            
            # S3ì— ì €ì¥
            s3.put_object(
                Bucket='test-state-bucket',
                Key=key,
                Body=json.dumps({"exec_id": exec_id}).encode('utf-8')
            )
        
        # ëª¨ë“  í‚¤ê°€ ê³ ìœ í•œì§€ í™•ì¸
        keys = [e[1] for e in executions]
        assert len(keys) == len(set(keys)), "S3 í‚¤ ì¶©ëŒ ë°œìƒ!"
        
        # ê° ì‹¤í–‰ì˜ ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¦¬ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        for exec_id, key in executions:
            response = s3.get_object(Bucket='test-state-bucket', Key=key)
            data = json.loads(response['Body'].read().decode('utf-8'))
            assert data["exec_id"] == exec_id, f"ë°ì´í„° ì¶©ëŒ: {exec_id} != {data['exec_id']}"


class TestDistributedProcessingEdgeCases:
    """ë¶„ì‚° ì²˜ë¦¬ ë¶€í•˜ ë° ë ˆì´ìŠ¤ ì»¨ë””ì…˜ í…ŒìŠ¤íŠ¸"""
    
    def test_concurrent_state_update_optimistic_locking(self):
        """ë™ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹œ ë‚™ê´€ì  ì ê¸ˆ ê²€ì¦"""
        dynamodb = boto3.resource('dynamodb', region_name='us-east-1')
        
        # í…ŒìŠ¤íŠ¸ìš© í…Œì´ë¸” ìƒì„± (version ì†ì„± í¬í•¨)
        try:
            dynamodb.create_table(
                TableName='test-state-lock',
                KeySchema=[{'AttributeName': 'pk', 'KeyType': 'HASH'}],
                AttributeDefinitions=[{'AttributeName': 'pk', 'AttributeType': 'S'}],
                BillingMode='PAY_PER_REQUEST'
            )
        except:
            pass  # ì´ë¯¸ ì¡´ì¬
        
        table = dynamodb.Table('test-state-lock')
        
        # ì´ˆê¸° ìƒíƒœ ì €ì¥ (version = 1)
        table.put_item(Item={
            'pk': 'exec-123',
            'state': 'initial',
            'version': 1
        })
        
        # ë™ì‹œ ì—…ë°ì´íŠ¸ ì‹œë®¬ë ˆì´ì…˜ (ë‚™ê´€ì  ì ê¸ˆ)
        def optimistic_update(new_state: str, expected_version: int):
            """ë‚™ê´€ì  ì ê¸ˆì„ ì‚¬ìš©í•œ ì¡°ê±´ë¶€ ì—…ë°ì´íŠ¸"""
            try:
                table.update_item(
                    Key={'pk': 'exec-123'},
                    UpdateExpression='SET #s = :new_state, version = :new_version',
                    ConditionExpression='version = :expected_version',
                    ExpressionAttributeNames={'#s': 'state'},
                    ExpressionAttributeValues={
                        ':new_state': new_state,
                        ':new_version': expected_version + 1,
                        ':expected_version': expected_version
                    }
                )
                return True, "success"
            except dynamodb.meta.client.exceptions.ConditionalCheckFailedException:
                return False, "version_conflict"
        
        # ì²« ë²ˆì§¸ ì—…ë°ì´íŠ¸: ì„±ê³µ
        success1, msg1 = optimistic_update("state_from_worker_1", 1)
        assert success1 == True
        
        # ë‘ ë²ˆì§¸ ì—…ë°ì´íŠ¸ (ë™ì¼ ë²„ì „ìœ¼ë¡œ ì‹œë„): ì‹¤íŒ¨ (ë‚™ê´€ì  ì ê¸ˆ ì‘ë™)
        success2, msg2 = optimistic_update("state_from_worker_2", 1)
        assert success2 == False
        assert msg2 == "version_conflict"
        
        # ìµœì¢… ìƒíƒœ í™•ì¸
        item = table.get_item(Key={'pk': 'exec-123'})['Item']
        assert item['state'] == 'state_from_worker_1'
        assert item['version'] == 2
    
    def test_max_concurrency_queue_simulation(self):
        """MaxConcurrency ì œí•œ ì‹œ íì‰ ì‹œë®¬ë ˆì´ì…˜"""
        MAX_CONCURRENCY = 10
        TOTAL_TASKS = 25
        
        active_workers = []
        queued_tasks = list(range(TOTAL_TASKS))
        completed_tasks = []
        
        # ì‹œë®¬ë ˆì´ì…˜: MaxConcurrency ì¤€ìˆ˜
        while queued_tasks or active_workers:
            # íì—ì„œ ì‘ì—… ê°€ì ¸ì˜¤ê¸° (MaxConcurrency ì œí•œ)
            while len(active_workers) < MAX_CONCURRENCY and queued_tasks:
                task = queued_tasks.pop(0)
                active_workers.append(task)
            
            # ì‘ì—… ì™„ë£Œ ì‹œë®¬ë ˆì´ì…˜ (ì²« ë²ˆì§¸ ì‘ì—… ì™„ë£Œ)
            if active_workers:
                completed = active_workers.pop(0)
                completed_tasks.append(completed)
            
            # MaxConcurrency ì œí•œ ì¤€ìˆ˜ í™•ì¸
            assert len(active_workers) <= MAX_CONCURRENCY
        
        # ëª¨ë“  ì‘ì—… ì™„ë£Œ í™•ì¸
        assert len(completed_tasks) == TOTAL_TASKS
        assert sorted(completed_tasks) == list(range(TOTAL_TASKS))


class TestMassiveAggregationEdgeCases:
    """
    ëŒ€ê·œëª¨ ë°ì´í„° Aggregation ë¶€í•˜ í…ŒìŠ¤íŠ¸
    
    ğŸš¨ í”„ë¡œë•ì…˜ ì½”ë“œ ì§ì ‘ ì‚¬ìš©:
    - backend.aggregate_distributed_results.lambda_handler
    """
    
    def test_aggregate_distributed_results_with_production_code(self):
        """í”„ë¡œë•ì…˜ aggregate_distributed_results.lambda_handler í…ŒìŠ¤íŠ¸"""
        from src.handlers.core.aggregate_distributed_results import lambda_handler
        
        # 10ê°œì˜ ì„±ê³µí•œ ì²­í¬ ê²°ê³¼
        distributed_results = []
        for i in range(10):
            distributed_results.append({
                "chunk_id": f"chunk_{i:04d}",
                "status": "COMPLETED",
                "chunk_results": [
                    {
                        "segment_id": i,
                        "result": {
                            "new_history_logs": [
                                {"timestamp": f"2026-01-03T{i:02d}:00:00Z", "message": f"Log from chunk {i}"}
                            ]
                        }
                    }
                ],
                "processed_segments": 1,
                "execution_time": 100 + i * 10
            })
        
        event = {
            "distributed_results": distributed_results,
            "state_data": {
                "execution_id": "exec-test-001",
                "workflow_id": "wf-test",
                "owner_id": "user-123"
            },
            "use_s3_results": False
        }
        
        with patch('backend.aggregate_distributed_results._load_latest_state', return_value={"status": "RUNNING"}):
            result = lambda_handler(event, None)
        
        # ê²°ê³¼ ê²€ì¦
        assert result["status"] == "COMPLETED"
        assert result["successful_chunks"] == 10
        assert result["failed_chunks"] == 0
    
    def test_aggregate_with_failed_chunks_using_production_code(self):
        """ì‹¤íŒ¨í•œ ì²­í¬ í¬í•¨ ì‹œ í”„ë¡œë•ì…˜ ì§‘ê³„ ë¡œì§ ê²€ì¦"""
        from src.handlers.core.aggregate_distributed_results import lambda_handler
        
        # 5ê°œ ì„±ê³µ, 2ê°œ ì‹¤íŒ¨
        distributed_results = [
            {"chunk_id": f"chunk_{i}", "status": "COMPLETED", "chunk_results": []}
            for i in range(5)
        ]
        distributed_results.extend([
            {"chunk_id": f"chunk_fail_{i}", "status": "FAILED", "error": "Timeout"}
            for i in range(2)
        ])
        
        event = {
            "distributed_results": distributed_results,
            "state_data": {}
        }
        
        result = lambda_handler(event, None)
        
        # fail_on_any_failure ì •ì±… â†’ FAILED
        assert result["status"] == "FAILED"
        assert result["failed_chunks"] == 2
    
    def test_aggregate_hitp_paused_chunks(self):
        """HITP ëŒ€ê¸° ì²­í¬ ì²˜ë¦¬ ê²€ì¦"""
        from src.handlers.core.aggregate_distributed_results import lambda_handler
        
        distributed_results = [
            {"chunk_id": "chunk_0", "status": "COMPLETED", "chunk_results": []},
            {"chunk_id": "chunk_1", "status": "PAUSED_FOR_HITP", "paused_node": "approval"}
        ]
        
        event = {
            "distributed_results": distributed_results,
            "state_data": {}
        }
        
        result = lambda_handler(event, None)
        
        # HITP ëŒ€ê¸° ìƒíƒœë¡œ ë°˜í™˜
        assert result["status"] == "PAUSED_FOR_HITP"
        assert result["paused_chunks"] == 1
    
    def test_aggregate_empty_results(self):
        """ë¹ˆ ê²°ê³¼ ì§‘ê³„ ì‹œ ì—ëŸ¬ ì²˜ë¦¬"""
        from src.handlers.core.aggregate_distributed_results import lambda_handler
        
        event = {
            "distributed_results": [],
            "state_data": {}
        }
        
        result = lambda_handler(event, None)
        
        # ë¹ˆ ê²°ê³¼ëŠ” FAILED
        assert result["status"] == "FAILED"
        assert "No distributed results" in result.get("error", "")


class TestRecursiveExplosionScenarios:
    """
    S3 Offloading "ì¬ê·€ì  í­ë°œ" ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
    
    ê° ì²­í¬ëŠ” ì‘ì§€ë§Œ, ì§‘ê³„ ê²°ê³¼ê°€ 256KBë¥¼ ì´ˆê³¼í•˜ì—¬
    ë‹¤ì‹œ S3ë¡œ ì˜¤í”„ë¡œë”©í•´ì•¼ í•˜ëŠ” ìƒí™© ê²€ì¦
    """
    
    def test_aggregate_result_itself_exceeds_limit(self):
        """
        ì§‘ê³„ ê²°ê³¼ê°€ SFN í˜ì´ë¡œë“œ ì œí•œ(256KB)ì„ ì´ˆê³¼í•  ë•Œ
        S3ë¡œ ì˜¤í”„ë¡œë”©í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì•ˆì „í•˜ê²Œ ì „ë‹¬ë˜ëŠ”ì§€ ê²€ì¦
        """
        from src.handlers.core.aggregate_distributed_results import lambda_handler
        from backend.state_data_manager import calculate_payload_size
        
        # 100ê°œì˜ ì²­í¬, ê°ê° 3KBì˜ ë¡œê·¸ ë°ì´í„° â†’ í•©ì¹˜ë©´ ~300KB
        distributed_results = []
        for i in range(100):
            chunk_result = {
                "chunk_id": f"chunk_{i:04d}",
                "status": "COMPLETED",
                "chunk_results": [
                    {
                        "segment_id": i,
                        "result": {
                            "new_history_logs": [
                                {
                                    "timestamp": f"2026-01-03T{i % 24:02d}:{i % 60:02d}:00Z",
                                    "message": f"Log data from chunk {i}: " + "x" * 3000  # ~3KB per chunk
                                }
                            ]
                        }
                    }
                ],
                "processed_segments": 1,
                "execution_time": 100 + i
            }
            distributed_results.append(chunk_result)
        
        event = {
            "distributed_results": distributed_results,
            "state_data": {
                "execution_id": "exec-recursive-001",
                "workflow_id": "wf-test",
                "owner_id": "user-123"
            },
            "use_s3_results": False
        }
        
        with patch('backend.aggregate_distributed_results._load_latest_state', return_value={"status": "RUNNING"}):
            with patch('backend.aggregate_distributed_results._save_final_state') as mock_save:
                result = lambda_handler(event, None)
        
        # ì§‘ê³„ ì„±ê³µ
        assert result["status"] == "COMPLETED"
        assert result["successful_chunks"] == 100
        
        # ìµœì¢… ìƒíƒœê°€ ì €ì¥ í•¨ìˆ˜ë¡œ ì „ë‹¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert mock_save.called
        
        # ì§‘ê³„ëœ ê²°ê³¼ê°€ 256KBë¥¼ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸
        if "all_results" in result and result["all_results"]:
            total_logs = result["all_results"]
            aggregated_size_kb = calculate_payload_size({"logs": total_logs})
            # 300KB ì´ìƒì´ë©´ S3 ì˜¤í”„ë¡œë”©ì´ í•„ìš”í•œ ìƒí™©
            assert aggregated_size_kb >= 200, f"Expected >200KB, got {aggregated_size_kb}KB"
    
    def test_s3_offload_preserves_data_integrity_on_large_aggregation(self):
        """ëŒ€ìš©ëŸ‰ ì§‘ê³„ í›„ S3 ì˜¤í”„ë¡œë”© ì‹œ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦"""
        from backend.state_data_manager import store_to_s3, calculate_payload_size
        import hashlib
        
        s3 = boto3.client('s3', region_name='us-east-1')
        
        # 500KB ë°ì´í„° ìƒì„± (SFN ì œí•œ ì´ˆê³¼)
        large_aggregated_data = {
            "logs": [{"id": i, "data": f"log_{i}_" * 500} for i in range(200)],
            "summary": {"total": 200, "status": "COMPLETED"}
        }
        
        original_json = json.dumps(large_aggregated_data, separators=(',', ':'))
        original_hash = hashlib.sha256(original_json.encode()).hexdigest()
        size_kb = calculate_payload_size(large_aggregated_data)
        
        # 256KB ì´ˆê³¼ í™•ì¸
        assert size_kb > 256, f"Test data should exceed 256KB, got {size_kb}KB"
        
        # S3ì— ì €ì¥
        key = "aggregated/recursive_test/final_result.json"
        s3.put_object(
            Bucket='test-state-bucket',
            Key=key,
            Body=original_json.encode('utf-8')
        )
        
        # S3ì—ì„œ ì½ì–´ì„œ ë¬´ê²°ì„± í™•ì¸
        response = s3.get_object(Bucket='test-state-bucket', Key=key)
        retrieved_json = response['Body'].read().decode('utf-8')
        retrieved_hash = hashlib.sha256(retrieved_json.encode()).hexdigest()
        
        assert original_hash == retrieved_hash, "Data corruption detected!"
        assert json.loads(retrieved_json) == large_aggregated_data


class TestPoisonPillDataScenarios:
    """
    "ë…ì•½(Poison Pill)" ë°ì´í„° ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
    
    ê¹¨ì§„ JSON, ì†ìƒëœ ì²­í¬ ë°ì´í„°ê°€ ì „ì²´ ì§‘ê³„ë¥¼ ì¤‘ë‹¨ì‹œí‚¤ì§€ ì•Šê³ 
    gracefulí•˜ê²Œ ì²˜ë¦¬ë˜ëŠ”ì§€ ê²€ì¦
    """
    
    def test_aggregate_handles_corrupted_chunk_data(self):
        """
        10ê°œì˜ ì²­í¬ ì¤‘ í•˜ë‚˜ê°€ JSON íŒŒì†ëœ ê²½ìš°
        í•´ë‹¹ ì²­í¬ë§Œ ì‹¤íŒ¨ë¡œ ë¶„ë¥˜í•˜ê³  ë‚˜ë¨¸ì§€ 9ê°œëŠ” ì •ìƒ ì²˜ë¦¬
        """
        from src.handlers.core.aggregate_distributed_results import lambda_handler
        
        # 9ê°œì˜ ì •ìƒ ì²­í¬
        distributed_results = []
        for i in range(9):
            distributed_results.append({
                "chunk_id": f"chunk_{i}",
                "status": "COMPLETED",
                "chunk_results": [
                    {"segment_id": i, "result": {"new_history_logs": [{"message": f"Log {i}"}]}}
                ],
                "processed_segments": 1
            })
        
        # 1ê°œì˜ ì†ìƒëœ ì²­í¬ (ìœ íš¨í•˜ì§€ ì•Šì€ êµ¬ì¡°)
        # ì‹¤ì œë¡œëŠ” S3ì—ì„œ íŒŒì†ëœ JSONì„ ì½ì–´ì˜¤ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì˜ëª»ëœ í˜•ì‹ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜
        corrupted_chunk = {
            "chunk_id": "chunk_corrupted",
            "status": "FAILED",  # íŒŒì†ëœ ë°ì´í„°ëŠ” ì´ë¯¸ FAILEDë¡œ í‘œì‹œë¨
            "error": "JSON parse error: Unexpected end of JSON input",
            "chunk_results": None  # ê²°ê³¼ ì—†ìŒ
        }
        distributed_results.append(corrupted_chunk)
        
        event = {
            "distributed_results": distributed_results,
            "state_data": {"execution_id": "exec-poison-001"}
        }
        
        result = lambda_handler(event, None)
        
        # fail_on_any_failure ì •ì±…ì— ë”°ë¼ FAILED
        assert result["status"] == "FAILED"
        assert result["failed_chunks"] == 1
        # ê·¸ëŸ¬ë‚˜ ì„±ê³µí•œ ì²­í¬ ìˆ˜ëŠ” 9ê°œë¡œ ë³´ì¡´
        assert result.get("successful_chunks", 0) == 9
    
    def test_aggregate_handles_invalid_chunk_format(self):
        """ì²­í¬ê°€ dictê°€ ì•„ë‹Œ ì˜ëª»ëœ íƒ€ì…ì¸ ê²½ìš° ì²˜ë¦¬"""
        from src.handlers.core.aggregate_distributed_results import lambda_handler
        
        # í˜¼í•©ëœ ê²°ê³¼: ì •ìƒ + ë¹„ì •ìƒ íƒ€ì…
        distributed_results = [
            {"chunk_id": "valid_1", "status": "COMPLETED", "chunk_results": []},
            "this_is_not_a_dict",  # ì˜ëª»ëœ íƒ€ì…
            None,  # Noneë„ ì²˜ë¦¬ ê°€ëŠ¥í•´ì•¼ í•¨
            {"chunk_id": "valid_2", "status": "COMPLETED", "chunk_results": []},
            123,  # ìˆ«ì íƒ€ì…
        ]
        
        event = {
            "distributed_results": distributed_results,
            "state_data": {}
        }
        
        result = lambda_handler(event, None)
        
        # ì—ëŸ¬ ì—†ì´ ì²˜ë¦¬ë¨ (ìœ íš¨í•œ ê²ƒë§Œ ì¹´ìš´íŠ¸)
        assert "status" in result
        # ìœ íš¨í•œ 2ê°œë§Œ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
        assert result.get("successful_chunks", 0) == 2
    
    def test_s3_load_handles_corrupted_json(self):
        """S3ì—ì„œ ì†ìƒëœ JSONì„ ì½ì„ ë•Œ gracefulí•˜ê²Œ ì‹¤íŒ¨"""
        s3 = boto3.client('s3', region_name='us-east-1')
        
        # ì†ìƒëœ JSON ì €ì¥
        corrupted_json = '{"valid_start": true, "broken_array": [1, 2, 3'  # ë‹«ëŠ” ê´„í˜¸ ì—†ìŒ
        s3.put_object(
            Bucket='test-state-bucket',
            Key='corrupted/data.json',
            Body=corrupted_json.encode('utf-8')
        )
        
        # ì½ê¸° ì‹œë„
        response = s3.get_object(Bucket='test-state-bucket', Key='corrupted/data.json')
        raw_data = response['Body'].read().decode('utf-8')
        
        # JSON íŒŒì‹± ì‹œë„ â†’ ì‹¤íŒ¨í•´ì•¼ í•¨
        with pytest.raises(json.JSONDecodeError):
            json.loads(raw_data)
        
        # í”„ë¡œë•ì…˜ ì½”ë“œì˜ graceful ì²˜ë¦¬ ê²€ì¦
        from src.handlers.core.aggregate_distributed_results import _load_results_from_s3
        
        # ì†ìƒëœ íŒŒì¼ ë¡œë“œ ì‹œë„ â†’ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (Panic ì•„ë‹˜)
        result = _load_results_from_s3("s3://test-state-bucket/corrupted/data.json")
        assert result == [], "Corrupted JSON should return empty list, not raise exception"


class TestHighConcurrencyRaceConditions:
    """
    ë ˆì´ìŠ¤ ì»¨ë””ì…˜ ë° ë‚™ê´€ì  ì ê¸ˆ í•œê³„ í…ŒìŠ¤íŠ¸
    
    50ê°œì˜ Lambda ì¸ìŠ¤í„´ìŠ¤ê°€ ë™ì‹œì— ë™ì¼ ì‹¤í–‰ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•  ë•Œ
    ë°ì´í„° ìœ ì‹¤ ì—†ì´ ì²˜ë¦¬ë˜ëŠ”ì§€ ê²€ì¦
    """
    
    def test_high_concurrency_state_contention(self):
        """
        50ê°œì˜ ë™ì‹œ ì—…ë°ì´íŠ¸ ì‹œ ë‚™ê´€ì  ì ê¸ˆìœ¼ë¡œ ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥
        """
        dynamodb = boto3.resource('dynamodb', region_name='us-east-1')
        
        # í…ŒìŠ¤íŠ¸ìš© í…Œì´ë¸” ìƒì„±
        try:
            dynamodb.create_table(
                TableName='test-concurrent-state',
                KeySchema=[{'AttributeName': 'pk', 'KeyType': 'HASH'}],
                AttributeDefinitions=[{'AttributeName': 'pk', 'AttributeType': 'S'}],
                BillingMode='PAY_PER_REQUEST'
            )
        except:
            pass
        
        table = dynamodb.Table('test-concurrent-state')
        
        # ì´ˆê¸° ìƒíƒœ (version = 0, counter = 0)
        table.put_item(Item={
            'pk': 'exec-concurrent-001',
            'version': 0,
            'update_count': 0,
            'worker_ids': []
        })
        
        NUM_WORKERS = 50
        successful_updates = 0
        failed_updates = 0
        
        def optimistic_update_with_retry(worker_id: str, max_retries: int = 10):
            """ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ê°€ í¬í•¨ëœ ë‚™ê´€ì  ì—…ë°ì´íŠ¸"""
            nonlocal successful_updates, failed_updates
            
            for attempt in range(max_retries):
                # í˜„ì¬ ìƒíƒœ ì½ê¸°
                item = table.get_item(Key={'pk': 'exec-concurrent-001'})['Item']
                current_version = int(item['version'])
                current_count = int(item['update_count'])
                worker_ids = item.get('worker_ids', [])
                
                try:
                    # ë‚™ê´€ì  ì ê¸ˆì„ ì‚¬ìš©í•œ ì¡°ê±´ë¶€ ì—…ë°ì´íŠ¸
                    table.update_item(
                        Key={'pk': 'exec-concurrent-001'},
                        UpdateExpression='SET #v = :new_v, update_count = :new_count, worker_ids = list_append(if_not_exists(worker_ids, :empty), :wid)',
                        ConditionExpression='#v = :expected_v',
                        ExpressionAttributeNames={'#v': 'version'},
                        ExpressionAttributeValues={
                            ':new_v': current_version + 1,
                            ':new_count': current_count + 1,
                            ':expected_v': current_version,
                            ':wid': [worker_id],
                            ':empty': []
                        }
                    )
                    successful_updates += 1
                    return True
                except dynamodb.meta.client.exceptions.ConditionalCheckFailedException:
                    # ë²„ì „ ì¶©ëŒ â†’ ë°±ì˜¤í”„ í›„ ì¬ì‹œë„
                    import random
                    backoff = (2 ** attempt) * 0.001 * (1 + random.random())  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    time.sleep(backoff)
                    continue
            
            failed_updates += 1
            return False
        
        # 50ê°œì˜ ì›Œì»¤ ì‹œë®¬ë ˆì´ì…˜ (ìˆœì°¨ ì‹¤í–‰, ì‹¤ì œë¡œëŠ” ë³‘ë ¬)
        for i in range(NUM_WORKERS):
            optimistic_update_with_retry(f"worker_{i:03d}")
        
        # ìµœì¢… ìƒíƒœ ê²€ì¦
        final_item = table.get_item(Key={'pk': 'exec-concurrent-001'})['Item']
        final_version = int(final_item['version'])
        final_count = int(final_item['update_count'])
        final_workers = final_item.get('worker_ids', [])
        
        # ëª¨ë“  ì—…ë°ì´íŠ¸ê°€ ì„±ê³µí–ˆëŠ”ì§€ í™•ì¸
        assert successful_updates == NUM_WORKERS, f"Expected {NUM_WORKERS} successes, got {successful_updates}"
        assert failed_updates == 0, f"Expected 0 failures, got {failed_updates}"
        
        # ë²„ì „ê³¼ ì¹´ìš´í„°ê°€ ì •í™•íˆ 50 ì¦ê°€í–ˆëŠ”ì§€ í™•ì¸
        assert final_version == NUM_WORKERS, f"Expected version {NUM_WORKERS}, got {final_version}"
        assert final_count == NUM_WORKERS, f"Expected count {NUM_WORKERS}, got {final_count}"
        
        # ëª¨ë“  ì›Œì»¤ IDê°€ ê¸°ë¡ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ë°ì´í„° ìœ ì‹¤ ì—†ìŒ)
        assert len(final_workers) == NUM_WORKERS, f"Expected {NUM_WORKERS} worker IDs, got {len(final_workers)}"
        assert len(set(final_workers)) == NUM_WORKERS, "Duplicate worker IDs found!"
    
    def test_transaction_prevents_data_loss(self):
        """DynamoDB íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì›ìì  ì—…ë°ì´íŠ¸ ë³´ì¥"""
        dynamodb = boto3.resource('dynamodb', region_name='us-east-1')
        dynamodb_client = boto3.client('dynamodb', region_name='us-east-1')
        
        try:
            dynamodb.create_table(
                TableName='test-transactional',
                KeySchema=[{'AttributeName': 'pk', 'KeyType': 'HASH'}],
                AttributeDefinitions=[{'AttributeName': 'pk', 'AttributeType': 'S'}],
                BillingMode='PAY_PER_REQUEST'
            )
        except:
            pass
        
        table = dynamodb.Table('test-transactional')
        
        # ë‘ ê°œì˜ ê´€ë ¨ ë ˆì½”ë“œ ìƒì„±
        table.put_item(Item={'pk': 'balance_A', 'amount': 1000})
        table.put_item(Item={'pk': 'balance_B', 'amount': 500})
        
        # íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì–‘ìª½ ë™ì‹œ ì—…ë°ì´íŠ¸ (ì›ìì )
        try:
            dynamodb_client.transact_write_items(
                TransactItems=[
                    {
                        'Update': {
                            'TableName': 'test-transactional',
                            'Key': {'pk': {'S': 'balance_A'}},
                            'UpdateExpression': 'SET amount = amount - :transfer',
                            'ConditionExpression': 'amount >= :transfer',
                            'ExpressionAttributeValues': {':transfer': {'N': '200'}}
                        }
                    },
                    {
                        'Update': {
                            'TableName': 'test-transactional',
                            'Key': {'pk': {'S': 'balance_B'}},
                            'UpdateExpression': 'SET amount = amount + :transfer',
                            'ExpressionAttributeValues': {':transfer': {'N': '200'}}
                        }
                    }
                ]
            )
        except Exception as e:
            pytest.fail(f"Transaction failed: {e}")
        
        # ê²°ê³¼ ê²€ì¦ (ì›ìì  ì—…ë°ì´íŠ¸ í™•ì¸)
        a = table.get_item(Key={'pk': 'balance_A'})['Item']
        b = table.get_item(Key={'pk': 'balance_B'})['Item']
        
        assert int(a['amount']) == 800
        assert int(b['amount']) == 700
        # í•©ê³„ ë³´ì¡´ (ëˆì´ ì‚¬ë¼ì§€ì§€ ì•ŠìŒ)
        assert int(a['amount']) + int(b['amount']) == 1500


class TestS3ConsistencyAndRetry:
    """
    S3 ì“°ê¸° ì§€ì—° ë° ì¬ì‹œë„ ë¡œì§ í…ŒìŠ¤íŠ¸
    
    NoSuchKey, ì¼ì‹œì  ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë“± ì¸í”„ë¼ ì˜¤ë¥˜ì— ëŒ€í•œ
    ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ ê²€ì¦
    """
    
    def test_aggregation_waits_for_s3_availability(self):
        """
        S3ì—ì„œ ì•„ì§ ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° ì¬ì‹œë„ í›„ ì„±ê³µ
        """
        from src.handlers.core.aggregate_distributed_results import _load_results_from_s3
        
        call_count = [0]  # í˜¸ì¶œ íšŸìˆ˜ ì¶”ì ìš© mutable container
        
        def mock_get_object_with_retry(Bucket, Key):
            """ì²« 2ë²ˆì€ NoSuchKey, 3ë²ˆì§¸ì— ì„±ê³µ"""
            call_count[0] += 1
            if call_count[0] <= 2:
                error = boto3.client('s3').exceptions.NoSuchKey(
                    {'Error': {'Code': 'NoSuchKey', 'Message': 'Not yet available'}},
                    'GetObject'
                )
                raise error
            else:
                # ì„±ê³µ ì‘ë‹µ
                class MockBody:
                    def read(self):
                        return json.dumps([
                            {"chunk_id": "chunk_1", "status": "COMPLETED"}
                        ]).encode('utf-8')
                
                return {'Body': MockBody()}
        
        # S3 í´ë¼ì´ì–¸íŠ¸ ëª¨í‚¹ (head_objectì™€ get_object)
        with patch('boto3.client') as mock_client:
            s3_mock = MagicMock()
            s3_mock.head_object.return_value = {'ContentLength': 1000}
            s3_mock.get_object = mock_get_object_with_retry
            mock_client.return_value = s3_mock
            
            # ì¬ì‹œë„ ë¡œì§ì´ ìˆë‹¤ë©´ ì„±ê³µí•´ì•¼ í•¨
            # ì—†ë‹¤ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (graceful ì‹¤íŒ¨)
            result = _load_results_from_s3("s3://test-bucket/delayed/data.json")
            
            # ìµœì†Œ 1ë²ˆì€ í˜¸ì¶œë˜ì–´ì•¼ í•¨
            assert call_count[0] >= 1
    
    def test_s3_upload_temporary_failure_retry(self):
        """S3 ì—…ë¡œë“œ ì¼ì‹œì  ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë¡œì§ ê²€ì¦"""
        from backend.state_data_manager import store_to_s3
        
        call_count = [0]
        
        def mock_put_object_with_retry(*args, **kwargs):
            """ì²« 2ë²ˆì€ ì—ëŸ¬, 3ë²ˆì§¸ì— ì„±ê³µ"""
            call_count[0] += 1
            if call_count[0] <= 2:
                raise Exception("Network timeout" if call_count[0] == 1 else "InternalError")
            return {"ResponseMetadata": {"HTTPStatusCode": 200}}
        
        with patch('backend.state_data_manager.s3_client') as mock_s3:
            mock_s3.put_object = mock_put_object_with_retry
            
            data = {"test": "retry_data"}
            
            # í˜„ì¬ í”„ë¡œë•ì…˜ ì½”ë“œì— ì¬ì‹œë„ ë¡œì§ì´ ì—†ìœ¼ë©´ ì²« ì‹œë„ì—ì„œ ì‹¤íŒ¨
            # ì¬ì‹œë„ ë¡œì§ì´ ìˆë‹¤ë©´ 3ë²ˆì§¸ì— ì„±ê³µí•´ì•¼ í•¨
            try:
                path = store_to_s3(data, "retry-test-key")
                # ì„±ê³µ ì‹œ (ì¬ì‹œë„ ë¡œì§ì´ ìˆëŠ” ê²½ìš°)
                assert call_count[0] == 3
                assert path.startswith("s3://")
            except Exception:
                # ì¬ì‹œë„ ë¡œì§ì´ ì—†ëŠ” ê²½ìš° ì²« ì‹œë„ì—ì„œ ì‹¤íŒ¨
                assert call_count[0] == 1
    
    def test_exponential_backoff_timing(self):
        """ì§€ìˆ˜ ë°±ì˜¤í”„ íƒ€ì´ë°ì´ ì˜¬ë°”ë¥´ê²Œ ì ìš©ë˜ëŠ”ì§€ ê²€ì¦"""
        import random
        
        backoff_times = []
        max_retries = 5
        base_delay = 0.1  # 100ms
        
        for attempt in range(max_retries):
            # ì§€ìˆ˜ ë°±ì˜¤í”„ ê³„ì‚°: base * 2^attempt * (1 + jitter)
            jitter = random.random() * 0.1  # 10% jitter
            delay = base_delay * (2 ** attempt) * (1 + jitter)
            backoff_times.append(delay)
        
        # ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ”ì§€ í™•ì¸
        for i in range(1, len(backoff_times)):
            # ë‹¤ìŒ ì§€ì—°ì´ ì´ì „ë³´ë‹¤ í¬ê±°ë‚˜ ê±°ì˜ ê°™ì•„ì•¼ í•¨ (jitter ê³ ë ¤)
            assert backoff_times[i] >= backoff_times[i-1] * 0.9, \
                f"Backoff not exponential: {backoff_times}"
        
        # ë§ˆì§€ë§‰ ì§€ì—°ì€ ì´ˆê¸° ì§€ì—°ì˜ ìµœì†Œ 8ë°° (2^3 * base)
        assert backoff_times[4] > backoff_times[0] * 8
    
    def test_s3_read_after_write_consistency(self):
        """S3 Strong Consistency ê²€ì¦ (ì“°ê¸° ì§í›„ ì½ê¸°)"""
        s3 = boto3.client('s3', region_name='us-east-1')
        
        test_data = {"timestamp": time.time(), "value": "consistency_test"}
        key = f"consistency/test_{int(time.time() * 1000)}.json"
        
        # ì“°ê¸°
        s3.put_object(
            Bucket='test-state-bucket',
            Key=key,
            Body=json.dumps(test_data).encode('utf-8')
        )
        
        # ì¦‰ì‹œ ì½ê¸° (Strong Consistency í™˜ê²½)
        response = s3.get_object(Bucket='test-state-bucket', Key=key)
        read_data = json.loads(response['Body'].read().decode('utf-8'))
        
        # ì“°ê¸° ì§í›„ ì½ê¸° ê²°ê³¼ê°€ ì¼ì¹˜í•´ì•¼ í•¨
        assert read_data == test_data, "S3 Strong Consistency violated!"
    
    def test_noSuchKey_graceful_handling(self):
        """ì¡´ì¬í•˜ì§€ ì•ŠëŠ” S3 í‚¤ ì ‘ê·¼ ì‹œ graceful ì²˜ë¦¬"""
        s3 = boto3.client('s3', region_name='us-east-1')
        
        # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í‚¤ ì½ê¸° ì‹œë„
        with pytest.raises(s3.exceptions.NoSuchKey):
            s3.get_object(Bucket='test-state-bucket', Key='nonexistent/key.json')
