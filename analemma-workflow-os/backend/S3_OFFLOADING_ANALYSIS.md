# ğŸ” S3 Offloading & Recovery Analysis Report

**ì‘ì„±ì¼**: 2026-01-29
**ë¶„ì„ ë²”ìœ„**: state_data_manager.pyì˜ ëª¨ë“  ê²½ë¡œ

---

## ğŸ“Š Executive Summary

### âœ… êµ¬í˜„ ì™„ë£Œ í•­ëª©
- S3 ì˜¤í”„ë¡œë”©: 5ê°œ ê²½ë¡œì—ì„œ êµ¬í˜„
- ë³µêµ¬ ë¡œì§: 3ê°œ í•¨ìˆ˜ (load_from_s3, cached_load_from_s3, decompress_data)
- í¬ì¸í„° ìµœì í™”: ì ìš©ë¨

### âš ï¸ ê°œì„  í•„ìš” í•­ëª©
- **P0**: aggregate_distributed_resultsì— S3 ì˜¤í”„ë¡œë”© ë¯¸ì ìš©
- **P1**: í¬ì¸í„° ë¹„ëŒ€í™” ë¦¬ìŠ¤í¬ (ì¼ë¶€ ê²½ë¡œ)
- **P2**: ë³µêµ¬ ì‹¤íŒ¨ ì‹œ Fallback ë¡œì§ ë¶€ì¬

---

## 1ï¸âƒ£ S3 ì˜¤í”„ë¡œë”© ì ìš© í˜„í™©

### 1.1 ì ìš©ëœ ê²½ë¡œ

| ê²½ë¡œ | í•¨ìˆ˜ | ì˜¤í”„ë¡œë”© ëŒ€ìƒ | íŠ¸ë¦¬ê±° ì¡°ê±´ | ìƒíƒœ |
|------|------|-------------|------------|------|
| **ë ˆê±°ì‹œ ê²½ë¡œ** | `update_and_compress_state_data()` | state_history, current_state, workflow_config, partition_map | payload > 200KB | âœ… ì™„ì „ êµ¬í˜„ |
| **v3: Sync** | `sync_state_data()` | state_history, partition_map | payload > MAX_PAYLOAD_SIZE_KB | âœ… ì™„ì „ êµ¬í˜„ |
| **v3: Aggregate Branches** | `aggregate_branches()` | - (í¬ì¸í„°ë§Œ ì‚¬ìš©) | load_from_s3=True | âœ… ì™„ì „ êµ¬í˜„ |
| **Internal** | `optimize_state_history()` | old_history (50ê°œ ì´ìƒ) | len > 50 | âœ… ì™„ì „ êµ¬í˜„ |
| **Internal** | `optimize_current_state()` | ê°œë³„ í•„ë“œ (30KB ì´ìƒ), full_state (100KB ì´ìƒ) | í•„ë“œë³„ í¬ê¸° ì´ˆê³¼ | âœ… ì™„ì „ êµ¬í˜„ |

**ì ìš©ë¥ **: 5/5 ì£¼ìš” ê²½ë¡œ (100%)

---

### 1.2 ë¯¸ì ìš© ê²½ë¡œ (âš ï¸ ë¦¬ìŠ¤í¬)

| ê²½ë¡œ | í•¨ìˆ˜ | ë¦¬ìŠ¤í¬ | ìš°ì„ ìˆœìœ„ |
|------|------|--------|---------|
| **v3: Aggregate Distributed** | `aggregate_distributed_results()` | chunk_results ëˆ„ì  ì‹œ 256KB ì´ˆê³¼ ê°€ëŠ¥ | **P0** |
| **v3: Merge Callback** | `merge_callback_result()` | callback_result í¬ê¸° ì œí•œ ì—†ìŒ | P2 |
| **v3: Merge Async** | `merge_async_result()` | async_result í¬ê¸° ì œí•œ ì—†ìŒ | P2 |
| **v3: Snapshot** | `create_snapshot()` | snapshot_data ì „ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì €ì¥ | P1 |

---

## 2ï¸âƒ£ ë³µêµ¬(Recovery) ë¡œì§ ë¶„ì„

### 2.1 ë³µêµ¬ í•¨ìˆ˜ êµ¬í˜„ í˜„í™©

#### âœ… `load_from_s3(s3_path: str)` - Lines 102-130
```python
def load_from_s3(s3_path: str) -> Any:
    if not s3_path or not s3_path.startswith('s3://'):
        return None
    
    try:
        path_parts = s3_path.replace('s3://', '').split('/', 1)
        bucket = path_parts[0]
        key = path_parts[1] if len(path_parts) > 1 else ''
        
        response = s3_client.get_object(Bucket=bucket, Key=key)
        content = response['Body'].read().decode('utf-8')
        return json.loads(content)
    except Exception as e:
        logger.error(f"Failed to load from S3 {s3_path}: {e}")
        return None  # âš ï¸ ë³µêµ¬ ì‹¤íŒ¨ ì‹œ None ë°˜í™˜ (ë°ì´í„° ì†ì‹¤)
```

**ì´ìŠˆ**:
- âœ… ê¸°ë³¸ ë³µêµ¬ ë¡œì§ êµ¬í˜„
- âš ï¸ **ì‹¤íŒ¨ ì‹œ None ë°˜í™˜** â†’ ë°ì´í„° ì†ì‹¤
- âš ï¸ ì¬ì‹œë„ ë¡œì§ ì—†ìŒ
- âš ï¸ Fallback ë©”ì»¤ë‹ˆì¦˜ ì—†ìŒ

---

#### âœ… `cached_load_from_s3(s3_path: str)` - Lines 210-249 (New!)
```python
def cached_load_from_s3(s3_path: str) -> Any:
    # Lambda Cold Start ë™ì•ˆ ìºì‹œ ìœ ì§€ (5ë¶„ TTL)
    # ìµœëŒ€ 20ê°œ í•­ëª© ìºì‹œ
    
    if s3_path in _s3_cache:
        cache_time = _cache_timestamps.get(s3_path, 0)
        if current_time - cache_time < CACHE_TTL_SECONDS:
            logger.debug(f"Cache hit for {s3_path}")
            return _s3_cache[s3_path]
    
    data = load_from_s3(s3_path)
    
    if data is not None:
        # ìºì‹œì— ì €ì¥ (ìµœëŒ€ 20ê°œ)
        _s3_cache[s3_path] = data
```

**ì¥ì **:
- âœ… S3 GET ìš”ì²­ 30% ê°ì†Œ ì˜ˆìƒ
- âœ… Lambda ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (ìµœëŒ€ 20ê°œ)
- âœ… TTLë¡œ stale data ë°©ì§€

**ì´ìŠˆ**:
- âš ï¸ load_from_s3 ì‹¤íŒ¨ ì‹œ ìºì‹œë„ ì‹¤íŒ¨

---

#### âœ… `decompress_data(compressed_str: str)` - Lines 68-77
```python
def decompress_data(compressed_str: str) -> Any:
    try:
        compressed = base64.b64decode(compressed_str.encode('utf-8'))
        decompressed = gzip.decompress(compressed)
        return json.loads(decompressed.decode('utf-8'))
    except Exception as e:
        logger.error(f"Failed to decompress data: {e}")
        raise  # âš ï¸ ì˜ˆì™¸ ì „íŒŒ (ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨)
```

**ì´ìŠˆ**:
- âœ… ì••ì¶• í•´ì œ ê¸°ë³¸ êµ¬í˜„
- âš ï¸ **ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ì „íŒŒ** â†’ ì›Œí¬í”Œë¡œìš° ì¤‘ë‹¨

---

### 2.2 ë³µêµ¬ ë¡œì§ ì‚¬ìš© ìœ„ì¹˜

| ìœ„ì¹˜ | ë³µêµ¬ ëŒ€ìƒ | í•¨ìˆ˜ | Fallback |
|------|----------|------|----------|
| `aggregate_branches()` | branch_data (S3) | `cached_load_from_s3()` | âŒ None ë°˜í™˜ ì‹œ ë¡œê·¸ ëˆ„ë½ |
| `update_and_compress()` | - (ì €ì¥ë§Œ ìˆ˜í–‰) | - | N/A |
| `sync_state_data()` | - (ì €ì¥ë§Œ ìˆ˜í–‰) | - | N/A |
| ExecuteSegment Lambda | current_state (S3) | `load_from_s3()` ì¶”ì • | âš ï¸ ë¯¸í™•ì¸ |

**ê²°ë¡ **: **ë³µêµ¬ ë¡œì§ì€ ì¡´ì¬í•˜ë‚˜ Fallback ë©”ì»¤ë‹ˆì¦˜ ë¶€ì¬**

---

## 3ï¸âƒ£ í¬ì¸í„° í¬ê¸° ë¶„ì„

### 3.1 S3 í¬ì¸í„° êµ¬ì¡°

#### ê°œë³„ í•„ë“œ ì˜¤í”„ë¡œë”© (optimize_current_state)
```python
{
    "type": "s3_reference",
    "s3_path": "s3://bucket/workflow-state/key/state_field_20260129.json",
    "size_kb": 45,
    "stored_at": "2026-01-29T10:30:45.123456Z"
}
```

**í¬ê¸°**: ~150 bytes

---

#### ì „ì²´ ìƒíƒœ ì˜¤í”„ë¡œë”© (full_state)
```python
{
    "__s3_offloaded": True,
    "__s3_path": "s3://bucket/workflow-state/key/full_state_20260129.json",
    "__original_size_kb": 120,
    "guardrail_verified": True,
    "batch_count_actual": 5,
    "scheduling_metadata": {...},  # âš ï¸ ì´ê²ƒë„ ì»¤ì§ˆ ìˆ˜ ìˆìŒ
    "__scheduling_metadata": {...},
    "__guardrail_verified": True,
    "__batch_count_actual": 5
}
```

**í¬ê¸°**: ~300-500 bytes (scheduling_metadataì— ë”°ë¼)

**âš ï¸ ë¦¬ìŠ¤í¬**: `scheduling_metadata`ê°€ í¬ë©´ í¬ì¸í„°ë„ ë¹„ëŒ€í•´ì§

---

#### íˆìŠ¤í† ë¦¬ ì•„ì¹´ì´ë¸Œ ì°¸ì¡°
```python
{
    "type": "history_archive",
    "s3_path": "s3://bucket/workflow-state/key/history_archive_20260129.json",
    "entry_count": 100,
    "archived_at": "2026-01-29T10:30:45.123456Z"
}
```

**í¬ê¸°**: ~150 bytes

---

#### Map ResultSelector í¬ì¸í„° (v3 ASL)
```python
{
    "branch_id": "branch_0",
    "status": "COMPLETE",
    "s3_path": "s3://bucket/final_state_path.json"
}
```

**í¬ê¸°**: ~100 bytes per branch

**âš ï¸ ë¦¬ìŠ¤í¬**: 20+ ë¸Œëœì¹˜ ì‹œ 2KB+, 100+ ë¸Œëœì¹˜ ì‹œ 10KB+

---

### 3.2 í¬ì¸í„° ë¹„ëŒ€í™” ë¦¬ìŠ¤í¬ ì‹œë‚˜ë¦¬ì˜¤

#### ğŸš¨ **Scenario 1: Distributed Map with 100 Chunks**
```python
# aggregate_distributed_resultsì—ì„œ ìƒì„±
'distributed_chunk_summary': {
    'total': 100,
    'succeeded': 95,
    'failed': 5,
    'chunk_results': [
        {'chunk_id': 0, 'status': 'COMPLETE', 's3_path': '...', 'execution_order': 0},
        {'chunk_id': 1, 'status': 'COMPLETE', 's3_path': '...', 'execution_order': 1},
        # ... ìµœëŒ€ 10ê°œë§Œ ì €ì¥ (í˜„ì¬ êµ¬í˜„)
    ]
}
```

**í˜„ì¬ í¬ê¸°**: ~1.5KB (10ê°œ ì œí•œ)
**âš ï¸ ë§Œì•½ ì œí•œ ì—†ìœ¼ë©´**: ~15KB (100ê°œ ì „ì²´)

**ê²°ë¡ **: âœ… **í˜„ì¬ëŠ” ì•ˆì „** (10ê°œ ì œí•œ)

---

#### ğŸš¨ **Scenario 2: Parallel Branches with 50 Branches**
```python
# ProcessParallelBranches ResultSelector (v3 ASL)
"branches": [
    {"branch_id": "branch_0", "status": "COMPLETE", "s3_path": "..."},
    {"branch_id": "branch_1", "status": "COMPLETE", "s3_path": "..."},
    # ... 50ê°œ
]
```

**í¬ì¸í„° í¬ê¸°**: 50 Ã— 100 bytes = **5KB**

**âš ï¸ ë¦¬ìŠ¤í¬**: ë¸Œëœì¹˜ ìˆ˜ ì¦ê°€ ì‹œ ì„ í˜• ì¦ê°€

---

#### ğŸš¨ **Scenario 3: Full State Offload with Large Metadata**
```python
{
    "__s3_offloaded": True,
    "__s3_path": "s3://...",
    "scheduling_metadata": {
        "batch_details": [
            {"batch_id": 0, "size": 100, "priority": 1, ...},
            {"batch_id": 1, "size": 100, "priority": 2, ...},
            # ... 20ê°œ ë°°ì¹˜
        ]
    }
}
```

**í¬ì¸í„° í¬ê¸°**: ~10-20KB

**âš ï¸ ë¦¬ìŠ¤í¬**: **í¬ì¸í„° ìì²´ê°€ ë¹„ëŒ€í•´ì ¸ 256KBì— ê·¼ì ‘**

---

### 3.3 ì œì–´ í•„ë“œ ì œì™¸ ëª©ë¡

```python
# Lines 190-203
CONTROL_FIELDS_NEVER_OFFLOAD = {
    'execution_id',
    'segment_to_run',
    'loop_counter',
    'next_action',
    'status',
    'idempotency_key',
    'state_s3_path',
    'pre_snapshot_s3_path',
    'post_snapshot_s3_path',
    'last_update_time',
    'payload_size_kb'
}
```

**âœ… ì¥ì **: ì œì–´ íë¦„ì— í•„ìˆ˜ì ì¸ í•„ë“œëŠ” ì˜¤í”„ë¡œë”© ì•ˆ í•¨

**âš ï¸ ëˆ„ë½**: `scheduling_metadata`, `batch_count_actual`, `guardrail_verified` ë“±ì€ **ì œì™¸ ëª©ë¡ì— ì—†ìŒ** â†’ ì˜¤í”„ë¡œë”© ê°€ëŠ¥

---

## 4ï¸âƒ£ ë°œê²¬ëœ ì´ìŠˆ ë° ê¶Œì¥ ì‚¬í•­

### ğŸš¨ P0: aggregate_distributed_resultsì— ì˜¤í”„ë¡œë”© ë¯¸ì ìš©

**ë¬¸ì œ**:
```python
# Lines 845-897 (í˜„ì¬ êµ¬í˜„)
updated_state['distributed_chunk_summary'] = {
    'total': len(execution_result),
    'succeeded': len(all_outputs),
    'failed': len(failed_segments),
    'chunk_results': chunk_results[:10]  # ì²˜ìŒ 10ê°œë§Œ
}
```

**ë¦¬ìŠ¤í¬**: 
- ì‹¤íŒ¨í•œ ì²­í¬ ì •ë³´(`failed_segments`)ê°€ ë§ìœ¼ë©´ 256KB ì´ˆê³¼ ê°€ëŠ¥
- ì „ì²´ `execution_result` ë°°ì—´ì´ ì—¬ì „íˆ ë©”ëª¨ë¦¬ì— ìˆìŒ

**í•´ê²°ì±…**:
```python
# failed_segmentsë„ S3ë¡œ ì˜¤í”„ë¡œë“œ
if len(failed_segments) > 5:
    failed_s3_path = store_to_s3(failed_segments, 
        generate_s3_key(execution_id, 'failed_segments'))
    updated_state['failed_segments_s3_path'] = failed_s3_path
    updated_state['failed_segments'] = failed_segments[:5]  # ìƒ˜í”Œë§Œ
```

---

### âš ï¸ P1: ë³µêµ¬ ì‹¤íŒ¨ ì‹œ Fallback ë¶€ì¬

**ë¬¸ì œ**:
```python
def load_from_s3(s3_path: str) -> Any:
    # ...
    except Exception as e:
        logger.error(f"Failed to load from S3 {s3_path}: {e}")
        return None  # âš ï¸ ë°ì´í„° ì†ì‹¤
```

**ë¦¬ìŠ¤í¬**:
- S3 ì¼ì‹œì  ì¥ì•  ì‹œ ì›Œí¬í”Œë¡œìš° ë°ì´í„° ì†ì‹¤
- aggregate_branchesì—ì„œ None ë°˜í™˜ ì‹œ ë¸Œëœì¹˜ ê²°ê³¼ ëˆ„ë½

**í•´ê²°ì±…**:
```python
def load_from_s3(s3_path: str, max_retries: int = 3) -> Any:
    for attempt in range(max_retries):
        try:
            # ... ê¸°ì¡´ ë¡œì§
            return json.loads(content)
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
                continue
            logger.error(f"Failed to load after {max_retries} attempts: {e}")
            return None
```

---

### âš ï¸ P1: í¬ì¸í„° ë¹„ëŒ€í™” (Scenario 3)

**ë¬¸ì œ**:
```python
# optimize_current_state - Lines 366-377
wrapper = {
    "__s3_offloaded": True,
    "__s3_path": s3_path,
    "__original_size_kb": final_size_kb,
    "guardrail_verified": optimized_state.get('guardrail_verified', False),
    "batch_count_actual": optimized_state.get('batch_count_actual', 1),
    "scheduling_metadata": optimized_state.get('scheduling_metadata', {}),  # âš ï¸
}
```

**ë¦¬ìŠ¤í¬**: `scheduling_metadata` ìì²´ê°€ í¬ë©´ í¬ì¸í„°ë„ ë¹„ëŒ€í•´ì§

**í•´ê²°ì±…**:
```python
# scheduling_metadataë„ ê°„ì†Œí™”
wrapper = {
    "__s3_offloaded": True,
    "__s3_path": s3_path,
    "__original_size_kb": final_size_kb,
    # ì œì–´ í•„ë“œë§Œ ë³´ì¡´
    "guardrail_verified": optimized_state.get('guardrail_verified', False),
    "batch_count_actual": optimized_state.get('batch_count_actual', 1),
    # scheduling_metadataëŠ” ìš”ì•½ë§Œ
    "scheduling_summary": {
        "total_batches": len(optimized_state.get('scheduling_metadata', {}).get('batch_details', [])),
        "priority": optimized_state.get('scheduling_metadata', {}).get('priority', 1)
    }
}
```

---

### âš ï¸ P2: create_snapshot ìµœì í™” ë¶€ì¬

**ë¬¸ì œ**:
```python
# Lines 958-1012
snapshot_data = {
    'snapshot_id': snapshot_id,
    'snapshot_type': snapshot_type,
    'execution_id': execution_id,
    'created_at': datetime.now().isoformat(),
    'state_data': state_data,  # âš ï¸ ì „ì²´ state_data ì €ì¥
    'segment_to_run': state_data.get('segment_to_run', 0),
    'loop_counter': state_data.get('loop_counter', 0)
}
```

**ë¦¬ìŠ¤í¬**: state_dataê°€ í¬ë©´ ìŠ¤ëƒ…ìƒ·ë„ ì»¤ì§ (ë¶ˆí•„ìš”í•œ ë³µì œ)

**í•´ê²°ì±…**:
```python
# state_dataê°€ ì´ë¯¸ S3ì— ìˆìœ¼ë©´ ê²½ë¡œë§Œ ì°¸ì¡°
snapshot_data = {
    'snapshot_id': snapshot_id,
    'snapshot_type': snapshot_type,
    'execution_id': execution_id,
    'created_at': datetime.now().isoformat(),
    'state_s3_path': state_data.get('state_s3_path'),  # í¬ì¸í„°ë§Œ
    'segment_to_run': state_data.get('segment_to_run', 0),
    'loop_counter': state_data.get('loop_counter', 0)
}
```

---

## 5ï¸âƒ£ ìµœì¢… í‰ê°€

### âœ… ê°•ì 
1. **ì˜¤í”„ë¡œë”© ì»¤ë²„ë¦¬ì§€**: ì£¼ìš” 5ê°œ ê²½ë¡œ ëª¨ë‘ êµ¬í˜„
2. **ìºì‹± ìµœì í™”**: cached_load_from_s3ë¡œ S3 ë¹„ìš© ì ˆê°
3. **ê³„ì¸µì  ì˜¤í”„ë¡œë”©**: ê°œë³„ í•„ë“œ â†’ ì „ì²´ ìƒíƒœ â†’ ì••ì¶•
4. **í¬ì¸í„° í¬ê¸° ê´€ë¦¬**: chunk_results 10ê°œ ì œí•œ ë“±

### âš ï¸ ì•½ì 
1. **ë³µêµ¬ Fallback ë¶€ì¬**: ì¬ì‹œë„ ì—†ìŒ, ì‹¤íŒ¨ ì‹œ None
2. **ì¼ë¶€ ê²½ë¡œ ë¯¸ì ìš©**: aggregate_distributed, create_snapshot
3. **í¬ì¸í„° ë¹„ëŒ€í™” ë¦¬ìŠ¤í¬**: scheduling_metadata ì œì™¸ ì•ˆ ë¨
4. **ì—ëŸ¬ ì²˜ë¦¬ ë¶ˆì™„ì „**: decompress_dataëŠ” ì˜ˆì™¸ ì „íŒŒ

### ğŸ“Š ì ìˆ˜

| í•­ëª© | ì ìˆ˜ | ìƒì„¸ |
|------|------|------|
| **ì˜¤í”„ë¡œë”© ì ìš©ë¥ ** | 85/100 | ì£¼ìš” ê²½ë¡œ êµ¬í˜„, ì¼ë¶€ ê²½ë¡œ ë¯¸ì ìš© |
| **ë³µêµ¬ ë¡œì§** | 70/100 | ê¸°ë³¸ êµ¬í˜„, Fallback ë¶€ì¬ |
| **í¬ì¸í„° ìµœì í™”** | 80/100 | ëŒ€ë¶€ë¶„ ì ì ˆ, ì¼ë¶€ ë¦¬ìŠ¤í¬ |
| **ì—ëŸ¬ ì²˜ë¦¬** | 65/100 | ë¡œê¹…ì€ ì¶©ë¶„, ì¬ì‹œë„ ë¶€ì¬ |
| **ì „ì²´ ì•ˆì •ì„±** | 75/100 | í”„ë¡œë•ì…˜ ì‚¬ìš© ê°€ëŠ¥, ê°œì„  í•„ìš” |

---

## 6ï¸âƒ£ ê¶Œì¥ ê°œì„  ì‚¬í•­ (ìš°ì„ ìˆœìœ„ë³„)

### ğŸ”´ P0 (ì¦‰ì‹œ ìˆ˜ì •)
1. **aggregate_distributedì— ì˜¤í”„ë¡œë”© ì¶”ê°€**
2. **load_from_s3ì— ì¬ì‹œë„ ë¡œì§ ì¶”ê°€**

### ğŸŸ¡ P1 (ë‹¨ê¸°)
3. **í¬ì¸í„° ë¹„ëŒ€í™” ë°©ì§€**: scheduling_metadata ê°„ì†Œí™”
4. **create_snapshot ìµœì í™”**: state_s3_path ì°¸ì¡°ë§Œ ì €ì¥

### ğŸŸ¢ P2 (ì¤‘ê¸°)
5. **decompress_data Fallback**: ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ëŒ€ì‹  ì›ë³¸ ë°˜í™˜
6. **merge_callback/merge_async í¬ê¸° ì²´í¬**: ëŒ€ìš©ëŸ‰ ë°©ì–´

---

**ì‘ì„±ì**: GitHub Copilot (Claude Sonnet 4.5)
**ë‹¤ìŒ ë‹¨ê³„**: P0 ì´ìŠˆ ìˆ˜ì • PR ìƒì„±
