{
    "workflow_name": "LLM Stage 6 - Distributed MAP_REDUCE + Loop + HITL",
    "description": "ë¶„ì‚° MAP_REDUCE ëª¨ë“œì—ì„œ LLM í˜¸ì¶œ, Loop ë°˜ë³µ, HITL ìŠ¹ì¸ í†µí•© ê²€ì¦",
    "version": "2.1.0",
    "test_stage": 6,
    "nodes": [
        {
            "id": "init_distributed",
            "type": "operator",
            "config": {
                "code": "print('ğŸš€ Stage 6: Distributed MAP_REDUCE + Loop + HITL ì´ˆê¸°í™”')\nimport time\n\n# ë¶„ì‚° ì²˜ë¦¬ ì„¤ì •\nstate['distributed_mode'] = True\nstate['distributed_strategy'] = 'MAP_REDUCE'\n\n# 20KB ë”ë¯¸ ë°ì´í„° ìƒì„± (S3 ì˜¤í”„ë¡œë”© íŠ¸ë¦¬ê±°ìš©)\ndef generate_dummy_content(size_kb=20):\n    base = 'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. '\n    return (base * (size_kb * 1024 // len(base) + 1))[:size_kb * 1024]\n\n# íŒŒí‹°ì…˜ ë°ì´í„° ìƒì„± (12ê°œ ì•„ì´í…œ â†’ 3ê°œ íŒŒí‹°ì…˜, ê° 20KB)\npartition_count = state.get('partition_count', 3)\nitems_per_partition = state.get('items_per_partition', 4)\ndummy_size_kb = state.get('dummy_content_size_kb', 20)\n\nall_items = []\nfor i in range(partition_count * items_per_partition):\n    all_items.append({\n        'item_id': f'item_{i}',\n        'content': f'Document {i}: ' + generate_dummy_content(dummy_size_kb),\n        'partition_id': i // items_per_partition\n    })\n\nstate['all_items'] = all_items\nstate['partition_map'] = {}\nfor p_id in range(partition_count):\n    state['partition_map'][f'partition_{p_id}'] = [\n        item for item in all_items if item['partition_id'] == p_id\n    ]\n\nstate['partition_results'] = []\nstate['loop_iterations'] = []\nstate['hitl_events'] = []\nstate['providers_used'] = []\n# NOTE: ì»¤ë„ aggregatorê°€ ìë™ ì§‘ê³„í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì´ˆê¸°í™”ë§Œ\nstate['test_start_time'] = int(time.time() * 1000)\n\ntotal_size_kb = len(all_items) * dummy_size_kb\nprint(f'âœ… íŒŒí‹°ì…˜ ìƒì„± ì™„ë£Œ: {partition_count}ê°œ íŒŒí‹°ì…˜, ì´ {len(all_items)}ê°œ ì•„ì´í…œ (~{total_size_kb}KB)')",
                "output_key": "init_result"
            }
        },
        {
            "id": "distributed_map_processor",
            "type": "for_each",
            "config": {
                "items_key": "partition_map",
                "item_variable": "current_partition",
                "output_key": "partition_results",
                "max_concurrency": 3,
                "nodes": [
                    {
                        "id": "partition_init",
                        "type": "operator",
                        "config": {
                            "code": "import time\n\npartition_key = list(state.get('current_partition', {}).keys())[0] if isinstance(state.get('current_partition'), dict) else 'partition_0'\npartition_items = state.get('current_partition', {}).get(partition_key, [])\npartition_id = int(partition_key.split('_')[-1]) if '_' in partition_key else 0\n\nstate['current_partition_id'] = partition_id\nstate['current_partition_items'] = partition_items\nstate['partition_start_time'] = int(time.time() * 1000)\nstate['partition_loop_count'] = 0\nstate['partition_llm_results'] = []\n\nprint(f'ğŸ“¦ íŒŒí‹°ì…˜ {partition_id} ì²˜ë¦¬ ì‹œì‘: {len(partition_items)}ê°œ ì•„ì´í…œ')",
                            "output_key": "partition_init_result"
                        }
                    },
                    {
                        "id": "partition_loop",
                        "type": "loop",
                        "config": {
                            "max_iterations": 2,
                            "condition": "state.get('loop_convergence_score', 0) < state.get('loop_convergence_threshold', 0.8)",
                            "nodes": [
                                {
                                    "id": "llm_analyze_partition",
                                    "type": "aiModel",
                                    "config": {
                                        "model": "gemini-2.0-flash",
                                        "provider": "gemini",
                                        "prompt_template": "Analyze the following documents and extract key insights:\n\n{% for item in state.current_partition_items %}\n- {{ item.content }}\n{% endfor %}\n\nProvide a structured JSON response with:\n1. main_themes: List of main themes\n2. sentiment: Overall sentiment (positive/neutral/negative)\n3. key_entities: Important entities mentioned\n4. confidence_score: Your confidence in this analysis (0-1)",
                                        "response_schema": {
                                            "type": "object",
                                            "properties": {
                                                "main_themes": {"type": "array", "items": {"type": "string"}},
                                                "sentiment": {"type": "string", "enum": ["positive", "neutral", "negative"]},
                                                "key_entities": {"type": "array", "items": {"type": "string"}},
                                                "confidence_score": {"type": "number"}
                                            },
                                            "required": ["main_themes", "sentiment", "confidence_score"]
                                        },
                                        "output_key": "llm_analysis_raw"
                                    }
                                },
                                {
                                    "id": "update_loop_state",
                                    "type": "operator",
                                    "config": {
                                        "code": "import json\n\npartition_id = state.get('current_partition_id', 0)\nloop_count = state.get('partition_loop_count', 0) + 1\nstate['partition_loop_count'] = loop_count\n\n# LLM ê²°ê³¼ íŒŒì‹±\nllm_raw = state.get('llm_analysis_raw', '{}')\ntry:\n    if isinstance(llm_raw, str):\n        llm_result = json.loads(llm_raw)\n    else:\n        llm_result = llm_raw\nexcept:\n    llm_result = {'confidence_score': 0.5}\n\n# ìˆ˜ë ´ ì ìˆ˜ ì—…ë°ì´íŠ¸\nconfidence = llm_result.get('confidence_score', 0.5)\nstate['loop_convergence_score'] = confidence\n\n# LLM ê²°ê³¼ ì €ì¥\nstate['partition_llm_results'].append({\n    'iteration': loop_count,\n    'confidence': confidence,\n    'themes': llm_result.get('main_themes', []),\n    'sentiment': llm_result.get('sentiment', 'neutral')\n})\n\n# Provider ì¶”ì \nusage = state.get('usage', {})\nprovider = usage.get('provider', 'unknown')\nif provider not in state.get('providers_used', []):\n    state.setdefault('providers_used', []).append(provider)\n\nstate['loop_iterations'].append({\n    'partition_id': partition_id,\n    'iteration': loop_count,\n    'llm_called': True,\n    'confidence': confidence\n})\n\nprint(f'ğŸ”„ íŒŒí‹°ì…˜ {partition_id} Loop {loop_count}: confidence={confidence:.2f}')",
                                        "output_key": "loop_update_result"
                                    }
                                }
                            ]
                        }
                    },
                    {
                        "id": "check_hitl_trigger",
                        "type": "operator",
                        "config": {
                            "code": "partition_id = state.get('current_partition_id', 0)\nhitl_checkpoint = state.get('hitl_checkpoint_at_partition', 1)\n\nif partition_id == hitl_checkpoint and state.get('hitl_enabled', True):\n    state['hitl_required'] = True\n    state['pre_hitl_state_keys'] = list(state.keys())\n    print(f'â¸ï¸ íŒŒí‹°ì…˜ {partition_id}ì—ì„œ HITL ì²´í¬í¬ì¸íŠ¸ ë°œë™')\nelse:\n    state['hitl_required'] = False\n    print(f'âœ… íŒŒí‹°ì…˜ {partition_id}: HITL ë¶ˆí•„ìš”, ê³„ì† ì§„í–‰')",
                            "output_key": "hitl_check_result"
                        }
                    },
                    {
                        "id": "hitl_approval",
                        "type": "hitl",
                        "config": {
                            "condition": "state.get('hitl_required', False)",
                            "question": "íŒŒí‹°ì…˜ ë¶„ì„ ê²°ê³¼ë¥¼ ìŠ¹ì¸í•˜ì‹œê² ìŠµë‹ˆê¹Œ?",
                            "context_keys": ["current_partition_id", "partition_llm_results", "loop_convergence_score"],
                            "auto_approve": true,
                            "auto_approve_delay_ms": 100,
                            "on_complete": {
                                "code": "partition_id = state.get('current_partition_id', 0)\nstate['post_hitl_state_keys'] = list(state.keys())\nstate['hitl_events'].append({\n    'partition_id': partition_id,\n    'pre_hitl_state_keys': state.get('pre_hitl_state_keys', []),\n    'post_hitl_state_keys': state.get('post_hitl_state_keys', []),\n    'decision': 'approved'\n})\nprint(f'âœ… íŒŒí‹°ì…˜ {partition_id} HITL ìŠ¹ì¸ ì™„ë£Œ')"
                            }
                        }
                    },
                    {
                        "id": "finalize_partition",
                        "type": "operator",
                        "config": {
                            "code": "import time\n\npartition_id = state.get('current_partition_id', 0)\nend_time = int(time.time() * 1000)\nstart_time = state.get('partition_start_time', end_time)\n\n# íŒŒí‹°ì…˜ ê²°ê³¼ ì§‘ê³„ - ì»¤ë„ aggregatorê°€ ìë™ í•©ì‚°í•˜ë¯€ë¡œ ê°œë³„ usageë§Œ í¬í•¨\nusage = state.get('usage', {})\nresult = {\n    'partition_id': partition_id,\n    'status': 'completed',\n    'loop_iterations': state.get('partition_loop_count', 0),\n    'final_confidence': state.get('loop_convergence_score', 0),\n    'llm_results': state.get('partition_llm_results', []),\n    'usage': usage,  # ê°œë³„ usage - ì»¤ë„ì´ í‰íƒ„í™”(Flatten) í›„ í•©ì‚°\n    'latency_ms': end_time - start_time\n}\n\n# âš ï¸ race condition ë°©ì§€: aggregated_usage ì§ì ‘ ìˆ˜ì • ê¸ˆì§€!\n# ì»¤ë„ ë ˆë²¨ aggregatorê°€ partition_results[].usageë¥¼ ìë™ ì§‘ê³„í•¨\n\nprint(f'âœ… íŒŒí‹°ì…˜ {partition_id} ì™„ë£Œ: {result[\"loop_iterations\"]} iterations, confidence={result[\"final_confidence\"]:.2f}')\nstate['partition_final_result'] = result",
                            "output_key": "partition_result"
                        }
                    }
                ]
            }
        },
        {
            "id": "merge_partition_results",
            "type": "operator",
            "config": {
                "code": "import time\n\npartition_results = state.get('partition_results', [])\n\n# ê²°ê³¼ ë³‘í•©\nall_themes = []\nall_sentiments = []\ntotal_confidence = 0\n\nfor p_result in partition_results:\n    if isinstance(p_result, dict):\n        for llm_result in p_result.get('llm_results', []):\n            all_themes.extend(llm_result.get('themes', []))\n            all_sentiments.append(llm_result.get('sentiment', 'neutral'))\n        total_confidence += p_result.get('final_confidence', 0)\n\n# ì¤‘ë³µ ì œê±°\nunique_themes = list(set(all_themes))\n\n# ìµœì¢… sentiment ê²°ì •\nsentiment_counts = {}\nfor s in all_sentiments:\n    sentiment_counts[s] = sentiment_counts.get(s, 0) + 1\nfinal_sentiment = max(sentiment_counts, key=sentiment_counts.get) if sentiment_counts else 'neutral'\n\n# í‰ê·  confidence\navg_confidence = total_confidence / len(partition_results) if partition_results else 0\nstate['loop_convergence_score'] = avg_confidence\n\nstate['merged_output'] = {\n    'unique_themes': unique_themes,\n    'final_sentiment': final_sentiment,\n    'average_confidence': avg_confidence,\n    'partition_count': len(partition_results)\n}\n\nstate['final_merged_result'] = state['merged_output']\n\nend_time = int(time.time() * 1000)\nstate['test_end_time'] = end_time\nstate['total_latency_ms'] = end_time - state.get('test_start_time', end_time)\n\nprint(f'âœ… MAP_REDUCE ë³‘í•© ì™„ë£Œ: {len(unique_themes)} themes, sentiment={final_sentiment}, confidence={avg_confidence:.2f}')",
                "output_key": "merge_result"
            }
        },
        {
            "id": "final_verification",
            "type": "operator",
            "config": {
                "code": "partition_results = state.get('partition_results', [])\ncompleted = len([p for p in partition_results if isinstance(p, dict) and p.get('status') == 'completed'])\nexpected = state.get('partition_count', 3)\n\nhitl_count = len(state.get('hitl_events', []))\nloop_iterations = len(state.get('loop_iterations', []))\n\n# ì»¤ë„ aggregatorê°€ ì§‘ê³„í•œ usage ì‚¬ìš© (ê°œë³„ partition ê²°ê³¼ì—ì„œ í‰íƒ„í™”)\ntotal_tokens = 0\ntotal_cost = 0\nfor p in partition_results:\n    if isinstance(p, dict):\n        usage = p.get('usage', {})\n        total_tokens += usage.get('total_tokens', usage.get('input_tokens', 0) + usage.get('output_tokens', 0))\n        total_cost += usage.get('cost', 0)\n\n# S3 ì˜¤í”„ë¡œë”© ì—¬ë¶€ í™•ì¸\ns3_offloaded = state.get('_s3_offload_triggered', False)\nstate_size_kb = len(str(state)) // 1024\n\nif completed >= expected:\n    state['TEST_RESULT'] = f'âœ… STAGE6 SUCCESS: {completed}/{expected} partitions, {loop_iterations} iters, {hitl_count} HITLs, {total_tokens} tokens, ${total_cost:.4f}, S3={s3_offloaded} ({state_size_kb}KB)'\nelse:\n    state['TEST_RESULT'] = f'âŒ STAGE6 PARTIAL: {completed}/{expected} partitions completed'\n\nprint(state['TEST_RESULT'])",
                "output_key": "verification_result"
            }
        }
    ],
    "edges": [
        {"source": "init_distributed", "target": "distributed_map_processor"},
        {"source": "distributed_map_processor", "target": "merge_partition_results"},
        {"source": "merge_partition_results", "target": "final_verification"}
    ],
    "initial_state": {
        "test_stage": 6,
        "distributed_mode": true,
        "distributed_strategy": "MAP_REDUCE",
        "partition_count": 3,
        "items_per_partition": 4,
        "max_concurrency": 3,
        "loop_convergence_threshold": 0.8,
        "hitl_enabled": true,
        "hitl_checkpoint_at_partition": 1,
        "cost_guardrail_enabled": true,
        "max_tokens_total": 50000,
        "dummy_content_size_kb": 20,
        "s3_offload_threshold_kb": 180,
        "verify_s3_offload": true
    }
}
