{
    "workflow_name": "LLM Stage 6 - Distributed MAP_REDUCE + Loop + HITL",
    "description": "Î∂ÑÏÇ∞ MAP_REDUCE Î™®ÎìúÏóêÏÑú LLM Ìò∏Ï∂ú, Loop Î∞òÎ≥µ, HITL ÏäπÏù∏ ÌÜµÌï© Í≤ÄÏ¶ù",
    "version": "2.1.0",
    "test_stage": 6,
    "nodes": [
        {
            "id": "distributed_map_processor",
            "type": "for_each",
            "config": {
                "items_key": "partition_map",
                "item_variable": "current_partition",
                "output_key": "partition_results",
                "max_concurrency": 3,
                "nodes": [
                    {
                        "id": "partition_init",
                        "type": "operator",
                        "config": {
                            "code": "import time\n\npartition_key = list(state.get('current_partition', {}).keys())[0] if isinstance(state.get('current_partition'), dict) else 'partition_0'\npartition_items = state.get('current_partition', {}).get(partition_key, [])\npartition_id = int(partition_key.split('_')[-1]) if '_' in partition_key else 0\n\nstate['current_partition_id'] = partition_id\nstate['current_partition_items'] = partition_items\nstate['partition_start_time'] = int(time.time() * 1000)\nstate['partition_loop_count'] = 0\nstate['partition_llm_results'] = []\n\nprint(f'üì¶ ÌååÌã∞ÏÖò {partition_id} Ï≤òÎ¶¨ ÏãúÏûë: {len(partition_items)}Í∞ú ÏïÑÏù¥ÌÖú')",
                            "output_key": "partition_init_result"
                        }
                    },
                    {
                        "id": "partition_loop",
                        "type": "loop",
                        "config": {
                            "max_iterations": 2,
                            "condition": "state.get('loop_convergence_score', 0) < state.get('loop_convergence_threshold', 0.8)",
                            "nodes": [
                                {
                                    "id": "llm_analyze_partition",
                                    "type": "aiModel",
                                    "config": {
                                        "model": "gemini-2.0-flash",
                                        "provider": "gemini",
                                        "prompt_template": "Analyze the following documents and extract key insights:\n\n{% for item in state.current_partition_items %}\n- {{ item.content }}\n{% endfor %}\n\nProvide a structured JSON response with:\n1. main_themes: List of main themes\n2. sentiment: Overall sentiment (positive/neutral/negative)\n3. key_entities: Important entities mentioned\n4. confidence_score: Your confidence in this analysis (0-1)",
                                        "response_schema": {
                                            "type": "object",
                                            "properties": {
                                                "main_themes": {
                                                    "type": "array",
                                                    "items": {
                                                        "type": "string"
                                                    }
                                                },
                                                "sentiment": {
                                                    "type": "string",
                                                    "enum": [
                                                        "positive",
                                                        "neutral",
                                                        "negative"
                                                    ]
                                                },
                                                "key_entities": {
                                                    "type": "array",
                                                    "items": {
                                                        "type": "string"
                                                    }
                                                },
                                                "confidence_score": {
                                                    "type": "number"
                                                }
                                            },
                                            "required": [
                                                "main_themes",
                                                "sentiment",
                                                "confidence_score"
                                            ]
                                        },
                                        "output_key": "llm_analysis_raw"
                                    }
                                },
                                {
                                    "id": "update_loop_state",
                                    "type": "operator",
                                    "config": {
                                        "code": "import json\n\npartition_id = state.get('current_partition_id', 0)\nloop_count = state.get('partition_loop_count', 0) + 1\nstate['partition_loop_count'] = loop_count\n\n# LLM Í≤∞Í≥º ÌååÏã±\nllm_raw = state.get('llm_analysis_raw', '{}')\ntry:\n    if isinstance(llm_raw, str):\n        llm_result = json.loads(llm_raw)\n    else:\n        llm_result = llm_raw\nexcept:\n    llm_result = {'confidence_score': 0.5}\n\n# ÏàòÎ†¥ Ï†êÏàò ÏóÖÎç∞Ïù¥Ìä∏\nconfidence = llm_result.get('confidence_score', 0.5)\nstate['loop_convergence_score'] = confidence\n\n# LLM Í≤∞Í≥º Ï†ÄÏû•\nstate['partition_llm_results'].append({\n    'iteration': loop_count,\n    'confidence': confidence,\n    'themes': llm_result.get('main_themes', []),\n    'sentiment': llm_result.get('sentiment', 'neutral')\n})\n\n# Provider Ï∂îÏ†Å\nusage = state.get('usage', {})\nprovider = usage.get('provider', 'unknown')\nif provider not in state.get('providers_used', []):\n    state.setdefault('providers_used', []).append(provider)\n\nstate['loop_iterations'].append({\n    'partition_id': partition_id,\n    'iteration': loop_count,\n    'llm_called': True,\n    'confidence': confidence\n})\n\nprint(f'üîÑ ÌååÌã∞ÏÖò {partition_id} Loop {loop_count}: confidence={confidence:.2f}')",
                                        "output_key": "loop_update_result"
                                    }
                                }
                            ]
                        }
                    },
                    {
                        "id": "check_hitl_trigger",
                        "type": "operator",
                        "config": {
                            "code": "partition_id = state.get('current_partition_id', 0)\nhitl_checkpoint = state.get('hitl_checkpoint_at_partition', 1)\n\nif partition_id == hitl_checkpoint and state.get('hitl_enabled', True):\n    state['hitl_required'] = True\n    state['pre_hitl_state_keys'] = list(state.keys())\n    print(f'‚è∏Ô∏è ÌååÌã∞ÏÖò {partition_id}ÏóêÏÑú HITL Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î∞úÎèô')\nelse:\n    state['hitl_required'] = False\n    print(f'‚úÖ ÌååÌã∞ÏÖò {partition_id}: HITL Î∂àÌïÑÏöî, Í≥ÑÏÜç ÏßÑÌñâ')",
                            "output_key": "hitl_check_result"
                        }
                    },
                    {
                        "id": "hitl_approval",
                        "type": "hitl",
                        "config": {
                            "condition": "state.get('hitl_required', False)",
                            "question": "ÌååÌã∞ÏÖò Î∂ÑÏÑù Í≤∞Í≥ºÎ•º ÏäπÏù∏ÌïòÏãúÍ≤†ÏäµÎãàÍπå?",
                            "context_keys": [
                                "current_partition_id",
                                "partition_llm_results",
                                "loop_convergence_score"
                            ],
                            "auto_approve": true,
                            "auto_approve_delay_ms": 100,
                            "on_complete": {
                                "code": "partition_id = state.get('current_partition_id', 0)\nstate['post_hitl_state_keys'] = list(state.keys())\nstate['hitl_events'].append({\n    'partition_id': partition_id,\n    'pre_hitl_state_keys': state.get('pre_hitl_state_keys', []),\n    'post_hitl_state_keys': state.get('post_hitl_state_keys', []),\n    'decision': 'approved'\n})\nprint(f'‚úÖ ÌååÌã∞ÏÖò {partition_id} HITL ÏäπÏù∏ ÏôÑÎ£å')"
                            }
                        }
                    },
                    {
                        "id": "finalize_partition",
                        "type": "operator",
                        "config": {
                            "code": "import time\n\npartition_id = state.get('current_partition_id', 0)\nend_time = int(time.time() * 1000)\nstart_time = state.get('partition_start_time', end_time)\n\n# ÌååÌã∞ÏÖò Í≤∞Í≥º ÏßëÍ≥Ñ - Ïª§ÎÑê aggregatorÍ∞Ä ÏûêÎèô Ìï©ÏÇ∞ÌïòÎØÄÎ°ú Í∞úÎ≥Ñ usageÎßå Ìè¨Ìï®\nusage = state.get('usage', {})\nresult = {\n    'partition_id': partition_id,\n    'status': 'completed',\n    'loop_iterations': state.get('partition_loop_count', 0),\n    'final_confidence': state.get('loop_convergence_score', 0),\n    'llm_results': state.get('partition_llm_results', []),\n    'usage': usage,  # Í∞úÎ≥Ñ usage - Ïª§ÎÑêÏù¥ ÌèâÌÉÑÌôî(Flatten) ÌõÑ Ìï©ÏÇ∞\n    'latency_ms': end_time - start_time\n}\n\n# ‚ö†Ô∏è race condition Î∞©ÏßÄ: aggregated_usage ÏßÅÏ†ë ÏàòÏ†ï Í∏àÏßÄ!\n# Ïª§ÎÑê Î†àÎ≤® aggregatorÍ∞Ä partition_results[].usageÎ•º ÏûêÎèô ÏßëÍ≥ÑÌï®\n\nprint(f'‚úÖ ÌååÌã∞ÏÖò {partition_id} ÏôÑÎ£å: {result[\"loop_iterations\"]} iterations, confidence={result[\"final_confidence\"]:.2f}')\nstate['partition_final_result'] = result",
                            "output_key": "partition_result"
                        }
                    }
                ]
            }
        },
        {
            "id": "merge_partition_results",
            "type": "operator",
            "config": {
                "code": "import time\n\npartition_results = state.get('partition_results', [])\n\n# Í≤∞Í≥º Î≥ëÌï©\nall_themes = []\nall_sentiments = []\ntotal_confidence = 0\n\nfor p_result in partition_results:\n    if isinstance(p_result, dict):\n        for llm_result in p_result.get('llm_results', []):\n            all_themes.extend(llm_result.get('themes', []))\n            all_sentiments.append(llm_result.get('sentiment', 'neutral'))\n        total_confidence += p_result.get('final_confidence', 0)\n\n# Ï§ëÎ≥µ Ï†úÍ±∞\nunique_themes = list(set(all_themes))\n\n# ÏµúÏ¢Ö sentiment Í≤∞Ï†ï\nsentiment_counts = {}\nfor s in all_sentiments:\n    sentiment_counts[s] = sentiment_counts.get(s, 0) + 1\nfinal_sentiment = max(sentiment_counts, key=sentiment_counts.get) if sentiment_counts else 'neutral'\n\n# ÌèâÍ∑† confidence\navg_confidence = total_confidence / len(partition_results) if partition_results else 0\nstate['loop_convergence_score'] = avg_confidence\n\nstate['merged_output'] = {\n    'unique_themes': unique_themes,\n    'final_sentiment': final_sentiment,\n    'average_confidence': avg_confidence,\n    'partition_count': len(partition_results)\n}\n\nstate['final_merged_result'] = state['merged_output']\n\nend_time = int(time.time() * 1000)\nstate['test_end_time'] = end_time\nstate['total_latency_ms'] = end_time - state.get('test_start_time', end_time)\n\nprint(f'‚úÖ MAP_REDUCE Î≥ëÌï© ÏôÑÎ£å: {len(unique_themes)} themes, sentiment={final_sentiment}, confidence={avg_confidence:.2f}')",
                "output_key": "merge_result"
            }
        },
        {
            "id": "final_verification",
            "type": "operator",
            "config": {
                "code": "partition_results = state.get('partition_results', [])\ncompleted = len([p for p in partition_results if isinstance(p, dict) and p.get('status') == 'completed'])\nexpected = state.get('partition_count', 3)\n\nhitl_count = len(state.get('hitl_events', []))\nloop_iterations = len(state.get('loop_iterations', []))\n\n# Ïª§ÎÑê aggregatorÍ∞Ä ÏßëÍ≥ÑÌïú usage ÏÇ¨Ïö© (Í∞úÎ≥Ñ partition Í≤∞Í≥ºÏóêÏÑú ÌèâÌÉÑÌôî)\ntotal_tokens = 0\ntotal_cost = 0\nfor p in partition_results:\n    if isinstance(p, dict):\n        usage = p.get('usage', {})\n        total_tokens += usage.get('total_tokens', usage.get('input_tokens', 0) + usage.get('output_tokens', 0))\n        total_cost += usage.get('cost', 0)\n\n# S3 Ïò§ÌîÑÎ°úÎî© Ïó¨Î∂Ä ÌôïÏù∏\ns3_offloaded = state.get('_s3_offload_triggered', False)\nstate_size_kb = len(str(state)) // 1024\n\nif completed >= expected:\n    state['TEST_RESULT'] = f'‚úÖ STAGE6 SUCCESS: {completed}/{expected} partitions, {loop_iterations} iters, {hitl_count} HITLs, {total_tokens} tokens, ${total_cost:.4f}, S3={s3_offloaded} ({state_size_kb}KB)'\nelse:\n    state['TEST_RESULT'] = f'‚ùå STAGE6 PARTIAL: {completed}/{expected} partitions completed'\n\nprint(state['TEST_RESULT'])",
                "output_key": "verification_result"
            }
        }
    ],
    "edges": [
        {
            "source": "distributed_map_processor",
            "target": "merge_partition_results"
        },
        {
            "source": "merge_partition_results",
            "target": "final_verification"
        }
    ],
    "start_node": "distributed_map_processor",
    "initial_state": {
        "test_stage": 6,
        "distributed_mode": true,
        "distributed_strategy": "MAP_REDUCE",
        "partition_count": 3,
        "items_per_partition": 4,
        "max_concurrency": 3,
        "loop_convergence_threshold": 0.8,
        "hitl_enabled": true,
        "hitl_checkpoint_at_partition": 1,
        "cost_guardrail_enabled": true,
        "max_tokens_total": 50000,
        "dummy_content_size_kb": 2,
        "s3_offload_threshold_kb": 180,
        "verify_s3_offload": true,
        "partition_map": {
            "partition_0": [
                {
                    "item_id": "item_0",
                    "content": "Document 0: Dummy data for partition 0",
                    "partition_id": 0
                },
                {
                    "item_id": "item_1",
                    "content": "Document 1: Dummy data for partition 0",
                    "partition_id": 0
                },
                {
                    "item_id": "item_2",
                    "content": "Document 2: Dummy data for partition 0",
                    "partition_id": 0
                },
                {
                    "item_id": "item_3",
                    "content": "Document 3: Dummy data for partition 0",
                    "partition_id": 0
                }
            ],
            "partition_1": [
                {
                    "item_id": "item_4",
                    "content": "Document 4: Dummy data for partition 1",
                    "partition_id": 1
                },
                {
                    "item_id": "item_5",
                    "content": "Document 5: Dummy data for partition 1",
                    "partition_id": 1
                },
                {
                    "item_id": "item_6",
                    "content": "Document 6: Dummy data for partition 1",
                    "partition_id": 1
                },
                {
                    "item_id": "item_7",
                    "content": "Document 7: Dummy data for partition 1",
                    "partition_id": 1
                }
            ],
            "partition_2": [
                {
                    "item_id": "item_8",
                    "content": "Document 8: Dummy data for partition 2",
                    "partition_id": 2
                },
                {
                    "item_id": "item_9",
                    "content": "Document 9: Dummy data for partition 2",
                    "partition_id": 2
                },
                {
                    "item_id": "item_10",
                    "content": "Document 10: Dummy data for partition 2",
                    "partition_id": 2
                },
                {
                    "item_id": "item_11",
                    "content": "Document 11: Dummy data for partition 2",
                    "partition_id": 2
                }
            ]
        },
        "partition_results": [],
        "loop_iterations": [],
        "hitl_events": [],
        "providers_used": []
    }
}