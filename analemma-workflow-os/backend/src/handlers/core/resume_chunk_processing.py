"""
[Critical Fix #3] HITL ì½œë°± í›„ ì²­í¬ ì²˜ë¦¬ ì¬ê°œ Lambda

ì½œë°± ì™„ë£Œ í›„ í•´ë‹¹ ì²­í¬ ë‚´ì—ì„œ ì¤‘ë‹¨ëœ ì§€ì  ì´í›„ì˜ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„
ë‹¤ì‹œ ê³„ì‚°í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤.

í•µì‹¬ ì›ë¦¬:
- paused_segment_idë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚¨ì€ ì„¸ê·¸ë¨¼íŠ¸ íŒŒí‹°ì…”ë‹
- ì½œë°± ê²°ê³¼ë¥¼ í˜„ì¬ ìƒíƒœì— ë³‘í•©
- ë‚¨ì€ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ ìˆœì°¨ ì‹¤í–‰
- ì¶”ê°€ HITL ë°œìƒ ì‹œ ë‹¤ì‹œ PAUSED_FOR_HITP ë°˜í™˜

ğŸš¨ [Critical Fixes Applied]:

â‘  ìƒíƒœ í˜ì´ë¡œë“œ í¬ê¸° ì œì–´ (Payload Management):
- Step Functions 256KB ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ S3 ì˜¤í”„ë¡œë”© êµ¬í˜„
- 200KB ì„ê³„ê°’ìœ¼ë¡œ ëŒ€ìš©ëŸ‰ ìƒíƒœ ìë™ ê°ì§€
- S3 URI ì°¸ì¡° ë°©ì‹ìœ¼ë¡œ í˜ì´ë¡œë“œ í¬ê¸° ìµœì†Œí™”
- ìƒíƒœ ìš”ì•½ ì •ë³´ëŠ” ì¸ë¼ì¸ìœ¼ë¡œ ìœ ì§€í•˜ì—¬ í˜¸í™˜ì„± ë³´ì¥

â‘¡ ë©±ë“±ì„± í‚¤ ì •í•©ì„± (Idempotency Key Consistency):
- #resumed# ì ‘ë¯¸ì‚¬ ì‚¬ìš© ì‹œ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ ë¡œì§ ê°•í™”
- ê¸°ì¡´ execution_id ê¸°ë°˜ ê³ ìœ  í‚¤ ìƒì„±ìœ¼ë¡œ ì•ˆì „ì„± í™•ë³´
- DynamoDB ë©±ë“±ì„± í…Œì´ë¸”ê³¼ì˜ ì •í•©ì„± ê²€ì¦
- í‚¤ ê¸¸ì´ ë° êµ¬ì¡° ê²€ì¦ìœ¼ë¡œ DynamoDB ì œí•œ ì¤€ìˆ˜
- í´ë°± ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ í‚¤ ìƒì„± ì‹¤íŒ¨ ì‹œì—ë„ ì•ˆì „í•œ ë™ì‘ ë³´ì¥

ì´ ìˆ˜ì •ìœ¼ë¡œ ì¬ê°œëœ ì²­í¬ê°€ Step Functions í˜ì´ë¡œë“œ ì œí•œì— ê±¸ë¦¬ì§€ ì•Šê³ ,
ì‚¬ìš©ìì˜ ì‹¤ìˆ˜ë¡œ ì¸í•œ ì¤‘ë³µ ì½œë°±ì—ë„ ì•ˆì „í•˜ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import json
import logging
import os
import time
from typing import Dict, List, Any, Optional

import boto3

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


def lambda_handler(event: Dict[str, Any], context: Any = None) -> Dict[str, Any]:
    """
    HITL ì½œë°± í›„ ì²­í¬ ì²˜ë¦¬ ì¬ê°œ
    
    Args:
        event: {
            "chunk_result": { ì´ì „ ì²­í¬ ì²˜ë¦¬ ê²°ê³¼ },
            "callback_result": { HITL ì½œë°± ê²°ê³¼ },
            "chunk_data": { ì›ë³¸ ì²­í¬ ë°ì´í„° },
            "paused_segment_id": ì¤‘ë‹¨ëœ ì„¸ê·¸ë¨¼íŠ¸ ID,
            "execution_id": "exec-123",
            "owner_id": "user-456",
            "workflow_id": "wf-789",
            "workflow_config": {...},
            "state_bucket": "my-bucket"
        }
    
    Returns:
        {
            "chunk_id": str,
            "status": "COMPLETED" | "PAUSED_FOR_HITP" | "FAILED",
            "final_state": {...},
            "paused_segment_id": int (if paused again)
        }
    """
    try:
        chunk_result = event.get('chunk_result', {})
        callback_result = event.get('callback_result', {})
        chunk_data = event.get('chunk_data', {})
        paused_segment_id = event.get('paused_segment_id')
        execution_id = event.get('execution_id')
        owner_id = event.get('owner_id')
        workflow_id = event.get('workflow_id')
        workflow_config = event.get('workflow_config', {})
        state_bucket = event.get('state_bucket') or os.environ.get('WORKFLOW_STATE_BUCKET')

        chunk_id = chunk_data.get('chunk_id', chunk_result.get('chunk_id', 'unknown'))
        partition_slice = chunk_data.get('partition_slice', [])
        start_segment = chunk_data.get('start_segment', 0)
        
        logger.info(
            f"Resuming chunk {chunk_id} after HITL callback, "
            f"paused at segment {paused_segment_id}"
        )
        
        # 1. ì½œë°± ê²°ê³¼ì—ì„œ ì‚¬ìš©ì ì…ë ¥ ì¶”ì¶œ
        user_input = _extract_user_input(callback_result)
        logger.info(f"Extracted user input from src.callback: {list(user_input.keys())}")
        
        # 2. ì´ì „ ìƒíƒœì™€ ì½œë°± ê²°ê³¼ ë³‘í•©
        previous_state = chunk_result.get('final_state', {})
        current_state = _merge_callback_state(previous_state, user_input, callback_result)
        
        # 3. ì¤‘ë‹¨ ì§€ì  ì´í›„ ì„¸ê·¸ë¨¼íŠ¸ ê³„ì‚°
        remaining_segments = _calculate_remaining_segments(
            partition_slice=partition_slice,
            start_segment=start_segment,
            paused_segment_id=paused_segment_id
        )
        
        if not remaining_segments:
            logger.info(f"No remaining segments after paused segment {paused_segment_id}")
            return {
                "chunk_id": chunk_id,
                "status": "COMPLETED",
                "final_state": current_state,
                "processed_after_resume": 0
            }
        
        logger.info(
            f"Processing {len(remaining_segments)} remaining segments "
            f"(from src.segment {remaining_segments[0]['global_index']} onwards)"
        )
        
        # 4. ë‚¨ì€ ì„¸ê·¸ë¨¼íŠ¸ ìˆœì°¨ ì‹¤í–‰
        return _process_remaining_segments(
            chunk_id=chunk_id,
            remaining_segments=remaining_segments,
            current_state=current_state,
            workflow_config=workflow_config,
            owner_id=owner_id,
            workflow_id=workflow_id,
            execution_id=execution_id,
            state_bucket=state_bucket,
            context=context
        )
        
    except Exception as e:
        logger.exception(f"Failed to resume chunk processing: {e}")
        return {
            "chunk_id": event.get('chunk_data', {}).get('chunk_id', 'unknown'),
            "status": "FAILED",
            "error": str(e),
            "final_state": event.get('chunk_result', {}).get('final_state', {})
        }


def _extract_user_input(callback_result: Dict[str, Any]) -> Dict[str, Any]:
    """ì½œë°± ê²°ê³¼ì—ì„œ ì‚¬ìš©ì ì…ë ¥ ì¶”ì¶œ"""
    # ë‹¤ì–‘í•œ ì½œë°± í˜•ì‹ ì§€ì›
    if 'user_input' in callback_result:
        return callback_result['user_input']
    elif 'Payload' in callback_result:
        payload = callback_result['Payload']
        return payload.get('user_input', payload)
    elif 'output' in callback_result:
        return callback_result['output']
    else:
        return callback_result


def _merge_callback_state(
    previous_state: Dict[str, Any],
    user_input: Dict[str, Any],
    callback_result: Dict[str, Any]
) -> Dict[str, Any]:
    """ì´ì „ ìƒíƒœì™€ ì½œë°± ê²°ê³¼ ë³‘í•©"""
    merged = {**previous_state}
    
    # ì‚¬ìš©ì ì…ë ¥ì„ ìƒíƒœì— ë³‘í•©
    if user_input:
        merged['__callback_input'] = user_input
        
        # íŠ¹ì • í•„ë“œë“¤ì€ ì§ì ‘ ë³‘í•©
        for key in ['selected_option', 'user_response', 'approval', 'feedback']:
            if key in user_input:
                merged[key] = user_input[key]
    
    # ì½œë°± ë©”íƒ€ë°ì´í„° ì¶”ê°€
    merged['__last_callback'] = {
        'timestamp': int(time.time()),
        'callback_type': callback_result.get('callback_type', 'unknown')
    }
    
    return merged


def _calculate_remaining_segments(
    partition_slice: List[Dict[str, Any]],
    start_segment: int,
    paused_segment_id: int
) -> List[Dict[str, Any]]:
    """ì¤‘ë‹¨ ì§€ì  ì´í›„ ì„¸ê·¸ë¨¼íŠ¸ ê³„ì‚°"""
    remaining = []
    
    for idx, segment in enumerate(partition_slice):
        global_index = start_segment + idx
        
        # ì¤‘ë‹¨ëœ ì„¸ê·¸ë¨¼íŠ¸ ì´í›„ë§Œ í¬í•¨
        if global_index > paused_segment_id:
            remaining.append({
                'segment': segment,
                'local_index': idx,
                'global_index': global_index
            })
    
    return remaining


def _process_remaining_segments(
    chunk_id: str,
    remaining_segments: List[Dict[str, Any]],
    current_state: Dict[str, Any],
    workflow_config: Dict[str, Any],
    owner_id: str,
    workflow_id: str,
    execution_id: str,
    state_bucket: str,
    context: Any
) -> Dict[str, Any]:
    """ë‚¨ì€ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ ìˆœì°¨ ì‹¤í–‰"""
    
    # segment_runner_handler ì„í¬íŠ¸
    try:
        from src.handlers.core.segment_runner_handler import lambda_handler as segment_runner_handler
    except ImportError:
        logger.error("Failed to import segment_runner_handler")
        return {
            "chunk_id": chunk_id,
            "status": "FAILED",
            "error": "segment_runner_handler import failed",
            "final_state": current_state
        }
    
    processed_count = 0
    s3_client = boto3.client('s3') if state_bucket else None
    
    for seg_info in remaining_segments:
        segment = seg_info['segment']
        global_index = seg_info['global_index']
        
        logger.info(f"Processing resumed segment {global_index} in chunk {chunk_id}")
        
        try:
            # ğŸš¨ [Critical Fix] ë©±ë“±ì„± í‚¤ ì •í•©ì„± - ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
            # ê¸°ì¡´ execution_id ê¸°ë°˜ìœ¼ë¡œ ê³ ìœ í•œ í‚¤ ìƒì„± (resumed ì ‘ë¯¸ì‚¬ ì‚¬ìš©í•˜ë˜ ì•ˆì „í•˜ê²Œ)
            base_idempotency_key = event.get('idempotency_key') or f"{execution_id}#chunk#{chunk_id}"
            segment_idempotency_key = f"{base_idempotency_key}#resumed_segment_{global_index}"
            
            # ğŸš¨ ë©±ë“±ì„± í‚¤ ì•ˆì „ì„± ê²€ì¦
            idempotency_validation = _validate_idempotency_safety(
                execution_id=execution_id,
                chunk_id=chunk_id,
                segment_id=global_index,
                base_idempotency_key=base_idempotency_key
            )
            
            if not idempotency_validation["is_safe"]:
                logger.warning(f"Idempotency key safety concerns: {idempotency_validation['warnings']}")
                # ì•ˆì „í•œ í´ë°± í‚¤ ì‚¬ìš©
                segment_idempotency_key = idempotency_validation["generated_key"]
            
            if idempotency_validation["recommendations"]:
                logger.info(f"Idempotency recommendations: {idempotency_validation['recommendations']}")
            
            # ì„¸ê·¸ë¨¼íŠ¸ ì‹¤í–‰ ì´ë²¤íŠ¸ êµ¬ì„±
            segment_event = {
                'segment_config': segment,
                'current_state': current_state,
                'ownerId': owner_id,
                'workflowId': workflow_id,
                'segment_to_run': global_index,
                'workflow_config': workflow_config,
                'execution_id': f"{chunk_id}_resumed_segment_{global_index}",
                'idempotency_key': segment_idempotency_key,  # ğŸš¨ ê²€ì¦ëœ ë©±ë“±ì„± í‚¤
                'distributed_context': {
                    'chunk_id': chunk_id,
                    'is_resumed': True,
                    'resumed_from_segment': seg_info.get('paused_at', global_index - 1),
                    'original_execution_id': execution_id,  # ì›ë³¸ ì‹¤í–‰ ID ì¶”ì 
                    'resume_timestamp': int(time.time()),
                    'idempotency_validated': True  # ê²€ì¦ ì™„ë£Œ í‘œì‹œ
                }
            }
            
            # ì„¸ê·¸ë¨¼íŠ¸ ì‹¤í–‰
            segment_result = segment_runner_handler(segment_event, context)
            processed_count += 1
            
            # ê²°ê³¼ ì²˜ë¦¬
            if segment_result.get('status') == 'COMPLETE':
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                if segment_result.get('final_state'):
                    current_state = segment_result['final_state']
                
                # S3ì— ì¤‘ê°„ ìƒíƒœ ì €ì¥
                if s3_client and state_bucket:
                    _save_intermediate_state(
                        s3_client=s3_client,
                        bucket=state_bucket,
                        owner_id=owner_id,
                        workflow_id=workflow_id,
                        execution_id=execution_id,
                        chunk_id=chunk_id,
                        segment_id=global_index,
                        state=current_state
                    )
                    
            elif segment_result.get('status') in ['PAUSE', 'PAUSED_FOR_HITP']:
                # ë‹¤ì‹œ HITL ëŒ€ê¸° í•„ìš”
                logger.info(
                    f"Segment {global_index} requires another HITL pause "
                    f"after resume in chunk {chunk_id}"
                )
                
                # ğŸš¨ [Critical Fix] ìƒíƒœ í˜ì´ë¡œë“œ í¬ê¸° ì œì–´
                return _build_chunk_response_with_payload_control(
                    chunk_id=chunk_id,
                    status="PAUSED_FOR_HITP",
                    final_state=current_state,
                    paused_segment_id=global_index,
                    processed_after_resume=processed_count,
                    task_token=segment_result.get('task_token'),
                    remaining_segments=len(remaining_segments) - processed_count,
                    state_bucket=state_bucket,
                    owner_id=owner_id,
                    workflow_id=workflow_id,
                    execution_id=execution_id
                )
                
            elif segment_result.get('status') == 'FAILED':
                logger.error(f"Segment {global_index} failed after resume")
                # ğŸš¨ [Critical Fix] ìƒíƒœ í˜ì´ë¡œë“œ í¬ê¸° ì œì–´
                return _build_chunk_response_with_payload_control(
                    chunk_id=chunk_id,
                    status="FAILED",
                    final_state=current_state,
                    failed_segment_id=global_index,
                    processed_after_resume=processed_count,
                    error=segment_result.get('error_info', 'Unknown error'),
                    state_bucket=state_bucket,
                    owner_id=owner_id,
                    workflow_id=workflow_id,
                    execution_id=execution_id
                )
                
        except Exception as e:
            logger.error(f"Error processing resumed segment {global_index}: {e}")
            # ğŸš¨ [Critical Fix] ìƒíƒœ í˜ì´ë¡œë“œ í¬ê¸° ì œì–´
            return _build_chunk_response_with_payload_control(
                chunk_id=chunk_id,
                status="FAILED",
                final_state=current_state,
                failed_segment_id=global_index,
                processed_after_resume=processed_count,
                error=str(e),
                state_bucket=state_bucket,
                owner_id=owner_id,
                workflow_id=workflow_id,
                execution_id=execution_id
            )
    
    # ëª¨ë“  ë‚¨ì€ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì™„ë£Œ
    logger.info(
        f"Chunk {chunk_id} completed after resume, "
        f"processed {processed_count} segments"
    )
    
    # ìµœì¢… ì„¸ê·¸ë¨¼íŠ¸ ID ê¸°ë¡
    current_state['__latest_segment_id'] = remaining_segments[-1]['global_index']
    
    # ğŸš¨ [Critical Fix] ìƒíƒœ í˜ì´ë¡œë“œ í¬ê¸° ì œì–´ - Step Functions 256KB ì œí•œ ë°©ì§€
    return _build_chunk_response_with_payload_control(
        chunk_id=chunk_id,
        status="COMPLETED",
        final_state=current_state,
        processed_after_resume=processed_count,
        state_bucket=state_bucket,
        owner_id=owner_id,
        workflow_id=workflow_id,
        execution_id=execution_id
    )


def _save_intermediate_state(
    s3_client,
    bucket: str,
    owner_id: str,
    workflow_id: str,
    execution_id: str,
    chunk_id: str,
    segment_id: int,
    state: Dict[str, Any]
) -> None:
    """ì¤‘ê°„ ìƒíƒœë¥¼ S3ì— ì €ì¥"""
    try:
        key = (
            f"distributed-states/{owner_id}/{workflow_id}/{execution_id}/"
            f"chunks/{chunk_id}/segment_{segment_id}_resumed.json"
        )
        
        s3_client.put_object(
            Bucket=bucket,
            Key=key,
            Body=json.dumps(state, ensure_ascii=False, default=str).encode('utf-8'),
            ContentType='application/json',
            Metadata={
                'chunk_id': chunk_id,
                'segment_id': str(segment_id),
                'is_resumed': 'true',
                'timestamp': str(int(time.time()))
            }
        )
        
        logger.debug(f"Saved intermediate state: s3://{bucket}/{key}")
        
    except Exception as e:
        logger.warning(f"Failed to save intermediate state (non-fatal): {e}")


def _build_chunk_response_with_payload_control(
    chunk_id: str,
    status: str,
    final_state: Dict[str, Any],
    state_bucket: str,
    owner_id: str,
    workflow_id: str,
    execution_id: str,
    processed_after_resume: int = 0,
    paused_segment_id: Optional[int] = None,
    task_token: Optional[str] = None,
    remaining_segments: Optional[int] = None,
    failed_segment_id: Optional[int] = None,
    error: Optional[str] = None
) -> Dict[str, Any]:
    """
    ğŸš¨ [Critical Fix] ìƒíƒœ í˜ì´ë¡œë“œ í¬ê¸° ì œì–´
    
    Step Functions 256KB ì œí•œì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ëŒ€ìš©ëŸ‰ ìƒíƒœëŠ” S3ì— ì €ì¥í•˜ê³ 
    URIë§Œ ë°˜í™˜í•˜ëŠ” ì˜¤í”„ë¡œë”© ë¡œì§ ì ìš©
    
    Args:
        chunk_id: ì²­í¬ ID
        status: ì²˜ë¦¬ ìƒíƒœ
        final_state: ìµœì¢… ìƒíƒœ ë°ì´í„°
        state_bucket: S3 ë²„í‚·
        owner_id: ì†Œìœ ì ID
        workflow_id: ì›Œí¬í”Œë¡œìš° ID
        execution_id: ì‹¤í–‰ ID
        processed_after_resume: ì¬ê°œ í›„ ì²˜ë¦¬ëœ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜
        paused_segment_id: ì¤‘ë‹¨ëœ ì„¸ê·¸ë¨¼íŠ¸ ID (PAUSED ìƒíƒœ ì‹œ)
        task_token: Task Token (PAUSED ìƒíƒœ ì‹œ)
        remaining_segments: ë‚¨ì€ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ (PAUSED ìƒíƒœ ì‹œ)
        failed_segment_id: ì‹¤íŒ¨í•œ ì„¸ê·¸ë¨¼íŠ¸ ID (FAILED ìƒíƒœ ì‹œ)
        error: ì˜¤ë¥˜ ë©”ì‹œì§€ (FAILED ìƒíƒœ ì‹œ)
    
    Returns:
        ì²­í¬ ì‘ë‹µ (í˜ì´ë¡œë“œ í¬ê¸° ì œì–´ ì ìš©)
    """
    try:
        # ê¸°ë³¸ ì‘ë‹µ êµ¬ì„±
        response = {
            "chunk_id": chunk_id,
            "status": status,
            "processed_after_resume": processed_after_resume
        }
        
        # ìƒíƒœë³„ ì¶”ê°€ í•„ë“œ
        if paused_segment_id is not None:
            response["paused_segment_id"] = paused_segment_id
        if task_token:
            response["task_token"] = task_token
        if remaining_segments is not None:
            response["remaining_segments"] = remaining_segments
        if failed_segment_id is not None:
            response["failed_segment_id"] = failed_segment_id
        if error:
            response["error"] = error
        
        # ğŸš¨ [Critical] final_state í¬ê¸° ê²€ì‚¬ ë° ì˜¤í”„ë¡œë”©
        if final_state:
            state_json = json.dumps(final_state, ensure_ascii=False)
            state_size_bytes = len(state_json.encode('utf-8'))
            
            # Step Functions Task Output ì œí•œ: 256KB
            # ì•ˆì „ ë§ˆì§„ì„ ìœ„í•´ 200KBë¡œ ì œí•œ (ê¸°ì¡´ íŒ¨í„´ê³¼ ë™ì¼)
            SIZE_LIMIT_BYTES = 200 * 1024  # 200KB
            
            logger.info(f"Resume response state size: {state_size_bytes} bytes (limit: {SIZE_LIMIT_BYTES})")
            
            if state_size_bytes <= SIZE_LIMIT_BYTES:
                # ì‘ì€ ìƒíƒœëŠ” ì¸ë¼ì¸ìœ¼ë¡œ ë°˜í™˜
                response["final_state"] = final_state
                response["payload_type"] = "inline"
                response["payload_size_bytes"] = state_size_bytes
                
                logger.info(f"Resume response: inline state ({state_size_bytes} bytes)")
                
            else:
                # ğŸš¨ [Critical] ëŒ€ìš©ëŸ‰ ìƒíƒœëŠ” S3ì— ì €ì¥í•˜ê³  URIë§Œ ë°˜í™˜
                logger.warning(f"Large resume state detected ({state_size_bytes} bytes), storing in S3")
                
                if not state_bucket:
                    logger.error("No state bucket available for large state offloading")
                    # í´ë°±: ìƒíƒœ ìš”ì•½ë§Œ í¬í•¨
                    response["final_state"] = _create_state_summary(final_state)
                    response["payload_type"] = "summary_fallback"
                    response["error"] = "Large state detected but no S3 bucket available"
                else:
                    # S3ì— ìƒíƒœ ì €ì¥
                    s3_uri = _store_large_state_in_s3(
                        state_data=final_state,
                        state_bucket=state_bucket,
                        owner_id=owner_id,
                        workflow_id=workflow_id,
                        execution_id=execution_id,
                        chunk_id=chunk_id,
                        context="resume_response"
                    )
                    
                    if s3_uri:
                        # S3 URIì™€ ë©”íƒ€ë°ì´í„°ë§Œ ë°˜í™˜
                        response["final_state_s3_uri"] = s3_uri
                        response["payload_type"] = "s3_reference"
                        response["payload_size_bytes"] = state_size_bytes
                        response["s3_offloaded"] = True
                        
                        # ì‘ì€ ìƒíƒœ ìš”ì•½ì€ ì¸ë¼ì¸ìœ¼ë¡œ í¬í•¨
                        response["state_summary"] = _create_state_summary(final_state)
                        
                        logger.info(f"Resume response: S3 offloaded to {s3_uri}")
                    else:
                        # S3 ì €ì¥ ì‹¤íŒ¨ ì‹œ í´ë°±
                        logger.error("Failed to store large state in S3, using summary")
                        response["final_state"] = _create_state_summary(final_state)
                        response["payload_type"] = "summary_fallback"
                        response["error"] = "Large state S3 storage failed"
        else:
            # ìƒíƒœê°€ ì—†ëŠ” ê²½ìš°
            response["final_state"] = {}
            response["payload_type"] = "empty"
        
        return response
        
    except Exception as e:
        logger.error(f"Failed to build chunk response with payload control: {e}")
        # ì‹¤íŒ¨ ì‹œ ìµœì†Œí•œì˜ ì‘ë‹µ ë°˜í™˜
        return {
            "chunk_id": chunk_id,
            "status": "FAILED",
            "error": f"Response building failed: {str(e)}",
            "final_state": {},
            "processed_after_resume": processed_after_resume
        }


def _store_large_state_in_s3(
    state_data: Dict[str, Any],
    state_bucket: str,
    owner_id: str,
    workflow_id: str,
    execution_id: str,
    chunk_id: str,
    context: str = "resume"
) -> Optional[str]:
    """
    ëŒ€ìš©ëŸ‰ ìƒíƒœë¥¼ S3ì— ì €ì¥
    
    Args:
        state_data: ìƒíƒœ ë°ì´í„°
        state_bucket: S3 ë²„í‚·
        owner_id: ì†Œìœ ì ID
        workflow_id: ì›Œí¬í”Œë¡œìš° ID
        execution_id: ì‹¤í–‰ ID
        chunk_id: ì²­í¬ ID
        context: ì €ì¥ ì»¨í…ìŠ¤íŠ¸
    
    Returns:
        S3 URI (ì„±ê³µ ì‹œ) ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    try:
        s3_client = boto3.client('s3')
        
        # ì¬ê°œ ì „ìš© S3 í‚¤ ìƒì„±
        timestamp = int(time.time())
        large_state_key = (
            f"distributed-states/{owner_id}/{workflow_id}/{execution_id}/"
            f"chunks/{chunk_id}/{context}_state_{timestamp}.json"
        )
        
        # ìƒíƒœë¥¼ JSONìœ¼ë¡œ ì§ë ¬í™”
        state_json = json.dumps(state_data, ensure_ascii=False, default=str)
        
        # S3ì— ì €ì¥
        s3_client.put_object(
            Bucket=state_bucket,
            Key=large_state_key,
            Body=state_json.encode('utf-8'),
            ContentType='application/json',
            Metadata={
                'execution_id': execution_id,
                'chunk_id': chunk_id,
                'owner_id': owner_id,
                'workflow_id': workflow_id,
                'context': context,
                'original_size': str(len(state_json.encode('utf-8'))),
                'created_at': str(timestamp),
                'payload_type': 'large_state_resume'
            }
        )
        
        s3_uri = f"s3://{state_bucket}/{large_state_key}"
        logger.info(f"Large resume state stored: {s3_uri}")
        
        return s3_uri
        
    except Exception as e:
        logger.error(f"Failed to store large state in S3: {e}")
        return None


def _create_state_summary(state_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    ìƒíƒœ ë°ì´í„°ì˜ ìš”ì•½ ìƒì„± (í¬ê¸° ì œí•œ ì¤€ìˆ˜)
    
    Args:
        state_data: ì›ë³¸ ìƒíƒœ ë°ì´í„°
    
    Returns:
        ìƒíƒœ ìš”ì•½ (ì‘ì€ í¬ê¸°)
    """
    try:
        if not isinstance(state_data, dict):
            return {"type": type(state_data).__name__, "size": len(str(state_data))}
        
        summary = {
            "segment_count": len(state_data.get('segments', [])),
            "has_chunks": 'chunks' in state_data,
            "has_callback_input": '__callback_input' in state_data,
            "latest_segment_id": state_data.get('__latest_segment_id'),
            "last_callback_timestamp": state_data.get('__last_callback', {}).get('timestamp'),
            "state_keys": list(state_data.keys())[:10],  # ì²˜ìŒ 10ê°œ í‚¤ë§Œ
            "total_keys": len(state_data.keys())
        }
        
        # ì¤‘ìš”í•œ ì‘ì€ í•„ë“œë“¤ì€ ì§ì ‘ í¬í•¨
        for key in ['selected_option', 'user_response', 'approval']:
            if key in state_data and isinstance(state_data[key], (str, int, bool, float)):
                value_str = str(state_data[key])
                if len(value_str) < 100:  # 100ì ë¯¸ë§Œë§Œ í¬í•¨
                    summary[key] = state_data[key]
        
        return summary
        
    except Exception as e:
        logger.error(f"Failed to create state summary: {e}")
        return {"error": "Summary creation failed", "original_type": type(state_data).__name__}


def _validate_idempotency_safety(
    execution_id: str,
    chunk_id: str,
    segment_id: int,
    base_idempotency_key: str
) -> Dict[str, Any]:
    """
    ğŸš¨ [Critical Fix] ë©±ë“±ì„± í‚¤ ì•ˆì „ì„± ê²€ì¦
    
    ì¬ê°œëœ ì„¸ê·¸ë¨¼íŠ¸ì˜ ë©±ë“±ì„± í‚¤ê°€ ì•ˆì „í•˜ê²Œ ìƒì„±ë˜ì—ˆëŠ”ì§€ ê²€ì¦í•˜ê³ 
    ì¤‘ë³µ ì‹¤í–‰ ìœ„í—˜ì„ í‰ê°€
    
    Args:
        execution_id: ì‹¤í–‰ ID
        chunk_id: ì²­í¬ ID
        segment_id: ì„¸ê·¸ë¨¼íŠ¸ ID
        base_idempotency_key: ê¸°ë³¸ ë©±ë“±ì„± í‚¤
    
    Returns:
        ê²€ì¦ ê²°ê³¼ ë° ê¶Œì¥ì‚¬í•­
    """
    try:
        validation = {
            "is_safe": True,
            "warnings": [],
            "recommendations": [],
            "generated_key": f"{base_idempotency_key}#resumed_segment_{segment_id}"
        }
        
        # 1. í‚¤ ê¸¸ì´ ê²€ì¦ (DynamoDB ì œí•œ)
        key_length = len(validation["generated_key"])
        if key_length > 2048:  # DynamoDB í‚¤ ê¸¸ì´ ì œí•œ
            validation["is_safe"] = False
            validation["warnings"].append(f"Idempotency key too long: {key_length} chars")
        
        # 2. ê³ ìœ ì„± ê²€ì¦
        if "#resumed#" in base_idempotency_key:
            validation["warnings"].append("Base key already contains #resumed# - potential double resumption")
            validation["recommendations"].append("Check for multiple resume attempts")
        
        # 3. êµ¬ì¡° ê²€ì¦
        expected_parts = ["execution_id", "chunk", "segment"]
        key_parts = validation["generated_key"].split("#")
        if len(key_parts) < 3:
            validation["warnings"].append(f"Idempotency key structure may be incomplete: {len(key_parts)} parts")
        
        # 4. íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ê³ ìœ ì„± ê¶Œì¥
        if str(int(time.time())) not in validation["generated_key"]:
            validation["recommendations"].append("Consider adding timestamp for stronger uniqueness")
        
        return validation
        
    except Exception as e:
        logger.error(f"Idempotency validation failed: {e}")
        return {
            "is_safe": False,
            "warnings": [f"Validation failed: {str(e)}"],
            "recommendations": ["Use fallback idempotency key generation"],
            "generated_key": f"fallback_{execution_id}_{chunk_id}_{segment_id}_{int(time.time())}"
        }
