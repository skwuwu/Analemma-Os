"""
AWS Distributed Mapì„ ìœ„í•œ ì„¸ê·¸ë¨¼íŠ¸ ì²­í‚¹ ë° ì¤€ë¹„ Lambda í•¨ìˆ˜

ì´ í•¨ìˆ˜ëŠ” ëŒ€ìš©ëŸ‰ ì›Œí¬í”Œë¡œìš°ë¥¼ Distributed Mapì—ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡
ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ ì ì ˆí•œ í¬ê¸°ì˜ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

[Critical Fix #2] ASL ë¬¸ë²• ìˆ˜ì •:
- ItemReaderê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ S3 ë²„í‚·ê³¼ í‚¤ë¥¼ ë¶„ë¦¬í•˜ì—¬ ë°˜í™˜
- Resourceì— S3 ê²½ë¡œë¥¼ ì§ì ‘ ë„£ëŠ” ë°©ì‹ ëŒ€ì‹  ì „ìš© ì„œë¹„ìŠ¤ ARN ì‚¬ìš©
"""

import json
import logging
import os
import time
from typing import Dict, List, Any, Optional

import boto3

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


def lambda_handler(event: Dict[str, Any], context: Any = None) -> Dict[str, Any]:
    """
    ëŒ€ìš©ëŸ‰ ì›Œí¬í”Œë¡œìš°ë¥¼ Distributed Mapìš© ì²­í¬ë¡œ ë¶„í• 
    
    [Critical Fix #2] ASL ItemReader ê·œê²© ì¤€ìˆ˜:
    - chunks_bucketê³¼ chunks_keyë¥¼ ë¶„ë¦¬í•˜ì—¬ ë°˜í™˜
    - ItemReader.Parameters.Bucketê³¼ Keyì—ì„œ ì§ì ‘ ì°¸ì¡° ê°€ëŠ¥
    
    Args:
        event: {
            "state_data": {...},
            "chunk_size": 100,
            "max_chunks": 100,
            "state_bucket": "bucket-name"  # ëª…ì‹œì  ë²„í‚· ì „ë‹¬
        }
        
    Returns:
        {
            "chunks_bucket": "bucket-name",
            "chunks_key": "path/to/chunks.json",
            "total_chunks": int,
            "use_s3_reader": true
        }
    """
    try:
        state_data = event.get('state_data', {})
        chunk_size = event.get('chunk_size', 100)
        max_chunks = event.get('max_chunks', 100)
        state_bucket = event.get('state_bucket') or os.environ.get('WORKFLOW_STATE_BUCKET')
        
        partition_map = state_data.get('partition_map')
        
        # ğŸš¨ [Critical Fix] S3 Offloading Support
        # ë¶„ì‚° ëª¨ë“œì—ì„œëŠ” partition_mapì´ S3ë¡œ ì˜¤í”„ë¡œë”©ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ
        if not partition_map:
            partition_map_s3_path = state_data.get('partition_map_s3_path')
            if partition_map_s3_path:
                logger.info(f"Loading partition_map from S3: {partition_map_s3_path}")
                try:
                    s3 = boto3.client('s3')
                    bucket_name = partition_map_s3_path.replace("s3://", "").split("/")[0]
                    key_name = "/".join(partition_map_s3_path.replace("s3://", "").split("/")[1:])
                    
                    obj = s3.get_object(Bucket=bucket_name, Key=key_name)
                    partition_map = json.loads(obj['Body'].read().decode('utf-8'))
                    logger.info(f"Successfully loaded partition_map from S3 (segments: {len(partition_map)})")
                except Exception as e:
                    logger.error(f"Failed to load partition_map from S3: {e}")
                    raise RuntimeError(f"Failed to load partition_map from S3: {e}")
        
        if not partition_map:
            partition_map = []
            
        total_segments = len(partition_map)

        
        if total_segments == 0:
            logger.warning("No segments found in partition_map")
            return {
                "chunks_bucket": state_bucket,
                "chunks_key": None,
                "total_chunks": 0,
                "chunk_size": 0,
                "original_segments": 0,
                "distributed_mode": False,
                "use_s3_reader": False
            }
        
        if not state_bucket:
            raise RuntimeError("state_bucket is required for Distributed Map execution")
        
        # ì²­í¬ í¬ê¸° ìµœì í™”
        optimal_chunk_size = min(chunk_size, max(10, total_segments // max_chunks))
        
        # ğŸš¨ [Critical Architecture Fix] ItemReader í˜¸í™˜ì„±ì„ ìœ„í•œ ì‚¬ì „ í¬ê¸° ê²€ì¦
        # ì˜ˆìƒ í˜ì´ë¡œë“œ í¬ê¸°ë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ ItemReader ì œí•œ ì¤€ìˆ˜
        estimated_chunk_size_kb = _estimate_chunks_payload_size(
            total_segments=total_segments,
            chunk_size=optimal_chunk_size,
            partition_map=partition_map
        )
        
        lambda_memory_mb = int(os.environ.get('AWS_LAMBDA_FUNCTION_MEMORY_SIZE', '2048'))
        max_payload_size_mb = lambda_memory_mb * 0.25  # 25%ë¡œ ë§¤ìš° ë³´ìˆ˜ì  ì œí•œ (ì••ì¶• ê³ ë ¤)
        max_payload_size_kb = max_payload_size_mb * 1024
        
        # ì˜ˆìƒ í¬ê¸°ê°€ ë„ˆë¬´ í¬ë©´ ì²­í¬ í¬ê¸°ë¥¼ ì‚¬ì „ ì¡°ì •
        if estimated_chunk_size_kb > max_payload_size_kb:
            logger.warning(f"Estimated payload too large: {estimated_chunk_size_kb:.1f}KB > {max_payload_size_kb:.1f}KB")
            
            # ì²­í¬ í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ ì¬ê³„ì‚°
            size_reduction_factor = estimated_chunk_size_kb / max_payload_size_kb
            adjusted_chunk_size = max(5, int(optimal_chunk_size / size_reduction_factor))
            
            logger.info(f"Adjusting chunk size: {optimal_chunk_size} -> {adjusted_chunk_size} for ItemReader compatibility")
            optimal_chunk_size = adjusted_chunk_size
        
        total_chunks = (total_segments + optimal_chunk_size - 1) // optimal_chunk_size
        
        # ì‹¤ì œ ì²­í¬ ìƒì„±
        chunks = []
        for chunk_idx in range(total_chunks):
            start_idx = chunk_idx * optimal_chunk_size
            end_idx = min(start_idx + optimal_chunk_size, total_segments)
            
            # ì²­í¬ì— í¬í•¨ë  ì„¸ê·¸ë¨¼íŠ¸ë“¤ ì¶”ì¶œ
            partition_slice = partition_map[start_idx:end_idx]
            
            chunk = {
                "chunk_id": f"chunk_{chunk_idx:04d}",
                "start_segment": start_idx,
                "end_segment": end_idx - 1,
                "segment_count": end_idx - start_idx,
                "partition_slice": partition_slice,
                "chunk_index": chunk_idx,
                "total_chunks": total_chunks,
                "estimated_events": (end_idx - start_idx) * 20,
                "created_at": context.aws_request_id if context else "local",
                "idempotency_key": f"{idempotency_key}#chunk#{chunk_idx:04d}",
                "owner_id": owner_id,
                "workflow_id": workflow_id
            }
            chunks.append(chunk)
        
        logger.info(f"Created {total_chunks} chunks (size: {optimal_chunk_size}) for {total_segments} segments")
        
        # [Critical Fix #2] S3ì— ì²­í¬ ë°°ì—´ ì €ì¥ (ItemReaderìš©)
        # Distributed Map ItemReaderëŠ” í•­ìƒ S3ì—ì„œ ì½ì–´ì•¼ í•¨
        s3_client = boto3.client('s3')
        
        # S3 í‚¤ ìƒì„± (ê²°ì •ë¡ ì )
        execution_id = context.aws_request_id if context else str(int(time.time()))
        chunks_key = f"distributed-chunks/{owner_id}/{workflow_id}/{execution_id}/chunks.json"
        
        chunks_json = json.dumps(chunks, ensure_ascii=False)
        chunks_size_kb = len(chunks_json.encode('utf-8')) / 1024
        
        # ğŸš¨ [Critical Fix] ëŒ€ìš©ëŸ‰ chunks ë°°ì—´ ì²˜ë¦¬ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬
        lambda_memory_mb = int(os.environ.get('AWS_LAMBDA_FUNCTION_MEMORY_SIZE', '2048'))
        max_payload_size_mb = lambda_memory_mb * 0.3  # 30%ë¡œ ë³´ìˆ˜ì  ì œí•œ (ì••ì¶• ê³ ë ¤)
        max_payload_size_kb = max_payload_size_mb * 1024
        
        logger.info(f"Chunks payload: {chunks_size_kb:.1f}KB, Lambda memory: {lambda_memory_mb}MB, Limit: {max_payload_size_kb:.1f}KB")
        
        if chunks_size_kb > max_payload_size_kb:
            logger.warning(f"Large chunks payload detected: {chunks_size_kb:.1f}KB > {max_payload_size_kb:.1f}KB")
            
            # ğŸš¨ [Critical Architecture Fix] ItemReader í˜¸í™˜ì„±ì„ ìœ„í•œ ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
            return _handle_large_chunks_upload(
                s3_client=s3_client,
                chunks=chunks,
                chunks_json=chunks_json,
                state_bucket=state_bucket,
                chunks_key=chunks_key,
                total_chunks=total_chunks,
                optimal_chunk_size=optimal_chunk_size,
                total_segments=total_segments,
                owner_id=owner_id,
                workflow_id=workflow_id
            )
        
        # ì¼ë°˜ í¬ê¸°ëŠ” ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì—…ë¡œë“œ
        s3_client.put_object(
            Bucket=state_bucket,
            Key=chunks_key,
            Body=chunks_json.encode('utf-8'),
            ContentType='application/json',
            Metadata={
                'total_chunks': str(total_chunks),
                'total_segments': str(total_segments),
                'chunk_size': str(optimal_chunk_size),
                'owner_id': owner_id or '',
                'workflow_id': workflow_id or '',
                'created_at': str(int(time.time())),
                'payload_size_kb': str(int(chunks_size_kb))
            }
        )
        
        logger.info(f"Chunks uploaded to S3: s3://{state_bucket}/{chunks_key} ({chunks_size_kb:.1f}KB)")
        
        # [Critical Fix #2] ASL ItemReader ê·œê²©ì— ë§ê²Œ ë²„í‚·ê³¼ í‚¤ ë¶„ë¦¬ ë°˜í™˜
        return {
            "chunks_bucket": state_bucket,
            "chunks_key": chunks_key,
            "total_chunks": total_chunks,
            "chunk_size": optimal_chunk_size,
            "original_segments": total_segments,
            "distributed_mode": True,
            "use_s3_reader": True,
            "payload_size_kb": chunks_size_kb,
            "itemreader_compatible": True,  # ğŸš¨ í˜¸í™˜ì„± ë³´ì¥
            "optimization_stats": {
                "requested_chunk_size": chunk_size,
                "optimal_chunk_size": optimal_chunk_size,
                "s3_bucket": state_bucket,
                "s3_key": chunks_key,
                "architecture_compliance": "itemreader_single_array"  # ğŸš¨ ì•„í‚¤í…ì²˜ ì¤€ìˆ˜
            }
        }
        
    except Exception as e:
        logger.exception("Failed to prepare distributed execution")
        
        # ğŸš¨ [Critical] ItemReader í˜¸í™˜ì„± ê´€ë ¨ ì˜¤ë¥˜ íŠ¹ë³„ ì²˜ë¦¬
        if "too large for ItemReader" in str(e):
            # ì²­í¬ í¬ê¸° ì¬ì¡°ì • ì œì•ˆê³¼ í•¨ê»˜ ì‹¤íŒ¨
            raise RuntimeError(
                f"Distributed execution preparation failed due to ItemReader size limits: {str(e)}. "
                f"Please reduce the workflow complexity or increase lambda memory to 3008MB."
            )
        
        raise RuntimeError(f"Distributed execution preparation failed: {str(e)}")


def _handle_large_chunks_upload(
    s3_client,
    chunks: List[Dict[str, Any]],
    chunks_json: str,
    state_bucket: str,
    chunks_key: str,
    total_chunks: int,
    optimal_chunk_size: int,
    total_segments: int,
    owner_id: str,
    workflow_id: str
) -> Dict[str, Any]:
    """
    ğŸš¨ [Critical Architecture Fix] ëŒ€ìš©ëŸ‰ ì²­í¬ ë°°ì—´ì„ ItemReader í˜¸í™˜ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
    
    Step Functions ItemReaderëŠ” ë‹¨ì¼ JSON ë°°ì—´ë§Œ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë¯€ë¡œ:
    1. ì••ì¶•ì„ ìµœìš°ì„ ìœ¼ë¡œ ì‹œë„í•˜ì—¬ ë‹¨ì¼ íŒŒì¼ ìœ ì§€
    2. ë©€í‹°íŒŒíŠ¸ ì—…ë¡œë“œë¡œ ì•ˆì •ì„± í™•ë³´
    3. ë¶„í•  ì—…ë¡œë“œëŠ” ItemReader í˜¸í™˜ì„± ë¬¸ì œë¡œ ì œê±°
    4. ê·¹í•œ ìƒí™©ì—ì„œëŠ” ì²­í¬ í¬ê¸° ì¬ì¡°ì •ìœ¼ë¡œ ëŒ€ì‘
    
    Args:
        s3_client: S3 í´ë¼ì´ì–¸íŠ¸
        chunks: ì²­í¬ ë°°ì—´
        chunks_json: JSON ë¬¸ìì—´
        state_bucket: S3 ë²„í‚·
        chunks_key: S3 í‚¤
        total_chunks: ì´ ì²­í¬ ìˆ˜
        optimal_chunk_size: ì²­í¬ í¬ê¸°
        total_segments: ì´ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜
        owner_id: ì†Œìœ ì ID
        workflow_id: ì›Œí¬í”Œë¡œìš° ID
    
    Returns:
        ì—…ë¡œë“œ ê²°ê³¼
    """
    try:
        import gzip
        import io
        
        # 1. ì••ì¶• ì‹œë„ (ìµœìš°ì„  ì „ëµ)
        logger.info("Attempting gzip compression for large chunks payload")
        
        # JSONì„ gzipìœ¼ë¡œ ì••ì¶•
        compressed_buffer = io.BytesIO()
        with gzip.GzipFile(fileobj=compressed_buffer, mode='wb') as gz_file:
            gz_file.write(chunks_json.encode('utf-8'))
        
        compressed_data = compressed_buffer.getvalue()
        compressed_size_kb = len(compressed_data) / 1024
        original_size_kb = len(chunks_json.encode('utf-8')) / 1024
        compression_ratio = compressed_size_kb / original_size_kb
        
        logger.info(f"Compression result: {original_size_kb:.1f}KB -> {compressed_size_kb:.1f}KB (ratio: {compression_ratio:.2f})")
        
        # 2. ğŸš¨ [Critical] ItemReader í˜¸í™˜ì„±ì„ ìœ„í•œ ë‹¨ì¼ íŒŒì¼ ê°•ì œ ìœ ì§€
        lambda_memory_mb = int(os.environ.get('AWS_LAMBDA_FUNCTION_MEMORY_SIZE', '512'))
        max_single_upload_mb = lambda_memory_mb * 0.3  # 30%ë¡œ ë” ë³´ìˆ˜ì  ì œí•œ (ì•ˆì „ ë§ˆì§„)
        max_single_upload_kb = max_single_upload_mb * 1024
        
        if compressed_size_kb > max_single_upload_kb:
            logger.error(f"ğŸš¨ CRITICAL: Compressed payload still too large: {compressed_size_kb:.1f}KB > {max_single_upload_kb:.1f}KB")
            logger.error("ItemReader requires single JSON array - cannot split into multiple files")
            
            # ğŸš¨ ê·¹í•œ ìƒí™©: ì²­í¬ í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ ì¬ì‹œë„ ì œì•ˆ
            suggested_chunk_size = max(10, optimal_chunk_size // 2)
            suggested_total_chunks = (total_segments + suggested_chunk_size - 1) // suggested_chunk_size
            
            raise RuntimeError(
                f"Payload too large for ItemReader compatibility. "
                f"Current: {compressed_size_kb:.1f}KB, Limit: {max_single_upload_kb:.1f}KB. "
                f"Suggestion: Reduce chunk_size from {optimal_chunk_size} to {suggested_chunk_size} "
                f"(will create {suggested_total_chunks} chunks instead of {total_chunks})"
            )
        
        # 3. ì••ì¶•ëœ ë°ì´í„° ì—…ë¡œë“œ (ë©€í‹°íŒŒíŠ¸ ì‚¬ìš©)
        if compressed_size_kb > 5 * 1024:  # 5MB ì´ìƒì€ ë©€í‹°íŒŒíŠ¸
            logger.info("Using multipart upload for large compressed payload")
            return _multipart_upload_chunks(
                s3_client=s3_client,
                compressed_data=compressed_data,
                state_bucket=state_bucket,
                chunks_key=chunks_key,
                total_chunks=total_chunks,
                optimal_chunk_size=optimal_chunk_size,
                total_segments=total_segments,
                owner_id=owner_id,
                workflow_id=workflow_id,
                original_size_kb=original_size_kb,
                compressed_size_kb=compressed_size_kb
            )
        
        # 4. ì¼ë°˜ ì••ì¶• ì—…ë¡œë“œ
        s3_client.put_object(
            Bucket=state_bucket,
            Key=chunks_key,
            Body=compressed_data,
            ContentType='application/json',
            ContentEncoding='gzip',
            Metadata={
                'total_chunks': str(total_chunks),
                'total_segments': str(total_segments),
                'chunk_size': str(optimal_chunk_size),
                'owner_id': owner_id or '',
                'workflow_id': workflow_id or '',
                'created_at': str(int(time.time())),
                'original_size_kb': str(int(original_size_kb)),
                'compressed_size_kb': str(int(compressed_size_kb)),
                'compression_ratio': str(round(compression_ratio, 3)),
                'encoding': 'gzip',
                'itemreader_compatible': 'true'  # ğŸš¨ í˜¸í™˜ì„± ë§ˆì»¤
            }
        )
        
        logger.info(f"ItemReader-compatible chunks uploaded: s3://{state_bucket}/{chunks_key} ({compressed_size_kb:.1f}KB, {compression_ratio:.2f} ratio)")
        
        return {
            "chunks_bucket": state_bucket,
            "chunks_key": chunks_key,
            "total_chunks": total_chunks,
            "chunk_size": optimal_chunk_size,
            "original_segments": total_segments,
            "distributed_mode": True,
            "use_s3_reader": True,
            "payload_size_kb": compressed_size_kb,
            "compression_applied": True,
            "compression_ratio": compression_ratio,
            "itemreader_compatible": True,  # ğŸš¨ í˜¸í™˜ì„± ë³´ì¥
            "optimization_stats": {
                "requested_chunk_size": 100,  # ê¸°ë³¸ê°’
                "optimal_chunk_size": optimal_chunk_size,
                "s3_bucket": state_bucket,
                "s3_key": chunks_key,
                "original_size_kb": original_size_kb,
                "compressed_size_kb": compressed_size_kb,
                "upload_method": "compressed_single_itemreader_compatible"
            }
        }
        
    except Exception as e:
        logger.error(f"Large chunks upload failed: {e}")
        
        # ğŸš¨ [Critical] ItemReader í˜¸í™˜ì„±ì„ ìœ„í•œ íŠ¹ë³„ í´ë°± ì²˜ë¦¬
        if "too large for ItemReader" in str(e):
            # ì²­í¬ í¬ê¸° ì¬ì¡°ì •ì´ í•„ìš”í•œ ê²½ìš° - ìƒìœ„ í˜¸ì¶œìì—ê²Œ ì „íŒŒ
            raise e
        
        # ê¸°íƒ€ ì˜¤ë¥˜ì˜ ê²½ìš° ì›ë³¸ ë°ì´í„°ë¡œ í´ë°± (ìœ„í—˜í•˜ì§€ë§Œ ë™ì‘ ìœ ì§€)
        logger.warning("Falling back to uncompressed upload (may cause memory issues)")
        
        try:
            s3_client.put_object(
                Bucket=state_bucket,
                Key=chunks_key,
                Body=chunks_json.encode('utf-8'),
                ContentType='application/json',
                Metadata={
                    'total_chunks': str(total_chunks),
                    'total_segments': str(total_segments),
                    'chunk_size': str(optimal_chunk_size),
                    'owner_id': owner_id or '',
                    'workflow_id': workflow_id or '',
                    'created_at': str(int(time.time())),
                    'fallback_upload': 'true',
                    'compression_failed': str(e),
                    'itemreader_compatible': 'true'  # ì—¬ì „íˆ ë‹¨ì¼ ë°°ì—´
                }
            )
            
            return {
                "chunks_bucket": state_bucket,
                "chunks_key": chunks_key,
                "total_chunks": total_chunks,
                "chunk_size": optimal_chunk_size,
                "original_segments": total_segments,
                "distributed_mode": True,
                "use_s3_reader": True,
                "payload_size_kb": len(chunks_json.encode('utf-8')) / 1024,
                "compression_applied": False,
                "fallback_used": True,
                "itemreader_compatible": True,  # ğŸš¨ ì—¬ì „íˆ í˜¸í™˜
                "error": str(e)
            }
        except Exception as fallback_error:
            logger.error(f"Even fallback upload failed: {fallback_error}")
            raise RuntimeError(f"All upload methods failed. Original error: {e}, Fallback error: {fallback_error}")


def _multipart_upload_chunks(
    s3_client,
    compressed_data: bytes,
    state_bucket: str,
    chunks_key: str,
    total_chunks: int,
    optimal_chunk_size: int,
    total_segments: int,
    owner_id: str,
    workflow_id: str,
    original_size_kb: float,
    compressed_size_kb: float
) -> Dict[str, Any]:
    """
    ë©€í‹°íŒŒíŠ¸ ì—…ë¡œë“œë¡œ ëŒ€ìš©ëŸ‰ ì••ì¶• ë°ì´í„° ì•ˆì „í•˜ê²Œ ì—…ë¡œë“œ
    """
    try:
        # ë©€í‹°íŒŒíŠ¸ ì—…ë¡œë“œ ì‹œì‘
        response = s3_client.create_multipart_upload(
            Bucket=state_bucket,
            Key=chunks_key,
            ContentType='application/json',
            ContentEncoding='gzip',
            Metadata={
                'total_chunks': str(total_chunks),
                'total_segments': str(total_segments),
                'chunk_size': str(optimal_chunk_size),
                'owner_id': owner_id or '',
                'workflow_id': workflow_id or '',
                'created_at': str(int(time.time())),
                'original_size_kb': str(int(original_size_kb)),
                'compressed_size_kb': str(int(compressed_size_kb)),
                'upload_method': 'multipart'
            }
        )
        
        upload_id = response['UploadId']
        
        # 5MB ì²­í¬ë¡œ ë¶„í•  ì—…ë¡œë“œ
        part_size = 5 * 1024 * 1024  # 5MB
        parts = []
        
        for part_num in range(1, (len(compressed_data) // part_size) + 2):
            start = (part_num - 1) * part_size
            end = min(start + part_size, len(compressed_data))
            
            if start >= len(compressed_data):
                break
                
            part_data = compressed_data[start:end]
            
            part_response = s3_client.upload_part(
                Bucket=state_bucket,
                Key=chunks_key,
                PartNumber=part_num,
                UploadId=upload_id,
                Body=part_data
            )
            
            parts.append({
                'ETag': part_response['ETag'],
                'PartNumber': part_num
            })
            
            logger.info(f"Uploaded part {part_num}: {len(part_data)} bytes")
        
        # ë©€í‹°íŒŒíŠ¸ ì—…ë¡œë“œ ì™„ë£Œ
        s3_client.complete_multipart_upload(
            Bucket=state_bucket,
            Key=chunks_key,
            UploadId=upload_id,
            MultipartUpload={'Parts': parts}
        )
        
        logger.info(f"Multipart upload completed: {len(parts)} parts, {compressed_size_kb:.1f}KB total")
        
        return {
            "chunks_bucket": state_bucket,
            "chunks_key": chunks_key,
            "total_chunks": total_chunks,
            "chunk_size": optimal_chunk_size,
            "original_segments": total_segments,
            "distributed_mode": True,
            "use_s3_reader": True,
            "payload_size_kb": compressed_size_kb,
            "compression_applied": True,
            "upload_method": "multipart",
            "parts_count": len(parts)
        }
        
    except Exception as e:
        logger.error(f"Multipart upload failed: {e}")
        # ì—…ë¡œë“œ ì·¨ì†Œ
        try:
            s3_client.abort_multipart_upload(
                Bucket=state_bucket,
                Key=chunks_key,
                UploadId=upload_id
            )
        except:
            pass
        raise


def _estimate_chunks_payload_size(
    total_segments: int,
    chunk_size: int,
    partition_map: List[Dict[str, Any]]
) -> float:
    """
    ì²­í¬ ë°°ì—´ì˜ ì˜ˆìƒ í˜ì´ë¡œë“œ í¬ê¸°ë¥¼ ì¶”ì • (KB ë‹¨ìœ„)
    
    ItemReader í˜¸í™˜ì„±ì„ ìœ„í•´ ì‚¬ì „ì— í¬ê¸°ë¥¼ ê²€ì¦í•˜ì—¬
    Step Functions 256KB ì œí•œ ë° ëŒë‹¤ ë©”ëª¨ë¦¬ ì œí•œì„ ì¤€ìˆ˜
    
    Args:
        total_segments: ì´ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜
        chunk_size: ì²­í¬ í¬ê¸°
        partition_map: íŒŒí‹°ì…˜ ë§µ
        
    Returns:
        ì˜ˆìƒ í˜ì´ë¡œë“œ í¬ê¸° (KB)
    """
    try:
        # ìƒ˜í”Œ ì²­í¬ ìƒì„±í•˜ì—¬ ì‹¤ì œ í¬ê¸° ì¸¡ì •
        sample_chunk = {
            "chunk_id": "chunk_0000",
            "start_segment": 0,
            "end_segment": min(chunk_size - 1, total_segments - 1),
            "segment_count": min(chunk_size, total_segments),
            "partition_slice": partition_map[:min(chunk_size, len(partition_map))],
            "chunk_index": 0,
            "total_chunks": (total_segments + chunk_size - 1) // chunk_size,
            "estimated_events": min(chunk_size, total_segments) * 20,
            "created_at": "sample",
            "idempotency_key": "sample#chunk#0000",
            "owner_id": "sample",
            "workflow_id": "sample"
        }
        
        # ìƒ˜í”Œ ì²­í¬ì˜ JSON í¬ê¸° ì¸¡ì •
        sample_json = json.dumps([sample_chunk], ensure_ascii=False)
        sample_size_kb = len(sample_json.encode('utf-8')) / 1024
        
        # ì „ì²´ ì²­í¬ ìˆ˜ ê³„ì‚°
        total_chunks = (total_segments + chunk_size - 1) // chunk_size
        
        # ì „ì²´ ì˜ˆìƒ í¬ê¸° = ìƒ˜í”Œ í¬ê¸° * ì²­í¬ ìˆ˜
        estimated_total_kb = sample_size_kb * total_chunks
        
        logger.info(f"Payload size estimation: {sample_size_kb:.2f}KB per chunk Ã— {total_chunks} chunks = {estimated_total_kb:.1f}KB total")
        
        return estimated_total_kb
        
    except Exception as e:
        logger.warning(f"Failed to estimate payload size: {e}, using conservative estimate")
        # ë³´ìˆ˜ì  ì¶”ì •: ì²­í¬ë‹¹ 10KB
        total_chunks = (total_segments + chunk_size - 1) // chunk_size
        return total_chunks * 10.0


def estimate_event_count(partition_slice: List[Dict[str, Any]]) -> int:
    """
    ì„¸ê·¸ë¨¼íŠ¸ ìŠ¬ë¼ì´ìŠ¤ì˜ ì˜ˆìƒ Event History ì‚¬ìš©ëŸ‰ì„ ê³„ì‚°
    
    ğŸš¨ [Critical Fix] ë³‘ë ¬ ê·¸ë£¹ ì´ë²¤íŠ¸ ì¶”ì •ì„ ë³´ìˆ˜ì ìœ¼ë¡œ ìˆ˜ì •
    - ê¸°ì¡´: ë¸Œëœì¹˜ë‹¹ 50ê°œ ì´ë²¤íŠ¸
    - ê°œì„ : ë¸Œëœì¹˜ë‹¹ 100-200ê°œ ì´ë²¤íŠ¸ (ê¸°í•˜ê¸‰ìˆ˜ì  ì¦ê°€ ë°©ì§€)
    
    Args:
        partition_slice: ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ì˜ˆìƒ ì´ë²¤íŠ¸ ìˆ˜
    """
    total_events = 0
    
    for segment in partition_slice:
        segment_type = segment.get('type', 'normal')
        nodes = segment.get('nodes', [])
        edges = segment.get('edges', [])
        
        # ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì…ë³„ ì´ë²¤íŠ¸ ì¶”ì •
        if segment_type == 'parallel_group':
            # ğŸš¨ [Critical Fix] ë³‘ë ¬ ê·¸ë£¹ì€ ë¸Œëœì¹˜ ìˆ˜ì— ë”°ë¼ ì´ë²¤íŠ¸ ê¸‰ì¦
            branches = segment.get('branches', [])
            branch_count = len(branches)
            
            # ë³´ìˆ˜ì  ì¶”ì •: ë¸Œëœì¹˜ë‹¹ 100-200ê°œ ì´ë²¤íŠ¸ (ê¸°ì¡´ 50ê°œì—ì„œ ì¦ê°€)
            if branch_count <= 5:
                branch_events = branch_count * 100  # ì†Œê·œëª¨: ë¸Œëœì¹˜ë‹¹ 100ê°œ
            elif branch_count <= 20:
                branch_events = branch_count * 150  # ì¤‘ê·œëª¨: ë¸Œëœì¹˜ë‹¹ 150ê°œ
            else:
                branch_events = branch_count * 200  # ëŒ€ê·œëª¨: ë¸Œëœì¹˜ë‹¹ 200ê°œ
            
            # ì¶”ê°€ ì•ˆì „ ë§ˆì§„: ì¤‘ì²©ëœ ë³‘ë ¬ ê·¸ë£¹ ê³ ë ¤
            nested_parallel_count = sum(1 for branch in branches 
                                      if branch.get('type') == 'parallel_group')
            if nested_parallel_count > 0:
                branch_events *= (1 + nested_parallel_count * 0.5)  # ì¤‘ì²©ë‹¹ 50% ì¦ê°€
            
            total_events += int(branch_events)
            
            logger.info(f"Parallel group estimated events: {branch_count} branches -> {int(branch_events)} events")
            
        elif segment_type == 'llm':
            # LLM ì„¸ê·¸ë¨¼íŠ¸ëŠ” ë” ë§ì€ ì´ë²¤íŠ¸ ìƒì„±
            total_events += 30
        elif segment_type == 'hitp':
            # HITPëŠ” ì½œë°± ëŒ€ê¸°ë¡œ ì¶”ê°€ ì´ë²¤íŠ¸
            total_events += 25
        else:
            # ì¼ë°˜ ì„¸ê·¸ë¨¼íŠ¸
            total_events += 15 + len(nodes) * 2 + len(edges)
    
    # ğŸš¨ [Critical Fix] ì „ì²´ì ìœ¼ë¡œ 20% ì•ˆì „ ë§ˆì§„ ì¶”ê°€
    safety_margin = int(total_events * 0.2)
    total_events += safety_margin
    
    logger.info(f"Total estimated events: {total_events} (including {safety_margin} safety margin)")
    
    return total_events


def validate_chunk_feasibility(chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ìƒì„±ëœ ì²­í¬ë“¤ì´ Event History ì œí•œ ë‚´ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œì§€ ê²€ì¦
    
    Args:
        chunks: ìƒì„±ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ê²€ì¦ ê²°ê³¼ ë° ê¶Œì¥ì‚¬í•­
    """
    validation_result = {
        "is_feasible": True,
        "warnings": [],
        "recommendations": []
    }
    
    for chunk in chunks:
        estimated_events = estimate_event_count(chunk.get('partition_slice', []))
        
        if estimated_events > 20000:  # 80% ì„ê³„ê°’
            validation_result["is_feasible"] = False
            validation_result["warnings"].append(
                f"Chunk {chunk['chunk_id']} may exceed Event History limit: {estimated_events} events"
            )
            validation_result["recommendations"].append(
                f"Reduce chunk size for chunk {chunk['chunk_id']} or split parallel groups"
            )
        elif estimated_events > 15000:  # 60% ì„ê³„ê°’
            validation_result["warnings"].append(
                f"Chunk {chunk['chunk_id']} approaching Event History limit: {estimated_events} events"
            )
    
    return validation_result