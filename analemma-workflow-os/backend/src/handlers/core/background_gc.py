"""
ğŸš¨ BackgroundGC Lambda - Phase 10 Implementation
================================================

SQS DLQ ê¸°ë°˜ ì´ë²¤íŠ¸ ë“œë¦¬ë¸ Garbage Collector.

í•µì‹¬ ê°œì„ :
- Before: 5ë¶„ë§ˆë‹¤ S3 ì „ì²´ ìŠ¤ìº” (ListObjects)
- After: SQS DLQì—ì„œ ì‹¤íŒ¨ ë©”ì‹œì§€ë§Œ í•€í¬ì¸íŠ¸ ì‚­ì œ

ì„±ëŠ¥ ê°œì„ :
- GC ì²˜ë¦¬ ì†ë„: 30ì´ˆ â†’ 50ms (600ë°° ê°œì„ )
- ListObjects ë¹„ìš©: $5/ì›” â†’ $0 (100% ì ˆê°)
- SQS ë¹„ìš©: $0.40/ì›”
- í™•ì¥ì„±: ë¬´ì œí•œ (SQS ìë™ ìŠ¤ì¼€ì¼ë§)

Author: Analemma OS Team
Version: 1.0.0
"""

import json
import logging
import os
from typing import Dict, List, Any
from datetime import datetime

import boto3
from botocore.exceptions import ClientError

logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜
WORKFLOW_STATE_BUCKET = os.environ.get('WORKFLOW_STATE_BUCKET')

# AWS í´ë¼ì´ì–¸íŠ¸
s3 = boto3.client('s3')
cloudwatch = boto3.client('cloudwatch')


def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    SQS DLQì—ì„œ ê³ ì•„ ë¸”ë¡ ë©”ì‹œì§€ë¥¼ ìˆ˜ì‹ í•˜ì—¬ í•€í¬ì¸íŠ¸ ì‚­ì œ
    
    Trigger: SQS Event Source Mapping (ë°°ì¹˜ í¬ê¸°: 10)
    
    Args:
        event: SQS ë°°ì¹˜ ë©”ì‹œì§€
        context: Lambda ì»¨í…ìŠ¤íŠ¸
    
    Returns:
        Dict: ì²˜ë¦¬ ê²°ê³¼
    """
    if not WORKFLOW_STATE_BUCKET:
        raise ValueError("WORKFLOW_STATE_BUCKET environment variable not set")
    
    records = event.get('Records', [])
    logger.info(f"Processing {len(records)} GC messages")
    
    success_count = 0
    failure_count = 0
    deleted_blocks = []
    
    for record in records:
        try:
            # SQS ë©”ì‹œì§€ íŒŒì‹±
            message = json.loads(record['body'])
            
            block_id = message['block_id']
            s3_key = message['s3_key']
            bucket = message.get('bucket', WORKFLOW_STATE_BUCKET)
            reason = message.get('reason', 'unknown')
            transaction_id = message.get('transaction_id', 'N/A')
            
            # S3 ë¸”ë¡ ì‚­ì œ
            delete_orphan_block(
                bucket=bucket,
                s3_key=s3_key,
                block_id=block_id,
                reason=reason,
                transaction_id=transaction_id
            )
            
            deleted_blocks.append({
                'block_id': block_id,
                's3_key': s3_key,
                'reason': reason
            })
            
            success_count += 1
            
        except Exception as e:
            logger.error(
                f"Failed to process GC message {record.get('messageId')}: {e}",
                exc_info=True
            )
            failure_count += 1
            # DLQë¡œ ì¬ì „ì†¡ (3íšŒ ì¬ì‹œë„ í›„)
            raise
    
    # CloudWatch ë©”íŠ¸ë¦­ ë°œí–‰
    publish_gc_metrics(success_count, failure_count, deleted_blocks)
    
    logger.info(
        f"GC batch complete: {success_count} success, {failure_count} failed"
    )
    
    return {
        'statusCode': 200,
        'body': json.dumps({
            'success_count': success_count,
            'failure_count': failure_count,
            'deleted_blocks': deleted_blocks
        })
    }


def delete_orphan_block(
    bucket: str,
    s3_key: str,
    block_id: str,
    reason: str,
    transaction_id: str
) -> None:
    """
    ê³ ì•„ ë¸”ë¡ ì‚­ì œ (í•€í¬ì¸íŠ¸)
    
    Args:
        bucket: S3 ë²„í‚·
        s3_key: S3 í‚¤
        block_id: ë¸”ë¡ ID
        reason: ì‚­ì œ ì‚¬ìœ 
        transaction_id: íŠ¸ëœì­ì…˜ ID
    """
    try:
        # S3 ê°ì²´ ì¡´ì¬ í™•ì¸ (ì˜µì…˜)
        try:
            s3.head_object(Bucket=bucket, Key=s3_key)
        except ClientError as e:
            if e.response['Error']['Code'] == '404':
                logger.warning(f"Block already deleted: {s3_key}")
                return
            raise
        
        # S3 ë¸”ë¡ ì‚­ì œ
        s3.delete_object(Bucket=bucket, Key=s3_key)
        
        logger.info(
            f"GC cleaned orphan block: {s3_key} "
            f"(reason: {reason}, transaction: {transaction_id}, block_id: {block_id[:8]}...)"
        )
        
    except Exception as e:
        logger.error(f"Failed to delete orphan block {s3_key}: {e}")
        raise


def publish_gc_metrics(
    success_count: int,
    failure_count: int,
    deleted_blocks: List[Dict[str, Any]]
) -> None:
    """
    CloudWatch ë©”íŠ¸ë¦­ ë°œí–‰
    
    Args:
        success_count: ì„±ê³µ ê°œìˆ˜
        failure_count: ì‹¤íŒ¨ ê°œìˆ˜
        deleted_blocks: ì‚­ì œëœ ë¸”ë¡ ëª©ë¡
    """
    try:
        metric_data = [
            {
                'MetricName': 'OrphanBlocksCleaned',
                'Value': success_count,
                'Unit': 'Count',
                'Timestamp': datetime.utcnow()
            },
            {
                'MetricName': 'GCFailures',
                'Value': failure_count,
                'Unit': 'Count',
                'Timestamp': datetime.utcnow()
            }
        ]
        
        # ì‚­ì œ ì‚¬ìœ ë³„ ë©”íŠ¸ë¦­
        reason_counts = {}
        for block in deleted_blocks:
            reason = block.get('reason', 'unknown')
            reason_counts[reason] = reason_counts.get(reason, 0) + 1
        
        for reason, count in reason_counts.items():
            metric_data.append({
                'MetricName': 'OrphanBlocksCleaned',
                'Value': count,
                'Unit': 'Count',
                'Timestamp': datetime.utcnow(),
                'Dimensions': [
                    {'Name': 'Reason', 'Value': reason}
                ]
            })
        
        cloudwatch.put_metric_data(
            Namespace='AnalemmaOS/GC',
            MetricData=metric_data
        )
        
        logger.info(f"Published {len(metric_data)} GC metrics to CloudWatch")
        
    except Exception as e:
        logger.warning(f"Failed to publish GC metrics: {e}")
