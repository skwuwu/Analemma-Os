"""
ë¶„ì‚° ì‹¤í–‰ì—ì„œ HITL Task Token ì €ì¥ Lambda

Distributed Mapì˜ ìì‹ ì‹¤í–‰ì—ì„œ waitForTaskToken ìƒíƒœì— ì§„ì…í•  ë•Œ
Task Tokenì„ DynamoDBì— ì €ì¥í•˜ì—¬ ì™¸ë¶€ì—ì„œ ì½œë°±í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

ì´ í•¨ìˆ˜ëŠ” Step Functionsì˜ .waitForTaskToken í†µí•©ì—ì„œ í˜¸ì¶œë©ë‹ˆë‹¤.

ğŸš¨ [Critical Fixes Applied]:

â‘  conversation_id ê³ ìœ ì„± ê²°í•¨ í•´ê²°:
- execution_idë¥¼ ì ‘ë‘ì–´ë¡œ í¬í•¨í•˜ì—¬ ì‚¬ìš©ì/ì›Œí¬í”Œë¡œìš° ê°„ ì¶©ëŒ ë°©ì§€
- ê³ ìœ ì„± ë³´ì¥ìœ¼ë¡œ Task Token ë®ì–´ì“°ê¸° ëŒ€ì°¸ì‚¬ ì˜ˆë°©

â‘¡ í° ìƒíƒœ(Large State) ì²˜ë¦¬ ì¼ê´€ì„±:
- Latest State Pointer ì „ëµ í™œìš©ìœ¼ë¡œ S3 URI ì°¸ì¡° ì €ì¥
- ResumeChunkProcessingFunctionì—ì„œ ì•ˆì „í•œ ìƒíƒœ ë³µêµ¬ ì§€ì›
"""

import json
import logging
import os
import time
from typing import Dict, Any

import boto3

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


def lambda_handler(event: Dict[str, Any], context: Any = None) -> Dict[str, Any]:
    """
    ë¶„ì‚° ì‹¤í–‰ì—ì„œ Task Token ì €ì¥
    
    ğŸš¨ [Critical Fixes]:
    - conversation_id ê³ ìœ ì„±: execution_id ì ‘ë‘ì–´ë¡œ ì¶©ëŒ ë°©ì§€
    - ëŒ€ìš©ëŸ‰ ìƒíƒœ ì²˜ë¦¬: S3 URI ì°¸ì¡°ë¡œ ì•ˆì „í•œ ë³µêµ¬ ì§€ì›
    
    Args:
        event: {
            "TaskToken": "arn:aws:states:...",
            "chunk_result": {...},
            "distributed_context": {
                "is_child_execution": true,
                "parent_execution_id": "exec-123",
                "chunk_id": "chunk_0001",
                "paused_segment_id": 42
            }
        }
    
    Returns:
        ì €ì¥ ê²°ê³¼ (ì½œë°± ì‹œ Step Functionsì— ì „ë‹¬ë¨)
    """
    try:
        task_token = event.get('TaskToken')
        chunk_result = event.get('chunk_result', {})
        distributed_context = event.get('distributed_context', {})
        
        chunk_id = distributed_context.get('chunk_id', chunk_result.get('chunk_id', 'unknown'))
        paused_segment_id = distributed_context.get('paused_segment_id')
        parent_execution_id = distributed_context.get('parent_execution_id')
        
        # ğŸš¨ [Critical Fix] ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ (ê³ ìœ ì„± ë³´ì¥ìš©)
        owner_id = event.get('owner_id') or distributed_context.get('owner_id')
        workflow_id = event.get('workflow_id') or distributed_context.get('workflow_id')
        
        logger.info(
            f"Storing task token for distributed HITL: "
            f"chunk={chunk_id}, segment={paused_segment_id}, "
            f"execution={parent_execution_id}, owner={owner_id}"
        )

        if not task_token:
            logger.error("No TaskToken provided")
            raise ValueError("TaskToken is required")
        
        if not parent_execution_id:
            logger.error("No parent_execution_id provided - required for uniqueness")
            raise ValueError("parent_execution_id is required for conversation_id uniqueness")
        
        # DynamoDBì— Task Token ì €ì¥
        dynamodb = boto3.resource('dynamodb')
        token_table_name = os.environ.get('TASK_TOKEN_TABLE') or os.environ.get('TASK_TOKENS_TABLE_NAME', 'TaskTokensTableV2')
        token_table = dynamodb.Table(token_table_name)
        
        timestamp = int(time.time())
        
        # ğŸš¨ [Critical Fix #1] conversation_id ê³ ìœ ì„± ë³´ì¥
        # execution_idë¥¼ ì ‘ë‘ì–´ë¡œ í¬í•¨í•˜ì—¬ ì‚¬ìš©ì/ì›Œí¬í”Œë¡œìš° ê°„ ì¶©ëŒ ë°©ì§€
        conversation_id = f"{parent_execution_id}_{chunk_id}"
        if paused_segment_id is not None:
            conversation_id = f"{conversation_id}_seg_{paused_segment_id}"
        
        # ì¶”ê°€ ì•ˆì „ì¥ì¹˜: owner_idë„ í¬í•¨ (ê·¹ë„ë¡œ ì•ˆì „í•œ ê³ ìœ ì„±)
        if owner_id:
            conversation_id = f"{owner_id}_{conversation_id}"
        
        logger.info(f"Generated unique conversation_id: {conversation_id}")
        
        # ğŸš¨ conversation_id ê³ ìœ ì„± ê²€ì¦
        uniqueness_validation = _validate_conversation_id_uniqueness(
            conversation_id=conversation_id,
            parent_execution_id=parent_execution_id,
            owner_id=owner_id,
            chunk_id=chunk_id,
            paused_segment_id=paused_segment_id
        )
        
        if not uniqueness_validation["is_unique"]:
            logger.error(f"conversation_id uniqueness validation failed: {uniqueness_validation['warnings']}")
            raise ValueError(f"conversation_id not unique enough: {uniqueness_validation['warnings']}")
        
        if uniqueness_validation["warnings"]:
            logger.warning(f"conversation_id warnings: {uniqueness_validation['warnings']}")
        
        if uniqueness_validation["collision_risk"] != "low":
            logger.warning(f"conversation_id collision risk: {uniqueness_validation['collision_risk']}")
        
        logger.info(f"conversation_id uniqueness validated: collision_risk={uniqueness_validation['collision_risk']}")
        
        token_item = {
            'conversation_id': conversation_id,
            'task_token': task_token,
            'chunk_id': chunk_id,
            'segment_id': paused_segment_id,
            'parent_execution_id': parent_execution_id,
            'owner_id': owner_id,
            'workflow_id': workflow_id,
            'distributed_execution': True,
            'is_child_execution': distributed_context.get('is_child_execution', True),
            'chunk_result_status': chunk_result.get('status'),
            'created_at': timestamp,
            'ttl': timestamp + 86400 * 7,  # 7ì¼ TTL
            'status': 'WAITING_FOR_CALLBACK'
        }
        
        # ğŸš¨ [Critical Fix #2] í° ìƒíƒœ(Large State) ì²˜ë¦¬ ì¼ê´€ì„±
        # Latest State Pointer ì „ëµ í™œìš©ìœ¼ë¡œ S3 URI ì°¸ì¡° ì €ì¥
        if chunk_result.get('final_state'):
            state_handling_result = _handle_chunk_final_state(
                final_state=chunk_result['final_state'],
                chunk_id=chunk_id,
                parent_execution_id=parent_execution_id,
                owner_id=owner_id,
                workflow_id=workflow_id,
                paused_segment_id=paused_segment_id
            )
            
            # ìƒíƒœ ì²˜ë¦¬ ê²°ê³¼ë¥¼ í† í° ì•„ì´í…œì— ë³‘í•©
            token_item.update(state_handling_result)
        
        token_table.put_item(Item=token_item)
        
        logger.info(
            f"Task token stored successfully: conversation_id={conversation_id}, "
            f"chunk={chunk_id}, segment={paused_segment_id}"
        )
        
        # ì½œë°± ì™„ë£Œ ì‹œ Step Functionsì— ì „ë‹¬ë  ê²°ê³¼
        return {
            "stored": True,
            "conversation_id": conversation_id,
            "chunk_id": chunk_id,
            "paused_segment_id": paused_segment_id,
            "parent_execution_id": parent_execution_id,
            "owner_id": owner_id,
            "workflow_id": workflow_id,
            "callback_info": {
                "table_name": token_table_name,
                "conversation_id": conversation_id,
                "instructions": "Use SendTaskSuccess with this conversation_id to resume",
                "uniqueness_guaranteed": True  # ğŸš¨ ê³ ìœ ì„± ë³´ì¥ í™•ì¸
            }
        }
        
    except Exception as e:
        logger.exception(f"Failed to store task token: {e}")
        raise


def _handle_chunk_final_state(
    final_state: Dict[str, Any],
    chunk_id: str,
    parent_execution_id: str,
    owner_id: str,
    workflow_id: str,
    paused_segment_id: int
) -> Dict[str, Any]:
    """
    ğŸš¨ [Critical Fix #2] í° ìƒíƒœ(Large State) ì²˜ë¦¬ ì¼ê´€ì„±
    
    Latest State Pointer ì „ëµì„ í™œìš©í•˜ì—¬ ëŒ€ìš©ëŸ‰ ìƒíƒœë¥¼ S3ì— ì €ì¥í•˜ê³ 
    URI ì°¸ì¡°ë¥¼ DynamoDBì— ì €ì¥í•˜ì—¬ ResumeChunkProcessingFunctionì—ì„œ
    ì•ˆì „í•˜ê²Œ ìƒíƒœë¥¼ ë³µêµ¬í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
    
    Args:
        final_state: ì²­í¬ì˜ ìµœì¢… ìƒíƒœ
        chunk_id: ì²­í¬ ID
        parent_execution_id: ë¶€ëª¨ ì‹¤í–‰ ID
        owner_id: ì†Œìœ ì ID
        workflow_id: ì›Œí¬í”Œë¡œìš° ID
        paused_segment_id: ì¤‘ë‹¨ëœ ì„¸ê·¸ë¨¼íŠ¸ ID
    
    Returns:
        DynamoDB ì•„ì´í…œì— ì¶”ê°€í•  ìƒíƒœ ê´€ë ¨ í•„ë“œë“¤
    """
    try:
        state_json = json.dumps(final_state, default=str, ensure_ascii=False)
        state_size_bytes = len(state_json.encode('utf-8'))
        
        # Step Functions ë° DynamoDB ì œí•œì„ ê³ ë ¤í•œ ì„ê³„ê°’
        # 400KBë¥¼ ë„˜ìœ¼ë©´ S3ì— ì €ì¥ (DynamoDB 400KB ì œí•œ ê³ ë ¤)
        SIZE_LIMIT_BYTES = 400 * 1024  # 400KB
        
        logger.info(f"Chunk final state size: {state_size_bytes} bytes (limit: {SIZE_LIMIT_BYTES})")
        
        if state_size_bytes <= SIZE_LIMIT_BYTES:
            # ì‘ì€ ìƒíƒœëŠ” DynamoDBì— ì§ì ‘ ì €ì¥
            logger.info(f"Storing state inline for chunk {chunk_id} ({state_size_bytes} bytes)")
            return {
                'chunk_final_state': state_json,
                'state_storage_type': 'inline',
                'state_size_bytes': state_size_bytes
            }
        
        # ğŸš¨ [Critical Fix] ëŒ€ìš©ëŸ‰ ìƒíƒœëŠ” S3ì— ì €ì¥í•˜ê³  URI ì°¸ì¡°
        logger.warning(f"Large state detected for chunk {chunk_id} ({state_size_bytes} bytes), storing in S3")
        
        state_bucket = os.environ.get('WORKFLOW_STATE_BUCKET')
        if not state_bucket:
            logger.error("No WORKFLOW_STATE_BUCKET configured for large state storage")
            # í´ë°±: ìƒíƒœ ìš”ì•½ë§Œ ì €ì¥
            return {
                'state_too_large': True,
                'state_size_bytes': state_size_bytes,
                'state_storage_type': 'too_large_no_bucket',
                'error': 'No S3 bucket configured for large state storage',
                'state_summary': _create_state_summary_for_token(final_state)
            }
        
        # S3ì— ìƒíƒœ ì €ì¥
        s3_uri = _store_large_state_in_s3_for_token(
            state_data=final_state,
            state_json=state_json,
            state_bucket=state_bucket,
            chunk_id=chunk_id,
            parent_execution_id=parent_execution_id,
            owner_id=owner_id,
            workflow_id=workflow_id,
            paused_segment_id=paused_segment_id
        )
        
        if s3_uri:
            # ğŸš¨ [Critical] S3 URI ì°¸ì¡°ë¡œ ResumeChunkProcessingFunctionì—ì„œ ë³µêµ¬ ê°€ëŠ¥
            logger.info(f"Large state stored in S3 for chunk {chunk_id}: {s3_uri}")
            return {
                'chunk_final_state_s3_path': s3_uri,  # ğŸš¨ í•µì‹¬: ì¬ê°œ ì‹œ ì‚¬ìš©í•  S3 ê²½ë¡œ
                'state_storage_type': 's3_reference',
                'state_size_bytes': state_size_bytes,
                'state_summary': _create_state_summary_for_token(final_state),
                's3_stored_at': int(time.time())
            }
        else:
            # S3 ì €ì¥ ì‹¤íŒ¨ ì‹œ í´ë°±
            logger.error(f"Failed to store large state in S3 for chunk {chunk_id}")
            return {
                'state_too_large': True,
                'state_size_bytes': state_size_bytes,
                'state_storage_type': 's3_storage_failed',
                'error': 'Failed to store large state in S3',
                'state_summary': _create_state_summary_for_token(final_state)
            }
        
    except Exception as e:
        logger.error(f"Error handling chunk final state: {e}")
        return {
            'state_handling_error': str(e),
            'state_storage_type': 'error',
            'state_summary': _create_state_summary_for_token(final_state) if final_state else {}
        }


def _store_large_state_in_s3_for_token(
    state_data: Dict[str, Any],
    state_json: str,
    state_bucket: str,
    chunk_id: str,
    parent_execution_id: str,
    owner_id: str,
    workflow_id: str,
    paused_segment_id: int
) -> str:
    """
    ëŒ€ìš©ëŸ‰ ìƒíƒœë¥¼ S3ì— ì €ì¥í•˜ê³  URI ë°˜í™˜
    
    Latest State Pointer ì „ëµê³¼ ì¼ê´€ëœ ê²½ë¡œ êµ¬ì¡° ì‚¬ìš©
    
    Args:
        state_data: ìƒíƒœ ë°ì´í„°
        state_json: JSON ë¬¸ìì—´
        state_bucket: S3 ë²„í‚·
        chunk_id: ì²­í¬ ID
        parent_execution_id: ë¶€ëª¨ ì‹¤í–‰ ID
        owner_id: ì†Œìœ ì ID
        workflow_id: ì›Œí¬í”Œë¡œìš° ID
        paused_segment_id: ì¤‘ë‹¨ëœ ì„¸ê·¸ë¨¼íŠ¸ ID
    
    Returns:
        S3 URI (ì„±ê³µ ì‹œ) ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    try:
        s3_client = boto3.client('s3')
        
        # Latest State Pointerì™€ ì¼ê´€ëœ ê²½ë¡œ êµ¬ì¡° ì‚¬ìš©
        timestamp = int(time.time())
        state_key = (
            f"distributed-states/{owner_id}/{workflow_id}/{parent_execution_id}/"
            f"chunks/{chunk_id}/paused_state_seg_{paused_segment_id}_{timestamp}.json"
        )
        
        # ë©”íƒ€ë°ì´í„°ì— ë³µêµ¬ì— í•„ìš”í•œ ì •ë³´ í¬í•¨
        metadata = {
            'chunk_id': chunk_id,
            'parent_execution_id': parent_execution_id,
            'owner_id': owner_id or '',
            'workflow_id': workflow_id or '',
            'paused_segment_id': str(paused_segment_id),
            'storage_type': 'task_token_large_state',
            'created_at': str(timestamp),
            'original_size_bytes': str(len(state_json.encode('utf-8'))),
            'resumable': 'true'  # ğŸš¨ ì¬ê°œ ê°€ëŠ¥ í‘œì‹œ
        }
        
        # S3ì— ì €ì¥
        s3_client.put_object(
            Bucket=state_bucket,
            Key=state_key,
            Body=state_json.encode('utf-8'),
            ContentType='application/json',
            Metadata=metadata
        )
        
        s3_uri = f"s3://{state_bucket}/{state_key}"
        logger.info(f"Large state stored for task token: {s3_uri}")
        
        return s3_uri
        
    except Exception as e:
        logger.error(f"Failed to store large state in S3: {e}")
        return None


def _create_state_summary_for_token(final_state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Task Tokenìš© ìƒíƒœ ìš”ì•½ ìƒì„± (DynamoDB í¬ê¸° ì œí•œ ì¤€ìˆ˜)
    
    Args:
        final_state: ì›ë³¸ ìƒíƒœ ë°ì´í„°
    
    Returns:
        ìƒíƒœ ìš”ì•½ (ì‘ì€ í¬ê¸°)
    """
    try:
        if not isinstance(final_state, dict):
            return {"type": type(final_state).__name__, "size": len(str(final_state))}
        
        summary = {
            "latest_segment_id": final_state.get('__latest_segment_id'),
            "has_callback_input": '__callback_input' in final_state,
            "last_callback_timestamp": final_state.get('__last_callback', {}).get('timestamp'),
            "segment_count": len(final_state.get('segments', [])),
            "total_keys": len(final_state.keys()),
            "state_keys_sample": list(final_state.keys())[:5]  # ì²˜ìŒ 5ê°œ í‚¤ë§Œ
        }
        
        # ì¤‘ìš”í•œ ì‘ì€ í•„ë“œë“¤ì€ ì§ì ‘ í¬í•¨ (ì¬ê°œ ì‹œ ìœ ìš©)
        important_fields = ['selected_option', 'user_response', 'approval', 'status']
        for field in important_fields:
            if field in final_state:
                value = final_state[field]
                if isinstance(value, (str, int, bool, float)) and len(str(value)) < 50:
                    summary[field] = value
        
        return summary
        
    except Exception as e:
        logger.error(f"Failed to create state summary: {e}")
        return {"error": "Summary creation failed", "original_type": type(final_state).__name__}


def _validate_conversation_id_uniqueness(
    conversation_id: str,
    parent_execution_id: str,
    owner_id: str,
    chunk_id: str,
    paused_segment_id: int
) -> Dict[str, Any]:
    """
    ğŸš¨ [Critical Fix #1] conversation_id ê³ ìœ ì„± ê²€ì¦
    
    ìƒì„±ëœ conversation_idê°€ ì¶©ë¶„íˆ ê³ ìœ í•œì§€ ê²€ì¦í•˜ê³ 
    ì¶©ëŒ ìœ„í—˜ì„ í‰ê°€í•©ë‹ˆë‹¤.
    
    Args:
        conversation_id: ìƒì„±ëœ conversation_id
        parent_execution_id: ë¶€ëª¨ ì‹¤í–‰ ID
        owner_id: ì†Œìœ ì ID
        chunk_id: ì²­í¬ ID
        paused_segment_id: ì¤‘ë‹¨ëœ ì„¸ê·¸ë¨¼íŠ¸ ID
    
    Returns:
        ê²€ì¦ ê²°ê³¼ ë° ê¶Œì¥ì‚¬í•­
    """
    try:
        validation = {
            "is_unique": True,
            "warnings": [],
            "recommendations": [],
            "collision_risk": "low"
        }
        
        # 1. í•„ìˆ˜ êµ¬ì„± ìš”ì†Œ í™•ì¸
        required_components = [parent_execution_id, chunk_id]
        if paused_segment_id is not None:
            required_components.append(str(paused_segment_id))
        
        missing_components = []
        for component in required_components:
            if not component or str(component) not in conversation_id:
                missing_components.append(component)
        
        if missing_components:
            validation["is_unique"] = False
            validation["warnings"].append(f"Missing components in conversation_id: {missing_components}")
            validation["collision_risk"] = "high"
        
        # 2. owner_id í¬í•¨ ì—¬ë¶€ í™•ì¸ (ì¶”ê°€ ì•ˆì „ì¥ì¹˜)
        if owner_id and owner_id not in conversation_id:
            validation["warnings"].append("owner_id not included - consider adding for extra uniqueness")
            validation["collision_risk"] = "medium"
        
        # 3. ê¸¸ì´ ê²€ì¦ (DynamoDB í‚¤ ì œí•œ)
        if len(conversation_id) > 2048:  # DynamoDB í‚¤ ê¸¸ì´ ì œí•œ
            validation["is_unique"] = False
            validation["warnings"].append(f"conversation_id too long: {len(conversation_id)} chars")
        
        # 4. íŠ¹ìˆ˜ ë¬¸ì ê²€ì¦
        if any(char in conversation_id for char in ['/', '\\', '?', '#']):
            validation["warnings"].append("Special characters in conversation_id may cause issues")
        
        # 5. ê¶Œì¥ì‚¬í•­ ìƒì„±
        if validation["collision_risk"] == "high":
            validation["recommendations"].append("Include execution_id and owner_id in conversation_id")
        elif validation["collision_risk"] == "medium":
            validation["recommendations"].append("Consider including owner_id for extra uniqueness")
        
        if len(conversation_id) < 20:
            validation["recommendations"].append("conversation_id may be too short for guaranteed uniqueness")
        
        return validation
        
    except Exception as e:
        logger.error(f"conversation_id validation failed: {e}")
        return {
            "is_unique": False,
            "warnings": [f"Validation failed: {str(e)}"],
            "recommendations": ["Use fallback conversation_id generation"],
            "collision_risk": "unknown"
        }