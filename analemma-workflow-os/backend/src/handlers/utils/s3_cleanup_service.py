"""
S3 ì •ë¦¬ ì„œë¹„ìŠ¤ - ì‹¤íŒ¨í•œ ì›Œí¬í”Œë¡œìš°ì˜ S3 íŒŒì¼ ì •ë¦¬

í…ŒìŠ¤íŠ¸ ê³„íš ìš”êµ¬ì‚¬í•­:
- ì‹¤íŒ¨í•œ ì‹¤í–‰ì˜ S3 ë°ì´í„° ì •ë¦¬
- Lifecycle ì •ì±… ì ìš© í™•ì¸
- ë¹„ìš© ëˆ„ìˆ˜ ë°©ì§€
"""

import os
import boto3
import logging
import json
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Optional
from botocore.exceptions import ClientError

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    from common.aws_clients import get_s3_client
    s3_client = get_s3_client()
except ImportError:
    s3_client = boto3.client('s3')


def cleanup_failed_execution_s3_files(
    execution_arn: str, 
    owner_id: str, 
    workflow_id: str,
    dry_run: bool = False
) -> Dict[str, Any]:
    """
    ì‹¤íŒ¨í•œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ì˜ S3 íŒŒì¼ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        execution_arn: Step Functions ì‹¤í–‰ ARN
        owner_id: ì‚¬ìš©ì ID (í…Œë„ŒíŠ¸ ê²©ë¦¬)
        workflow_id: ì›Œí¬í”Œë¡œìš° ID
        dry_run: Trueë©´ ì‹¤ì œ ì‚­ì œí•˜ì§€ ì•Šê³  ëª©ë¡ë§Œ ë°˜í™˜
        
    Returns:
        dict: ì •ë¦¬ ê²°ê³¼ ì •ë³´
    """
    cleanup_result = {
        'execution_arn': execution_arn,
        'owner_id': owner_id,
        'workflow_id': workflow_id,
        'files_found': 0,
        'files_deleted': 0,
        'bytes_freed': 0,
        'errors': [],
        'dry_run': dry_run,
        'cleanup_timestamp': datetime.now(timezone.utc).isoformat()
    }
    
    try:
        bucket = os.environ.get('WORKFLOW_STATE_BUCKET') or os.environ.get('SKELETON_S3_BUCKET')
        if not bucket:
            cleanup_result['errors'].append("No S3 bucket configured for cleanup")
            return cleanup_result
        
        # ì‹¤í–‰ ID ì¶”ì¶œ (ARNì—ì„œ)
        execution_id = execution_arn.split(':')[-1] if execution_arn else 'unknown'
        
        # ì •ë¦¬í•  S3 ê²½ë¡œ íŒ¨í„´ë“¤
        cleanup_patterns = [
            f"workflow-states/{owner_id}/{workflow_id}/",
            f"workflow-manifests/{owner_id}/{workflow_id}/",
            f"workflow-partitions/{owner_id}/{workflow_id}/",
            f"segment-outputs/{owner_id}/{workflow_id}/",
            f"execution-inputs/{owner_id}/{workflow_id}/",
            f"merge-states/{owner_id}/{workflow_id}/",
            f"temp-states/{owner_id}/{execution_id}/",
        ]
        
        total_files = 0
        total_bytes = 0
        deleted_files = 0
        
        for pattern in cleanup_patterns:
            try:
                # S3 ê°ì²´ ëª©ë¡ ì¡°íšŒ
                paginator = s3_client.get_paginator('list_objects_v2')
                pages = paginator.paginate(Bucket=bucket, Prefix=pattern)
                
                for page in pages:
                    objects = page.get('Contents', [])
                    
                    for obj in objects:
                        key = obj['Key']
                        size = obj['Size']
                        last_modified = obj['LastModified']
                        
                        total_files += 1
                        total_bytes += size
                        
                        logger.info(f"Found S3 object: s3://{bucket}/{key} ({size} bytes, modified: {last_modified})")
                        
                        # ì‹¤ì œ ì‚­ì œ (dry_runì´ ì•„ë‹Œ ê²½ìš°)
                        if not dry_run:
                            try:
                                s3_client.delete_object(Bucket=bucket, Key=key)
                                deleted_files += 1
                                logger.info(f"âœ… Deleted: s3://{bucket}/{key}")
                            except ClientError as e:
                                error_msg = f"Failed to delete s3://{bucket}/{key}: {e}"
                                cleanup_result['errors'].append(error_msg)
                                logger.error(error_msg)
                        else:
                            logger.info(f"ğŸ” Would delete: s3://{bucket}/{key} (dry run)")
                            
            except ClientError as e:
                error_msg = f"Failed to list objects with pattern {pattern}: {e}"
                cleanup_result['errors'].append(error_msg)
                logger.error(error_msg)
        
        cleanup_result.update({
            'files_found': total_files,
            'files_deleted': deleted_files if not dry_run else 0,
            'bytes_freed': total_bytes if not dry_run else 0
        })
        
        if total_files > 0:
            logger.info(f"ğŸ§¹ S3 cleanup completed: {deleted_files}/{total_files} files deleted, {total_bytes:,} bytes freed")
        else:
            logger.info("ğŸ” No S3 files found for cleanup")
            
    except Exception as e:
        error_msg = f"S3 cleanup failed: {e}"
        cleanup_result['errors'].append(error_msg)
        logger.error(error_msg)
    
    return cleanup_result


def cleanup_old_workflow_files(
    owner_id: str,
    days_old: int = 7,
    dry_run: bool = False
) -> Dict[str, Any]:
    """
    ì§€ì •ëœ ê¸°ê°„ë³´ë‹¤ ì˜¤ë˜ëœ ì›Œí¬í”Œë¡œìš° íŒŒì¼ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        owner_id: ì‚¬ìš©ì ID (í…Œë„ŒíŠ¸ ê²©ë¦¬)
        days_old: ì‚­ì œí•  íŒŒì¼ì˜ ìµœì†Œ ë‚˜ì´ (ì¼)
        dry_run: Trueë©´ ì‹¤ì œ ì‚­ì œí•˜ì§€ ì•Šê³  ëª©ë¡ë§Œ ë°˜í™˜
        
    Returns:
        dict: ì •ë¦¬ ê²°ê³¼ ì •ë³´
    """
    cleanup_result = {
        'owner_id': owner_id,
        'days_old': days_old,
        'files_found': 0,
        'files_deleted': 0,
        'bytes_freed': 0,
        'errors': [],
        'dry_run': dry_run,
        'cleanup_timestamp': datetime.now(timezone.utc).isoformat()
    }
    
    try:
        bucket = os.environ.get('WORKFLOW_STATE_BUCKET') or os.environ.get('SKELETON_S3_BUCKET')
        if not bucket:
            cleanup_result['errors'].append("No S3 bucket configured for cleanup")
            return cleanup_result
        
        # ê¸°ì¤€ ë‚ ì§œ ê³„ì‚°
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=days_old)
        
        # ì‚¬ìš©ìë³„ ê²½ë¡œ íŒ¨í„´
        user_patterns = [
            f"workflow-states/{owner_id}/",
            f"workflow-manifests/{owner_id}/",
            f"workflow-partitions/{owner_id}/",
            f"segment-outputs/{owner_id}/",
            f"execution-inputs/{owner_id}/",
            f"merge-states/{owner_id}/",
            f"temp-states/{owner_id}/",
        ]
        
        total_files = 0
        total_bytes = 0
        deleted_files = 0
        
        for pattern in user_patterns:
            try:
                paginator = s3_client.get_paginator('list_objects_v2')
                pages = paginator.paginate(Bucket=bucket, Prefix=pattern)
                
                for page in pages:
                    objects = page.get('Contents', [])
                    
                    for obj in objects:
                        key = obj['Key']
                        size = obj['Size']
                        last_modified = obj['LastModified']
                        
                        # ë‚ ì§œ ë¹„êµ (timezone-aware)
                        if last_modified.replace(tzinfo=timezone.utc) < cutoff_date:
                            total_files += 1
                            total_bytes += size
                            
                            logger.info(f"Found old S3 object: s3://{bucket}/{key} ({size} bytes, age: {(datetime.now(timezone.utc) - last_modified.replace(tzinfo=timezone.utc)).days} days)")
                            
                            if not dry_run:
                                try:
                                    s3_client.delete_object(Bucket=bucket, Key=key)
                                    deleted_files += 1
                                    logger.info(f"âœ… Deleted old file: s3://{bucket}/{key}")
                                except ClientError as e:
                                    error_msg = f"Failed to delete s3://{bucket}/{key}: {e}"
                                    cleanup_result['errors'].append(error_msg)
                                    logger.error(error_msg)
                            else:
                                logger.info(f"ğŸ” Would delete old file: s3://{bucket}/{key} (dry run)")
                                
            except ClientError as e:
                error_msg = f"Failed to list objects with pattern {pattern}: {e}"
                cleanup_result['errors'].append(error_msg)
                logger.error(error_msg)
        
        cleanup_result.update({
            'files_found': total_files,
            'files_deleted': deleted_files if not dry_run else 0,
            'bytes_freed': total_bytes if not dry_run else 0
        })
        
        if total_files > 0:
            logger.info(f"ğŸ§¹ Old files cleanup completed: {deleted_files}/{total_files} files deleted, {total_bytes:,} bytes freed")
        else:
            logger.info("ğŸ” No old files found for cleanup")
            
    except Exception as e:
        error_msg = f"Old files cleanup failed: {e}"
        cleanup_result['errors'].append(error_msg)
        logger.error(error_msg)
    
    return cleanup_result


def apply_s3_lifecycle_policy(bucket_name: str) -> Dict[str, Any]:
    """
    S3 ë²„í‚·ì— Lifecycle ì •ì±…ì„ ì ìš©í•˜ì—¬ ìë™ ì •ë¦¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    
    Args:
        bucket_name: S3 ë²„í‚· ì´ë¦„
        
    Returns:
        dict: ì •ì±… ì ìš© ê²°ê³¼
    """
    policy_result = {
        'bucket_name': bucket_name,
        'policy_applied': False,
        'errors': [],
        'timestamp': datetime.now(timezone.utc).isoformat()
    }
    
    try:
        # Lifecycle ì •ì±… ì •ì˜
        lifecycle_policy = {
            'Rules': [
                {
                    'ID': 'WorkflowTempFilesCleanup',
                    'Status': 'Enabled',
                    'Filter': {
                        'Prefix': 'temp-states/'
                    },
                    'Expiration': {
                        'Days': 1  # ì„ì‹œ íŒŒì¼ì€ 1ì¼ í›„ ì‚­ì œ
                    }
                },
                {
                    'ID': 'WorkflowStateFilesCleanup',
                    'Status': 'Enabled',
                    'Filter': {
                        'Prefix': 'workflow-states/'
                    },
                    'Expiration': {
                        'Days': 30  # ì›Œí¬í”Œë¡œìš° ìƒíƒœëŠ” 30ì¼ í›„ ì‚­ì œ
                    }
                },
                {
                    'ID': 'ExecutionInputsCleanup',
                    'Status': 'Enabled',
                    'Filter': {
                        'Prefix': 'execution-inputs/'
                    },
                    'Expiration': {
                        'Days': 7  # ì‹¤í–‰ ì…ë ¥ì€ 7ì¼ í›„ ì‚­ì œ
                    }
                },
                {
                    'ID': 'SegmentOutputsCleanup',
                    'Status': 'Enabled',
                    'Filter': {
                        'Prefix': 'segment-outputs/'
                    },
                    'Expiration': {
                        'Days': 14  # ì„¸ê·¸ë¨¼íŠ¸ ì¶œë ¥ì€ 14ì¼ í›„ ì‚­ì œ
                    }
                }
            ]
        }
        
        # ì •ì±… ì ìš©
        s3_client.put_bucket_lifecycle_configuration(
            Bucket=bucket_name,
            LifecycleConfiguration=lifecycle_policy
        )
        
        policy_result['policy_applied'] = True
        logger.info(f"âœ… S3 Lifecycle policy applied to bucket: {bucket_name}")
        
    except ClientError as e:
        error_msg = f"Failed to apply lifecycle policy to {bucket_name}: {e}"
        policy_result['errors'].append(error_msg)
        logger.error(error_msg)
    except Exception as e:
        error_msg = f"Unexpected error applying lifecycle policy: {e}"
        policy_result['errors'].append(error_msg)
        logger.error(error_msg)
    
    return policy_result


def lambda_handler(event, context):
    """
    S3 ì •ë¦¬ Lambda í•¸ë“¤ëŸ¬
    
    EventBridgeì—ì„œ í˜¸ì¶œë˜ì–´ ì‹¤íŒ¨í•œ ì›Œí¬í”Œë¡œìš°ì˜ S3 íŒŒì¼ì„ ì •ë¦¬í•©ë‹ˆë‹¤.
    """
    logger.info("S3 cleanup service started")
    
    try:
        # EventBridge ì´ë²¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
        detail = event.get('detail', {})
        execution_arn = detail.get('executionArn')
        execution_status = detail.get('status')
        
        # ì‹¤í–‰ ì…ë ¥ì—ì„œ owner_id, workflow_id ì¶”ì¶œ
        input_raw = detail.get('input')
        if isinstance(input_raw, str):
            try:
                input_obj = json.loads(input_raw)
            except json.JSONDecodeError:
                input_obj = {}
        else:
            input_obj = input_raw or {}
        
        owner_id = input_obj.get('ownerId')
        workflow_id = input_obj.get('workflowId')
        
        # ì‹¤íŒ¨í•œ ì‹¤í–‰ë§Œ ì •ë¦¬
        if execution_status in ['FAILED', 'TIMED_OUT', 'ABORTED']:
            if execution_arn and owner_id and workflow_id:
                cleanup_result = cleanup_failed_execution_s3_files(
                    execution_arn=execution_arn,
                    owner_id=owner_id,
                    workflow_id=workflow_id,
                    dry_run=False
                )
                
                logger.info(f"S3 cleanup completed for failed execution: {cleanup_result}")
                
                return {
                    'statusCode': 200,
                    'body': json.dumps(cleanup_result)
                }
            else:
                logger.warning(f"Missing required fields for cleanup: execution_arn={bool(execution_arn)}, owner_id={bool(owner_id)}, workflow_id={bool(workflow_id)}")
        else:
            logger.info(f"Execution status '{execution_status}' does not require cleanup")
        
        return {
            'statusCode': 200,
            'body': json.dumps({'message': 'No cleanup required'})
        }
        
    except Exception as e:
        logger.error(f"S3 cleanup service failed: {e}")
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }