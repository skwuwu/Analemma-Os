"""
ğŸ¯ Universal Sync Core - Function-Agnostic ë°ì´í„° íŒŒì´í”„ë¼ì¸

v3.3 - "Unified Pipe: íƒ„ìƒë¶€í„° ì†Œë©¸ê¹Œì§€"

í•µì‹¬ ì›ì¹™:
    "í•¨ìˆ˜ê°€ ë¬´ì—‡ì´ë“  ìƒê´€ì—†ì´, ë°ì´í„°ê°€ íë¥´ëŠ” íŒŒì´í”„ ìì²´ë¥¼ í‘œì¤€í™”"
    
    ëª¨ë“  ì•¡ì…˜ í•¨ìˆ˜ëŠ” ì´ì œ "3ì¤„ì§œë¦¬ ë˜í¼"ì…ë‹ˆë‹¤:
        1. ì…ë ¥ ì „ì²˜ë¦¬ (ì•¡ì…˜ë³„ íŠ¹ìˆ˜ í•„ë“œ ì¶”ì¶œ)
        2. universal_sync_core() í˜¸ì¶œ
        3. ì‘ë‹µ í¬ë§·íŒ…

ëª¨ë“  StateDataManager ì•¡ì…˜ì€ ì´ ì½”ì–´ë¥¼ í†µê³¼í•©ë‹ˆë‹¤:
    1. flatten_result() - ì…ë ¥ ì •ê·œí™” (ì•¡ì…˜ë³„ ìŠ¤ë§ˆíŠ¸ ì¶”ì¶œ)
    2. merge_logic() - ìƒíƒœ ë³‘í•© (Shallow Merge + Copy-on-Write)
    3. optimize_and_offload() - ìë™ ìµœì í™” (P0~P2 ìë™ í•´ê²°)

ë°ì´í„° ìƒì•  ì£¼ê¸° (Unified Pipe):
    - íƒ„ìƒ (Init): {} â†’ Universal Sync â†’ StateBag v0
    - ì„±ì¥ (Sync): StateBag vN + Result â†’ Universal Sync â†’ StateBag vN+1
    - í˜‘ì—… (Aggregate): StateBag vN + Branches â†’ Universal Sync â†’ StateBag vFinal

ì„±ëŠ¥ ìµœì í™”:
    - â‘  Copy-on-Write: ì „ì²´ deepcopy ëŒ€ì‹  ë³€ê²½ëœ ì„œë¸ŒíŠ¸ë¦¬ë§Œ ë³µì‚¬
    - â‘¡ Shallow Merge: ë¶ˆí•„ìš”í•œ ì¤‘ì²© ë³µì‚¬ ë°©ì§€
    - â‘¢ Checksum ê²€ì¦: S3 ë¡œë“œ ì‹œ ë°ì´í„° ë¬´ê²°ì„± í™•ì¸

P0~P2 ìë™ í•´ê²°:
    - P0: ì–´ë–¤ ê²½ë¡œë¡œ ë“¤ì–´ì™”ë“  í¬ë©´ S3ë¡œ ê°„ë‹¤ (T=0 ê°€ë“œë ˆì¼ í¬í•¨)
    - P1: í¬ì¸í„° ë¹„ëŒ€í™” ë°©ì§€ (ëª¨ë“  ì•¡ì…˜ì— ìë™ ì ìš©)
    - P2: ìŠ¤ëƒ…ìƒ·ë„ ì½”ì–´ í†µê³¼ â†’ ì¤‘ë³µ ì €ì¥ ë°©ì§€

v3.3 - 2026-01-29 (Unified Pipe: Day-Zero Sync)
"""

import json
import hashlib
import time
from typing import Dict, Any, Optional, List, Callable, TypedDict, Literal
from datetime import datetime, timezone
from abc import ABC, abstractmethod

# Lazy imports to avoid circular dependencies
_logger = None
_s3_client = None
_S3_BUCKET = None

def _get_logger():
    global _logger
    if _logger is None:
        from aws_lambda_powertools import Logger
        import os
        _logger = Logger(
            service=os.getenv("AWS_LAMBDA_FUNCTION_NAME", "universal-sync-core"),
            level=os.getenv("LOG_LEVEL", "INFO"),
            child=True
        )
    return _logger

def _get_s3_client():
    global _s3_client
    if _s3_client is None:
        import boto3
        _s3_client = boto3.client('s3')
    return _s3_client

def _get_s3_bucket():
    global _S3_BUCKET
    if _S3_BUCKET is None:
        import os
        # ğŸ›¡ï¸ [Constraint] Bucket Name Consistency
        # Prioritize unified WORKFLOW_STATE_BUCKET, then legacy fallbacks
        _S3_BUCKET = (
            os.environ.get('WORKFLOW_STATE_BUCKET') or 
            os.environ.get('S3_BUCKET') or 
            os.environ.get('STATE_STORAGE_BUCKET') or
            ''
        )
        # ğŸ›¡ï¸ [Guard] Fail-fast if no bucket configured
        if not _S3_BUCKET:
            _get_logger().error(
                "[CRITICAL] No S3 bucket configured! "
                "Set WORKFLOW_STATE_BUCKET, S3_BUCKET, or STATE_STORAGE_BUCKET env var."
            )
    return _S3_BUCKET


# ============================================
# Type Definitions
# ============================================

class MergeStrategy(TypedDict, total=False):
    """í•„ë“œë³„ ë³‘í•© ì „ëµ"""
    list_strategy: Literal['append', 'replace', 'dedupe_append', 'set_union']
    conflict_resolution: Literal['latest', 'base', 'delta']
    deep_merge_fields: List[str]


class SyncContext(TypedDict, total=False):
    """ë™ê¸°í™” ì»¨í…ìŠ¤íŠ¸"""
    execution_id: str
    action: str
    merge_strategy: MergeStrategy
    idempotency_key: str


# ============================================
# Constants
# ============================================

# ì˜¤í”„ë¡œë”© ì œì™¸ ì œì–´ í•„ë“œ
CONTROL_FIELDS_NEVER_OFFLOAD = frozenset({
    'execution_id',
    'segment_to_run', 
    'segment_id',  # ğŸ›¡ï¸ [Fix] Routing safety
    'loop_counter',
    'next_action',
    'status',
    'idempotency_key',
    'state_s3_path',
    'pre_snapshot_s3_path',
    'post_snapshot_s3_path',
    'last_update_time',
    'payload_size_kb',
    'AUTO_RESUME_HITP',  # ì‹œë®¬ë ˆì´í„° HITP ìë™ ìŠ¹ì¸ í”Œë˜ê·¸ (USC ê²½ë¡œ ë³´ì¡´)
    'MOCK_MODE',         # ëª¨ì˜ ì‹¤í–‰ ëª¨ë“œ í”Œë˜ê·¸ (USC ê²½ë¡œ ë³´ì¡´)
})

# ë¦¬ìŠ¤íŠ¸ í•„ë“œ ê¸°ë³¸ ë³‘í•© ì „ëµ
LIST_FIELD_STRATEGIES: Dict[str, str] = {
    'state_history': 'dedupe_append',      # ì¤‘ë³µ ì œê±° í›„ ì¶”ê°€
    'new_history_logs': 'dedupe_append',
    'failed_branches': 'append',           # ê·¸ëƒ¥ ì¶”ê°€
    'distributed_outputs': 'append',
    'branches': 'replace',                 # êµì²´ (ìµœì‹  ë¸Œëœì¹˜ ì •ë³´)
    'chunk_results': 'replace',
    '_failed_segments': 'replace',         # ë§¤ aggregateë§ˆë‹¤ ìµœì‹  ê°’ìœ¼ë¡œ êµì²´ (ëˆ„ì  ë°©ì§€)
}

# [v3.22] Bag êµ¬ì¡° í‚¤ â€” flat-merge ì‹œ ë£¨íŠ¸ì— ì˜¬ë¦¬ì§€ ì•Šì„ ì»¤ë„ ì „ìš© í‚¤
# run_workflow() ê²°ê³¼ë¥¼ delta ë£¨íŠ¸ì— ì§ì ‘ í”Œë« ë¨¸ì§€í•  ë•Œ ì´ í‚¤ë“¤ì€ ì œì™¸í•¨
_BAG_STRUCTURAL_SKIP: frozenset = frozenset({
    # Routing / identity keys â€” kernel owns these, never merge from workflow output
    'workflow_config', 'partition_map', 'segment_manifest_s3_path',
    'execution_id', 'idempotency_key', 'ownerId', 'workflowId',
    'segment_to_run', 'loop_counter', 'segment_id', 'total_segments',
    'state_s3_path', 'payload_size_kb', 'last_update_time',
    'AUTO_RESUME_HITP', 'MOCK_MODE', 'manifest_id', 'branches_s3_path',
    'pre_snapshot_s3_path', 'post_snapshot_s3_path',
    # [v3.22 SFN-size fix] Volatile runtime-internal keys â€” flat-merging these to bag root
    # causes unbounded payload growth and SFN 256 KB limit violations:
    #   â€¢ step_history  â€” LangGraph per-node trace list; merge_logic uses 'append' strategy
    #                     so it grows O(segments Ã— nodes). Handled via new_history_logs instead.
    #   â€¢ execution_logs â€” similar ephemeral log list; not consumed by verifiers
    #   â€¢ _metadata      â€” per-LLM-call internal dict written by llm_chat_runner
    #   â€¢ _kernel_execution_summary â€” accumulating kernel stats dict
    'step_history',
    'execution_logs',
    '_metadata',
    '_kernel_execution_summary',
})

# í¬ê¸° ì„ê³„ê°’ (KB)
FIELD_OFFLOAD_THRESHOLD_KB = 30
FULL_STATE_OFFLOAD_THRESHOLD_KB = 100
MAX_PAYLOAD_SIZE_KB = 200
POINTER_BLOAT_WARNING_THRESHOLD_KB = 10


# ============================================
# Retry Strategy (Abstract + Concrete)
# ============================================

class RetryStrategy(ABC):
    """ì¬ì‹œë„ ì „ëµ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    @abstractmethod
    def execute(self, func: Callable, fallback: Any = None) -> Any:
        """ì¬ì‹œë„ë¥¼ ì ìš©í•˜ì—¬ í•¨ìˆ˜ ì‹¤í–‰"""
        pass


class ExponentialBackoffRetry(RetryStrategy):
    """Exponential Backoff ì¬ì‹œë„ ì „ëµ"""
    
    def __init__(self, max_retries: int = 3, base_delay: float = 0.5, max_delay: float = 8.0):
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.max_delay = max_delay
    
    def execute(self, func: Callable, fallback: Any = None) -> Any:
        logger = _get_logger()
        last_exception = None
        
        for attempt in range(self.max_retries):
            try:
                return func()
            except Exception as e:
                last_exception = e
                if attempt < self.max_retries - 1:
                    delay = min(self.base_delay * (2 ** attempt), self.max_delay)
                    logger.warning(f"Retry {attempt+1}/{self.max_retries} after {delay:.2f}s: {e}")
                    time.sleep(delay)
        
        logger.error(f"Failed after {self.max_retries} attempts: {last_exception}")
        return fallback


class NoRetry(RetryStrategy):
    """ì¬ì‹œë„ ì—†ëŠ” ì „ëµ (í…ŒìŠ¤íŠ¸ìš©)"""
    
    def execute(self, func: Callable, fallback: Any = None) -> Any:
        try:
            return func()
        except Exception:
            return fallback


# ============================================
# StateHydrator - S3 ë³µêµ¬ ì „ë‹´ í´ë˜ìŠ¤
# ============================================

class StateHydrator:
    """
    ìƒíƒœ ë³µêµ¬ ì „ë‹´ í´ë˜ìŠ¤ (Control Plane)
    
    v3.1 ê°œì„ ì‚¬í•­:
        - Retry Strategy ì£¼ì… ê°€ëŠ¥
        - Checksum ê²€ì¦ ì§€ì›
        - ìºì‹œ í†µí•©
    """
    
    def __init__(
        self, 
        retry_strategy: Optional[RetryStrategy] = None,
        validate_checksum: bool = True
    ):
        self.retry_strategy = retry_strategy or ExponentialBackoffRetry()
        self.validate_checksum = validate_checksum
        self._cache: Dict[str, Any] = {}
        self._cache_timestamps: Dict[str, float] = {}
        self._cache_ttl = 300  # 5ë¶„
        self._max_cache_size = 20
    
    def load_from_s3(
        self, 
        s3_path: str, 
        expected_checksum: Optional[str] = None,
        use_cache: bool = True
    ) -> Any:
        """
        ì¬ì‹œë„ + ì²´í¬ì„¬ ê²€ì¦ì´ í†µí•©ëœ S3 ë¡œë”©
        
        Args:
            s3_path: S3 ê²½ë¡œ (s3://bucket/key)
            expected_checksum: ì˜ˆìƒ MD5 í•´ì‹œ (ê²€ì¦ìš©)
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            ë¡œë“œëœ ë°ì´í„° ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
        """
        if not s3_path or not s3_path.startswith('s3://'):
            return None
        
        # ìºì‹œ í™•ì¸
        if use_cache:
            cached = self._get_from_cache(s3_path)
            if cached is not None:
                return cached
        
        # ì¬ì‹œë„ ì „ëµ ì ìš©
        result = self.retry_strategy.execute(
            func=lambda: self._load_and_validate(s3_path, expected_checksum),
            fallback=None
        )
        
        # ìºì‹œ ì €ì¥
        if result is not None and use_cache:
            self._put_to_cache(s3_path, result)
        
        return result
    
    def _load_and_validate(self, s3_path: str, expected_checksum: Optional[str]) -> Any:
        """ë‹¨ì¼ ì‹œë„: S3 ë¡œë“œ + ì²´í¬ì„¬ ê²€ì¦"""
        logger = _get_logger()
        s3_client = _get_s3_client()
        
        # Parse s3://bucket/key
        path_parts = s3_path.replace('s3://', '').split('/', 1)
        bucket = path_parts[0]
        key = path_parts[1] if len(path_parts) > 1 else ''
        
        response = s3_client.get_object(Bucket=bucket, Key=key)
        content = response['Body'].read()
        content_str = content.decode('utf-8')
        
        # â‘¢ Checksum ê²€ì¦ - ë°ì´í„°ê°€ ê¹¨ì¡Œë‹¤ë©´ ì—ëŸ¬ë¥¼ ë‚´ê³  ì¬ì‹œë„
        if self.validate_checksum and expected_checksum:
            actual_checksum = hashlib.md5(content).hexdigest()
            if actual_checksum != expected_checksum:
                raise ValueError(
                    f"Checksum mismatch! Expected {expected_checksum}, got {actual_checksum}. "
                    "Data corrupted - triggering retry."
                )
        
        return json.loads(content_str)
    
    def _get_from_cache(self, s3_path: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ (TTL ì²´í¬)"""
        if s3_path not in self._cache:
            return None
        
        cache_time = self._cache_timestamps.get(s3_path, 0)
        if time.time() - cache_time >= self._cache_ttl:
            # TTL ë§Œë£Œ
            del self._cache[s3_path]
            del self._cache_timestamps[s3_path]
            return None
        
        _get_logger().debug(f"Cache hit: {s3_path}")
        return self._cache[s3_path]
    
    def _put_to_cache(self, s3_path: str, data: Any) -> None:
        """ìºì‹œì— ë°ì´í„° ì €ì¥ (LRU ì •ì±…)"""
        if len(self._cache) >= self._max_cache_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest = min(self._cache_timestamps, key=self._cache_timestamps.get)
            del self._cache[oldest]
            del self._cache_timestamps[oldest]
        
        self._cache[s3_path] = data
        self._cache_timestamps[s3_path] = time.time()
    
    def clear_cache(self) -> None:
        """ìºì‹œ ì´ˆê¸°í™”"""
        self._cache.clear()
        self._cache_timestamps.clear()


# ëª¨ë“ˆ ë ˆë²¨ ì‹±ê¸€í„´
_default_hydrator: Optional[StateHydrator] = None

def get_default_hydrator() -> StateHydrator:
    """ê¸°ë³¸ StateHydrator ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _default_hydrator
    if _default_hydrator is None:
        _default_hydrator = StateHydrator()
    return _default_hydrator


# ============================================
# Universal Sync Core - í•µì‹¬ í•¨ìˆ˜
# ============================================

def calculate_checksum(data: Any) -> str:
    """ë°ì´í„°ì˜ MD5 ì²´í¬ì„¬ ê³„ì‚°"""
    json_str = json.dumps(data, separators=(',', ':'), sort_keys=True, default=str)
    return hashlib.md5(json_str.encode('utf-8')).hexdigest()


def calculate_payload_size(data: Dict[str, Any]) -> int:
    """í˜ì´ë¡œë“œ í¬ê¸° ê³„ì‚° (KB)"""
    try:
        json_str = json.dumps(data, separators=(',', ':'))
        return len(json_str.encode('utf-8')) // 1024
    except Exception:
        return 0


def flatten_result(result: Any, context: Optional[SyncContext] = None) -> Dict[str, Any]:
    """
    ğŸ“¥ ì…ë ¥ ì •ê·œí™” (Normalize) - v3.2 ìŠ¤ë§ˆíŠ¸ ì¶”ì¶œ
    
    ì•¡ì…˜ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ í•„ë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë¦¬ìŠ¤íŠ¸ë“  ë‹¨ì¼ ê°ì²´ë“  ë™ì¼í•œ Delta í˜•íƒœë¡œ í‰íƒ„í™”í•©ë‹ˆë‹¤.
    
    ì•¡ì…˜ë³„ ì¶”ì¶œ ê·œì¹™:
        - sync: execution_resultì—ì„œ ìƒíƒœ ì¶”ì¶œ
        - aggregate_branches: ë³‘ë ¬ ê²°ê³¼ ë°°ì—´ì—ì„œ ë¡œê·¸/ìƒíƒœ ì§‘ê³„
        - aggregate_distributed: Map ê²°ê³¼ì—ì„œ ì •ë ¬ í›„ ë§ˆì§€ë§‰ ìƒíƒœ ì„ íƒ
        - merge_callback: callback_resultì—ì„œ ì‚¬ìš©ì ì‘ë‹µ ì¶”ì¶œ
        - merge_async: async_resultì—ì„œ LLM ì‘ë‹µ ì¶”ì¶œ
        - create_snapshot: í¬ì¸í„° ëª¨ë“œ ê²°ì •
    
    ğŸ›¡ï¸ [v3.4] NEVER returns None - always returns dict
    """
    # ğŸ›¡ï¸ [v3.4 Deep Guard] None ë°©ì§€
    if result is None:
        _get_logger().debug("[Deep Guard] flatten_result received None, returning empty dict")
        return {}
    
    # ğŸ›¡ï¸ contextë„ Noneì¼ ìˆ˜ ìˆìŒ
    if context is None:
        context = {'action': 'sync'}
    
    action = context.get('action', 'sync') if context else 'sync'
    
    # ============================================
    # Distributed Map ResultWriter ì²˜ë¦¬ (Manifest Pointer)
    # ============================================
    if isinstance(result, dict) and 'ResultWriterDetails' in result:
        rw_details = result['ResultWriterDetails']
        bucket = rw_details.get('Bucket')
        key = rw_details.get('Key')
        
        if bucket and key:
            try:
                # 1. Load Manifest Summary (Lightweight)
                s3 = _get_s3_client()
                obj = s3.get_object(Bucket=bucket, Key=key)
                manifest_data = json.loads(obj['Body'].read().decode('utf-8'))
                
                # 2. Extract Stats
                # Manifest structure varies, but typically contains stats or pointers
                # Assuming standard SFN Distributed Map output or custom aggregator format
                succeeded_count = 0
                failed_count = 0
                
                # Standard SFN Manifest typically separates Success/Failure file shards
                # We won't iterate all shards here (too heavy).
                # Instead, we rely on the manifest path itself as the "result".
                
                # If the manifest contains direct stats (some versions do):
                # otherwise we might need to assume success or check execution output?
                # Actually, for huge maps, we just store the pointer.
                
                return {
                    'segment_manifest_s3_path': f"s3://{bucket}/{key}",
                    'distributed_chunk_summary': {
                         'status': 'MANIFEST_ONLY',
                         'manifest_bucket': bucket,
                         'manifest_key': key
                    },
                    '_aggregation_complete': True
                }
            except Exception as e:
                _get_logger().error(f"Failed to process ResultWriter manifest: {e}")
                return {
                    'error': f"Failed to load manifest: {str(e)}",
                    '_aggregation_complete': False
                }

    # ============================================
    # Distributed Map ê²°ê³¼ (ë¦¬ìŠ¤íŠ¸ ì…ë ¥ - ì¸ë¼ì¸ ëª¨ë“œ)
    # ============================================
    if isinstance(result, list):
        # P1: execution_order ê¸°ì¤€ ì •ë ¬ â†’ ë…¼ë¦¬ì  ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ë³´ì¥
        sorted_results = sorted(
            [r for r in result if isinstance(r, dict)],
            key=lambda x: (str(x.get('execution_order', x.get('chunk_id', ''))), str(x.get('chunk_id', '')))
        )
        
        # ì„±ê³µ/ì‹¤íŒ¨ ë¶„ë¦¬
        successful = [r for r in sorted_results if r.get('status') in ('COMPLETE', 'SUCCESS')]
        failed = [r for r in sorted_results if r.get('status') not in ('COMPLETE', 'SUCCESS', None)]
        
        # ë§ˆì§€ë§‰ ì„±ê³µ ê²°ê³¼ì—ì„œ ìƒíƒœ ì¶”ì¶œ
        last_s3_path = None
        if successful:
            last_result = successful[-1]
            last_s3_path = last_result.get('output_s3_path') or last_result.get('final_state_s3_path')
        
        return {
            'state_s3_path': last_s3_path,
            'distributed_chunk_summary': {
                'total': len(result),
                'succeeded': len(successful),
                'failed': len(failed),
                'chunk_results': sorted_results[:10]  # 256KB ë°©ì§€
            },
            '_failed_segments': failed,  # ë‚´ë¶€ ì²˜ë¦¬ìš©
            '_aggregation_complete': True,
            # ğŸŒ¿ [Pointer Strategy] Manifest extraction
            'segment_manifest_s3_path': successful[-1].get('segment_manifest_s3_path') if successful else None
        }
    
    # ============================================
    # ë‹¨ì¼ ê°ì²´ (ë”•ì…”ë„ˆë¦¬ ì…ë ¥)
    # ============================================
    if isinstance(result, dict):
        delta = {}
        
        # ë˜í¼ íŒ¨í„´ ì œê±° ë° ì•¡ì…˜ë³„ ì¶”ì¶œ
        if action == 'merge_callback':
            payload = result.get('Payload', result.get('callback_result', result))
            if payload.get('user_response'):
                delta['last_hitp_response'] = payload['user_response']
            if payload.get('new_state_s3_path'):
                delta['state_s3_path'] = payload['new_state_s3_path']
            if payload.get('new_history_logs'):
                delta['new_history_logs'] = payload['new_history_logs']
            delta['_increment_segment'] = payload.get('segment_to_run') is None
                
        elif action == 'merge_async':
            payload = result.get('async_result', result)
            if payload.get('final_state_s3_path'):
                delta['state_s3_path'] = payload['final_state_s3_path']
            if payload.get('new_history_logs'):
                delta['new_history_logs'] = payload['new_history_logs']
            delta['_increment_segment'] = payload.get('segment_to_run') is None
            
        elif action == 'sync':
            payload = result.get('execution_result', result)
            if payload.get('final_state_s3_path'):
                delta['state_s3_path'] = payload['final_state_s3_path']
            if payload.get('next_segment_to_run') is not None:
                delta['segment_to_run'] = payload['next_segment_to_run']
            if payload.get('new_history_logs'):
                delta['new_history_logs'] = payload['new_history_logs']
            if payload.get('branches'):
                delta['pending_branches'] = payload['branches']
            # ğŸŒ¿ [Pointer Strategy] branches_s3_pathë„ State Bagì— ì €ì¥
            if payload.get('branches_s3_path'):
                delta['branches_s3_path'] = payload['branches_s3_path']
            # ğŸŒ¿ [Pointer Strategy] Manifest extraction
            if payload.get('segment_manifest_s3_path'):
                 delta['segment_manifest_s3_path'] = payload['segment_manifest_s3_path']
            if payload.get('inner_partition_map'):
                delta['partition_map'] = payload['inner_partition_map']
                delta['segment_to_run'] = 0  # restart from inner partition segment 0
            delta['_status'] = payload.get('status', 'CONTINUE')
            
            # ï¿½ï¸ [v3.16 Fix] CONTINUE ìƒíƒœì¼ ë•Œ next_segment_to_run í•„ìˆ˜ ê²€ì¦
            if delta['_status'] == 'CONTINUE' and payload.get('next_segment_to_run') is None:
                _get_logger().error(
                    f"[flatten_result] CRITICAL: status=CONTINUE but next_segment_to_run is None! "
                    f"This will cause infinite loop. Payload keys: {list(payload.keys())[:20]}"
                )
                # ê°•ì œë¡œ segment_to_run ìœ ì§€ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
                if 'segment_to_run' not in delta or delta['segment_to_run'] is None:
                    _get_logger().error(f"[flatten_result] EMERGENCY: Forcing status to FAILED to prevent infinite loop")
                    delta['_status'] = 'FAILED'
                    delta['_error'] = 'CONTINUE status without next_segment_to_run'
            
            # ï¿½ğŸ” [v3.15 Debug] Log status extraction for troubleshooting
            _get_logger().info(f"[flatten_result sync] Extracted status={delta['_status']} from payload, next_segment={payload.get('next_segment_to_run')}")
            
            # [v3.22] Flat-merge final_state directly into delta root
            # Replaces v3.20 burial (delta['current_state'] = final_state) which hid
            # all user keys one level deep and broke cross-segment state propagation.
            # Kernel structural keys (workflowId, execution_id, â€¦) are skipped via
            # _BAG_STRUCTURAL_SKIP to prevent user data from overwriting kernel config.
            final_state = payload.get('final_state')
            # [v3.23 S3 Hydration] When segment_runner has offloaded final_state to S3
            # (e.g., has_next_segment=True + size>20KB), USC receives a pointer dict:
            #   {"__s3_offloaded": True, "__s3_path": "s3://bucket/key", ...}
            # Flat-merging the pointer alone would write __s3_offloaded/__s3_path to
            # the bag root and lose all user keys (TEST_RESULT etc).
            # Solution: detect the pointer and hydrate the full state from S3 before merging.
            if isinstance(final_state, dict) and final_state.get('__s3_offloaded'):
                s3_path = (
                    final_state.get('__s3_path')
                    or final_state.get('s3_path')
                    or payload.get('final_state_s3_path')
                    or payload.get('state_s3_path')
                )
                if s3_path:
                    _get_logger().info(
                        f"[v3.23] final_state is S3 pointer, hydrating from {s3_path}"
                    )
                    try:
                        hydrated = get_default_hydrator().load_from_s3(s3_path)
                        if isinstance(hydrated, dict):
                            final_state = hydrated
                            _get_logger().info(
                                f"[v3.23] Hydration OK â€” {len(final_state)} keys from S3"
                            )
                        else:
                            _get_logger().warning(
                                f"[v3.23] Hydrated value is not dict: {type(hydrated)}. "
                                "Falling back to pointer merge."
                            )
                    except Exception as _hydrate_err:
                        _get_logger().error(
                            f"[v3.23] S3 hydration failed ({s3_path}): {_hydrate_err}. "
                            "Falling back to pointer merge."
                        )
            if isinstance(final_state, dict):
                # Unwrap one level if the result is still wrapped in current_state
                # (handles rare case where run_workflow itself returned a nested bag)
                inner = final_state.get('current_state')
                source = inner if isinstance(inner, dict) else final_state
                merged_keys = []
                for k, v in source.items():
                    if k not in _BAG_STRUCTURAL_SKIP:
                        delta[k] = v
                        merged_keys.append(k)
                _get_logger().info(
                    f"[v3.22] flat-merged final_state keys to delta root "
                    f"({len(merged_keys)} keys): {merged_keys[:10]}"
                )

        elif action == 'sync_branch':
            # [C-1/C-2 Fix] sync_branchë¥¼ syncì™€ ë™ì¼í•œ 1ê¸‰ ì‹œë¯¼ìœ¼ë¡œ ê²©ìƒ
            # ë¸Œëœì¹˜ ì „ìš© next_segment_to_run â†’ segment_to_run ëª…ì‹œì  ë§¤í•‘
            payload = result.get('execution_result', result)
            if payload.get('final_state_s3_path'):
                delta['state_s3_path'] = payload['final_state_s3_path']
            if payload.get('next_segment_to_run') is not None:
                delta['segment_to_run'] = payload['next_segment_to_run']
            if payload.get('new_history_logs'):
                delta['new_history_logs'] = payload['new_history_logs']
            delta['_status'] = payload.get('status', 'CONTINUE')
            # [v3.22] Flat-merge (sync_branch) â€” mirrors sync path above
            # [v3.23] S3 pointer hydration identical to action='sync'
            final_state = payload.get('final_state')
            if isinstance(final_state, dict) and final_state.get('__s3_offloaded'):
                s3_path = (
                    final_state.get('__s3_path')
                    or final_state.get('s3_path')
                    or payload.get('final_state_s3_path')
                    or payload.get('state_s3_path')
                )
                if s3_path:
                    try:
                        hydrated = get_default_hydrator().load_from_s3(s3_path)
                        if isinstance(hydrated, dict):
                            final_state = hydrated
                            _get_logger().info(
                                f"[v3.23 sync_branch] Hydrated from S3: {len(final_state)} keys"
                            )
                    except Exception as _hydrate_err:
                        _get_logger().error(
                            f"[v3.23 sync_branch] S3 hydration failed: {_hydrate_err}"
                        )
            if isinstance(final_state, dict):
                inner = final_state.get('current_state')
                source = inner if isinstance(inner, dict) else final_state
                for k, v in source.items():
                    if k not in _BAG_STRUCTURAL_SKIP:
                        delta[k] = v
            _get_logger().info(
                f"[flatten_result sync_branch] status={delta['_status']}, "
                f"next_segment={payload.get('next_segment_to_run')}, "
                f"state_s3_path={'set' if delta.get('state_s3_path') else 'unset'}"
            )

        elif action == 'aggregate_branches':
            # ë³‘ë ¬ ë¸Œëœì¹˜ ê²°ê³¼ (í¬ì¸í„° ë°°ì—´)
            pointers = result.get('parallel_results', result.get('branch_pointers', []))
            if isinstance(pointers, list):
                list_delta = flatten_result(pointers, context)  # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ë¡œ ìœ„ì„
                # [Soft-fail] _soft_fail_branchesë¥¼ ë¦¬ìŠ¤íŠ¸ ê²°ê³¼ì— ë³‘í•©
                # state_data_manager.aggregate_branches()ê°€ ì±„ì›Œì„œ ë„˜ê¸´ ê°’ ë³´ì¡´
                soft_fail = result.get('_soft_fail_branches')
                if soft_fail:
                    existing = list_delta.get('_failed_segments', [])
                    # _soft_fail_branchesê°€ _failed_segmentsì™€ ì¤‘ë³µë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ
                    # branch_id ê¸°ì¤€ deduplicate (soft_fail ì •ë³´ê°€ ë” í’ë¶€í•¨)
                    existing_ids = {s.get('branch_id') for s in existing if isinstance(s, dict)}
                    extra = [s for s in soft_fail if s.get('branch_id') not in existing_ids]
                    list_delta['_failed_segments'] = existing + extra
                    _get_logger().warning(
                        f"[flatten_result aggregate_branches] "
                        f"{len(soft_fail)} soft-fail branch(es) added to _failed_segments: "
                        f"{[s['branch_id'] for s in soft_fail]}"
                    )
                # new_history_logsë„ ì „ë‹¬ (S3ì—ì„œ ë¡œë“œí•œ partial ë¡œê·¸)
                if result.get('new_history_logs'):
                    list_delta.setdefault('new_history_logs', [])
                    list_delta['new_history_logs'] = (
                        list_delta['new_history_logs'] + result['new_history_logs']
                    )
                return list_delta
            delta = result
            
        elif action == 'create_snapshot':
            # ìŠ¤ëƒ…ìƒ·: state_s3_path ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸
            delta['_is_pointer_mode'] = bool(result.get('state_s3_path'))
        
        elif action == 'init':
            # íƒ„ìƒ (Day-Zero Sync): íŒŒí‹°ì…”ë‹ ê²°ê³¼ + ì´ˆê¸° ìƒíƒœë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬
            # required metadataëŠ” merge_logicì—ì„œ ê°•ì œ ì£¼ì…ë¨
            
            # ğŸ”‘ [Critical] Extract bag contents and merge into delta
            # InitializeStateData passes {'bag': payload}, we need to extract payload
            # ğŸ›¡ï¸ [Guard] bagì´ Noneì´ê±°ë‚˜ ì—†ëŠ” ê²½ìš° result ìì²´ë¥¼ ì‚¬ìš©
            bag_contents = result.get('bag') if isinstance(result, dict) else None
            if bag_contents is None:
                # bag í‚¤ê°€ ì—†ê±°ë‚˜ ê°’ì´ Noneì¸ ê²½ìš° result ìì²´ ì‚¬ìš©
                bag_contents = result if isinstance(result, dict) else {}
            
            if isinstance(bag_contents, dict):
                delta.update(bag_contents)
            else:
                _get_logger().warning(f"[Init] bag_contents is not dict: {type(bag_contents)}")
            
            delta['_is_init'] = True
            delta['_status'] = 'STARTED'
            # ğŸŒ¿ [Pointer Strategy] Manifest extraction for Init
            if isinstance(result, dict) and result.get('segment_manifest_s3_path'):
                 delta['segment_manifest_s3_path'] = result['segment_manifest_s3_path']
            
        else:
            # ê¸°ë³¸: ë˜í¼ ì œê±°
            if 'callback_result' in result and len(result) <= 2:
                delta = result['callback_result']
            elif 'async_result' in result and len(result) <= 2:
                delta = result['async_result']
            elif 'execution_result' in result and len(result) <= 2:
                delta = result['execution_result']
            else:
                delta = result
            
            # ğŸ›¡ï¸ [v3.21 Fix] else ë¸Œëœì¹˜ì—ì„œ _statusê°€ ì—†ìœ¼ë©´ status ê°’ì„ ìŠ¹ê²©
            # action='error' ë“± ë¹„í‘œì¤€ ì•¡ì…˜ì´ ì—¬ê¸° ë–¨ì–´ì§ˆ ë•Œ _status ë¯¸ì„¤ì • ì‹œ
            # _compute_next_actionì´ CONTINUEë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©í•´ ë¬´í•œë£¨í”„ ìœ ë°œ
            # í•´ê²°: deltaì— _statusê°€ ì—†ìœ¼ë©´ delta.status â†’ 'FAILED' ìˆœìœ¼ë¡œ í´ë°±
            if isinstance(delta, dict) and '_status' not in delta:
                fallback_status = delta.get('status')
                if isinstance(fallback_status, str) and fallback_status.upper() in (
                    'FAILED', 'COMPLETE', 'SUCCESS', 'SUCCEEDED', 'PAUSED_FOR_HITP', 'CONTINUE'
                ):
                    delta['_status'] = fallback_status.upper()
                elif action == 'error':
                    # action='error'ëŠ” í•­ìƒ FAILEDì—¬ì•¼ í•¨ â€” ë£¨í”„ ë°©ì§€
                    delta['_status'] = 'FAILED'
                    _get_logger().warning(
                        f"[flatten_result] action='error' had no _status. "
                        f"Forcing _status=FAILED to prevent infinite loop. "
                        f"delta keys: {list(delta.keys())[:10]}"
                    )
        
        return delta
    
    # ê¸°íƒ€ íƒ€ì… (ë¬¸ìì—´, ìˆ«ì ë“±)
    return {'raw_result': result}


def _shallow_copy_with_cow(base_state: Dict[str, Any], fields_to_modify: set) -> Dict[str, Any]:
    """
    â‘  Copy-on-Write ë°©ì‹ì˜ ì–•ì€ ë³µì‚¬
    
    ì „ì²´ deepcopy ëŒ€ì‹  ë³€ê²½ë  í•„ë“œë§Œ ë³µì‚¬í•©ë‹ˆë‹¤.
    14ë§Œ ì¤„ ì»¤ë„ ìƒíƒœì˜ CPU/GC ë¶€í•˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    # ê¸°ë³¸ì€ ì–•ì€ ë³µì‚¬ (ì°¸ì¡° ìœ ì§€)
    result = base_state.copy()
    
    # ë³€ê²½ë  í•„ë“œë§Œ ê¹Šì€ ë³µì‚¬
    for field in fields_to_modify:
        if field in result:
            value = result[field]
            if isinstance(value, dict):
                result[field] = value.copy()
            elif isinstance(value, list):
                result[field] = value.copy()
    
    return result


def _get_log_key(log: Dict) -> str:
    """íˆìŠ¤í† ë¦¬ ë¡œê·¸ì˜ ê³ ìœ  í‚¤ ìƒì„± (ì¤‘ë³µ ì œê±°ìš©)"""
    if not isinstance(log, dict):
        return str(hash(str(log)))
    
    node_id = log.get('node_id', log.get('id', ''))
    timestamp = log.get('timestamp', log.get('created_at', ''))
    
    if node_id and timestamp:
        return f"{node_id}:{timestamp}"
    elif node_id:
        return f"node:{node_id}"
    elif timestamp:
        return f"ts:{timestamp}"
    else:
        return hashlib.md5(json.dumps(log, sort_keys=True, default=str).encode()).hexdigest()


def _merge_list_field(
    base_list: List,
    delta_list: List,
    strategy: str
) -> List:
    """
    â‘¡ ë¦¬ìŠ¤íŠ¸ í•„ë“œ ë³‘í•© (ì›ìì„± ë³´ì¥)
    
    ì „ëµ:
        - 'append': ë‹¨ìˆœ ì¶”ê°€
        - 'replace': êµì²´
        - 'dedupe_append': ì¤‘ë³µ ì œê±° í›„ ì¶”ê°€
        - 'set_union': ì§‘í•© í•©ì§‘í•©
    """
    if strategy == 'replace':
        return delta_list.copy()
    
    if strategy == 'append':
        return base_list + delta_list
    
    if strategy == 'dedupe_append':
        # ë¡œê·¸ ì¤‘ë³µ ì œê±° (node_id + timestamp ê¸°ë°˜)
        seen_keys = {_get_log_key(item) for item in base_list}
        unique_delta = [
            item for item in delta_list 
            if _get_log_key(item) not in seen_keys
        ]
        return base_list + unique_delta
    
    if strategy == 'set_union':
        # ë¬¸ìì—´/ìˆ«ì ì§‘í•©
        result_set = set(base_list) | set(delta_list)
        return list(result_set)
    
    # ê¸°ë³¸: append
    return base_list + delta_list


def _deep_merge_dicts(base: Dict[str, Any], delta: Dict[str, Any]) -> Dict[str, Any]:
    """
    ğŸ”€ ì¬ê·€ì  ë”•ì…”ë„ˆë¦¬ ë”¥ ë¨¸ì§€

    [M-1 Fix] dict.update()ëŠ” 2ë‹¨ê³„ ì•„ë˜ í‚¤ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    current_stateì²˜ëŸ¼ ì¤‘ì²© êµ¬ì¡°ê°€ ê¹Šì€ í•„ë“œëŠ” ì„œë¸Œí‚¤ë¥¼ ë³´ì¡´í•´ì•¼ í•©ë‹ˆë‹¤:
        - ë‘ ê°’ì´ ëª¨ë‘ dictì´ë©´ â†’ ì¬ê·€ ë”¥ ë¨¸ì§€ (ì„œë¸Œí‚¤ ë³´ì¡´)
        - ê·¸ ì™¸ â†’ delta ê°’ì´ base ê°’ì„ ëŒ€ì²´
    """
    merged = base.copy()
    for k, v in delta.items():
        if isinstance(v, dict) and isinstance(merged.get(k), dict):
            merged[k] = _deep_merge_dicts(merged[k], v)
        else:
            merged[k] = v
    return merged


# í•„ìˆ˜ ë©”íƒ€ë°ì´í„° ê¸°ë³¸ê°’ (action='init' ì „ìš©)
INIT_REQUIRED_METADATA = {
    'segment_to_run': 0,
    'loop_counter': 0,
    'state_history': [],
    'max_loop_iterations': 100,
    'max_branch_iterations': 100,
    'distributed_mode': False,
    'distributed_strategy': 'SAFE',
    'max_concurrency': 1,
}


def merge_logic(
    base_state: Dict[str, Any],
    delta: Dict[str, Any],
    context: Optional[SyncContext] = None
) -> Dict[str, Any]:
    """
    ğŸ”€ ìƒíƒœ ë³‘í•© (Shallow Merge + Copy-on-Write)
    
    ê·œì¹™:
        1. ì œì–´ í•„ë“œëŠ” delta ìš°ì„ 
        2. íˆìŠ¤í† ë¦¬ëŠ” dedupe_append (ì¤‘ë³µ ì œê±° í›„ ì¶”ê°€)
        3. ë”•ì…”ë„ˆë¦¬ í•„ë“œëŠ” shallow merge
        4. ë¦¬ìŠ¤íŠ¸ í•„ë“œëŠ” context.merge_strategyì— ë”°ë¦„
    
    ì„±ëŠ¥:
        - deepcopy ëŒ€ì‹  Copy-on-Write ì‚¬ìš©
        - ë³€ê²½ë˜ëŠ” ì„œë¸ŒíŠ¸ë¦¬ë§Œ ë³µì‚¬
    
    Special:
        - action='init': ë¹ˆ base_stateì— í•„ìˆ˜ ë©”íƒ€ë°ì´í„° ê°•ì œ ì£¼ì…
    
    ğŸ›¡ï¸ [v3.4] NEVER returns None - always returns dict
    """
    logger = _get_logger()
    
    # ğŸ›¡ï¸ [v3.4 Deep Guard] None ë°©ì§€ - Immutable Empty Dict
    if base_state is None:
        logger.warning("ğŸš¨ [Deep Guard] merge_logic received None base_state!")
        base_state = {}
    
    if delta is None:
        logger.debug("[Deep Guard] merge_logic received None delta, returning base_state")
        return base_state if base_state else {}
    
    if context is None:
        context = {'action': 'sync'}
    
    action = context.get('action', 'sync') if context else 'sync'
    
    # íƒ„ìƒ (init): í•„ìˆ˜ ë©”íƒ€ë°ì´í„° ê°•ì œ ì£¼ì…
    if action == 'init':
        # ê¸°ë³¸ê°’ ë¨¼ì € ì ìš©, ê·¸ ìœ„ì— delta ë©ì–´ì“°ê¸°
        base_with_defaults = INIT_REQUIRED_METADATA.copy()
        base_with_defaults.update(base_state)
        base_state = base_with_defaults
        logger.info(f"[Init] Injected required metadata: {list(INIT_REQUIRED_METADATA.keys())}")
    
    if not delta:
        return base_state
    
    # ë³€ê²½ë  í•„ë“œ ì‹ë³„ (CoWìš©)
    fields_to_modify = set(delta.keys())
    if 'state_history' in delta or 'new_history_logs' in delta:
        fields_to_modify.add('state_history')
    
    # â‘  Copy-on-Write ë°©ì‹ ë³µì‚¬
    updated_state = _shallow_copy_with_cow(base_state, fields_to_modify)
    
    # merge_strategy ì¶”ì¶œ
    merge_strategy = (context.get('merge_strategy', {}) if context else {})
    
    for key, value in delta.items():
        # ì œì–´ í•„ë“œ: ë¬´ì¡°ê±´ delta ìš°ì„ 
        if key in CONTROL_FIELDS_NEVER_OFFLOAD:
            updated_state[key] = value
            continue
        
        # new_history_logs â†’ state_historyë¡œ ë³‘í•©
        if key == 'new_history_logs':
            existing = updated_state.get('state_history', [])
            strategy = LIST_FIELD_STRATEGIES.get('state_history', 'dedupe_append')
            updated_state['state_history'] = _merge_list_field(existing, value, strategy)
            continue
        
        # ê¸°ì¡´ ê°’ í™•ì¸
        base_value = updated_state.get(key)
        
        # ë¦¬ìŠ¤íŠ¸ í•„ë“œ
        if isinstance(value, list):
            if isinstance(base_value, list):
                strategy = LIST_FIELD_STRATEGIES.get(key, 'append')
                updated_state[key] = _merge_list_field(base_value, value, strategy)
            else:
                updated_state[key] = value.copy()
        
        # ë”•ì…”ë„ˆë¦¬ í•„ë“œ: current_stateëŠ” ë”¥ ë¨¸ì§€, ë‚˜ë¨¸ì§€ëŠ” Shallow Merge
        elif isinstance(value, dict):
            if isinstance(base_value, dict):
                if key == 'current_state':
                    # [M-1 Fix] current_state ë”¥ ë¨¸ì§€ë¡œ ì„œë¸Œí‚¤ ë³´ì¡´
                    updated_state[key] = _deep_merge_dicts(base_value, value)
                else:
                    # Shallow merge: delta í‚¤ê°€ base í‚¤ë¥¼ ë®ì–´ì”€
                    merged = base_value.copy()
                    merged.update(value)
                    updated_state[key] = merged
            else:
                updated_state[key] = value.copy() if isinstance(value, dict) else value
        
        # ê¸°íƒ€ íƒ€ì… (ë¬¸ìì—´, ìˆ«ì ë“±)
        else:
            updated_state[key] = value
    
    return updated_state


def prevent_pointer_bloat(
    state: Dict[str, Any],
    idempotency_key: str
) -> Dict[str, Any]:
    """
    ğŸ”’ í¬ì¸í„° ë¹„ëŒ€í™” ë°©ì§€
    
    scheduling_metadata, failed_segments ë“± í¬ì¸í„°ê°€ ì»¤ì§ˆ ìˆ˜ ìˆëŠ”
    í•„ë“œë¥¼ ê°„ì†Œí™”í•©ë‹ˆë‹¤.
    """
    logger = _get_logger()
    
    # failed_segments ì˜¤í”„ë¡œë”©
    if 'failed_segments' in state:
        failed = state['failed_segments']
        if isinstance(failed, list) and len(failed) > 5:
            from .state_data_manager import store_to_s3, generate_s3_key
            try:
                s3_key = generate_s3_key(idempotency_key, 'failed_segments')
                s3_path = store_to_s3(failed, s3_key)
                state['failed_segments_s3_path'] = s3_path
                state['failed_segments'] = failed[:5]  # ìƒ˜í”Œë§Œ
                logger.info(f"Offloaded {len(failed)} failed_segments to S3")
            except Exception as e:
                logger.warning(f"Failed to offload failed_segments: {e}")
    
    # current_state ë‚´ scheduling_metadata ê°„ì†Œí™”
    if isinstance(state.get('current_state'), dict):
        current = state['current_state']
        if isinstance(current.get('scheduling_metadata'), dict):
            metadata = current['scheduling_metadata']
            batch_details = metadata.get('batch_details', [])
            if len(batch_details) > 5:
                current['scheduling_summary'] = {
                    'total_batches': len(batch_details),
                    'priority': metadata.get('priority', 1),
                    'total_items': sum(b.get('size', 0) for b in batch_details if isinstance(b, dict))
                }
                del current['scheduling_metadata']
                logger.info("Simplified scheduling_metadata to scheduling_summary")
    
    return state


def emergency_offload_large_arrays(
    state: Dict[str, Any],
    idempotency_key: str
) -> Dict[str, Any]:
    """
    ğŸš¨ ì‘ê¸‰ ëŒ€ìš©ëŸ‰ ë°°ì—´ ì˜¤í”„ë¡œë”©
    
    í˜ì´ë¡œë“œê°€ 200KBì˜ 75%ë¥¼ ì´ˆê³¼í•  ë•Œ í˜¸ì¶œë©ë‹ˆë‹¤.
    """
    logger = _get_logger()
    
    from .state_data_manager import store_to_s3, generate_s3_key
    
    # distributed_outputs ì˜¤í”„ë¡œë”©
    if 'distributed_outputs' in state:
        outputs = state['distributed_outputs']
        if isinstance(outputs, list) and len(outputs) > 10:
            try:
                s3_key = generate_s3_key(idempotency_key, 'distributed_outputs')
                s3_path = store_to_s3(outputs, s3_key)
                state['distributed_outputs_s3_path'] = s3_path
                state['distributed_outputs'] = outputs[:10]  # ìƒ˜í”Œ 10ê°œë§Œ
                logger.warning(f"Emergency offload: distributed_outputs ({len(outputs)} items)")
            except Exception as e:
                logger.error(f"Emergency offload failed: {e}")
    
    return state


def optimize_and_offload(
    state: Dict[str, Any],
    context: Optional[SyncContext] = None
) -> Dict[str, Any]:
    """
    ğŸš€ í†µí•© ìµœì í™” íŒŒì´í”„ë¼ì¸ - P0~P2 ìë™ í•´ê²°
    
    ì²˜ë¦¬ ìˆœì„œ:
        1. íˆìŠ¤í† ë¦¬ ì•„ì¹´ì´ë¹™ (>50 entries)
        2. ê°œë³„ í•„ë“œ ì˜¤í”„ë¡œë”© (>30KB)
        3. ì „ì²´ ìƒíƒœ ì˜¤í”„ë¡œë”© (>100KB)
        4. í¬ì¸í„° ë¹„ëŒ€í™” ë°©ì§€
        5. ìµœì¢… í¬ê¸° ì²´í¬ (>200KB ê²½ê³ )
    
    ğŸ›¡ï¸ [v3.4] NEVER returns None - always returns dict
    """
    logger = _get_logger()
    
    # ğŸ›¡ï¸ [v3.4 Deep Guard] None ë°©ì§€
    if state is None:
        logger.warning("ğŸš¨ [Deep Guard] optimize_and_offload received None state!")
        state = {}
    
    if context is None:
        context = {'action': 'sync'}
    
    # state_data_managerì˜ ê¸°ì¡´ í•¨ìˆ˜ë“¤ ì¬ì‚¬ìš©
    from .state_data_manager import (
        optimize_state_history,
        optimize_current_state,
        calculate_payload_size as calc_size
    )
    
    idempotency_key = (
        context.get('idempotency_key') if context 
        else state.get('idempotency_key', 'unknown')
    )
    
    # 1. íˆìŠ¤í† ë¦¬ ìµœì í™”
    if state.get('state_history'):
        optimized_history, _ = optimize_state_history(
            state['state_history'],
            idempotency_key=idempotency_key,
            max_entries=50
        )
        state['state_history'] = optimized_history
    
    # 2. current_state ìµœì í™” (ê°œë³„ í•„ë“œ + ì „ì²´ ìƒíƒœ)
    if state.get('current_state'):
        optimized_current, _ = optimize_current_state(
            state['current_state'],
            idempotency_key
        )
        state['current_state'] = optimized_current
    else:
        # [v3.22 SFN-size fix] Fix 1 flat-merge mode: all workflow output keys are at bag
        # root (no current_state sub-dict).  optimize_current_state would be a no-op above,
        # so apply the same field-level S3 offloading at root level â€” but only for
        # non-critical user fields (skip CONTROL_FIELDS_NEVER_OFFLOAD and _BAG_STRUCTURAL_SKIP).
        offload_candidates = {
            k: v for k, v in state.items()
            if k not in CONTROL_FIELDS_NEVER_OFFLOAD and k not in _BAG_STRUCTURAL_SKIP
        }
        if offload_candidates:
            optimized_root, any_offloaded = optimize_current_state(
                offload_candidates, idempotency_key
            )
            if any_offloaded:
                state.update(optimized_root)
                logger.info("[v3.22] Root-level field offload applied (Fix 1 flat-merge mode)")

    # 3. í¬ì¸í„° ë¹„ëŒ€í™” ë°©ì§€
    state = prevent_pointer_bloat(state, idempotency_key)
    
    # 4. ìµœì¢… í¬ê¸° ì²´í¬
    final_size_kb = calc_size(state)
    warning_threshold = MAX_PAYLOAD_SIZE_KB * 0.75  # 150KB
    
    if final_size_kb > warning_threshold:
        logger.warning(f"Payload approaching limit: {final_size_kb}KB / {MAX_PAYLOAD_SIZE_KB}KB")
        state = emergency_offload_large_arrays(state, idempotency_key)
    
    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    state['payload_size_kb'] = calc_size(state)
    state['last_update_time'] = datetime.now(timezone.utc).isoformat()
    
    return state


def universal_sync_core(
    base_state: Dict[str, Any],
    new_result: Any,
    context: Optional[SyncContext] = None
) -> Dict[str, Any]:
    """
    ğŸ¯ Function-Agnostic ë™ê¸°í™” ì½”ì–´ (v3.2 Engine)
    
    ëª¨ë“  StateDataManager ì•¡ì…˜ì´ ì´ í•¨ìˆ˜ë¥¼ í†µê³¼í•©ë‹ˆë‹¤.
    9ê°œì˜ ì•¡ì…˜ í•¨ìˆ˜ëŠ” ì´ì œ "3ì¤„ì§œë¦¬ ë˜í¼"ì…ë‹ˆë‹¤.
    
    íŒŒì´í”„ë¼ì¸:
        1. flatten_result() - ì…ë ¥ ì •ê·œí™” (ì•¡ì…˜ë³„ ìŠ¤ë§ˆíŠ¸ ì¶”ì¶œ)
        2. merge_logic() - ìƒíƒœ ë³‘í•© (Shallow Merge + CoW)
        3. optimize_and_offload() - ìë™ ìµœì í™” (P0~P2 í•´ê²°)
        4. _compute_next_action() - next_action ê²°ì •
    
    Args:
        base_state: ê¸°ì¡´ state_data
        new_result: ìƒˆë¡œìš´ ì‹¤í–‰ ê²°ê³¼ (ë‹¨ì¼ ê°ì²´ or ë¦¬ìŠ¤íŠ¸)
        context: ë™ê¸°í™” ì»¨í…ìŠ¤íŠ¸ (action, execution_id, merge_strategy ë“±)
    
    Returns:
        {
            'state_data': ìµœì í™”ëœ ìƒíƒœ,
            'next_action': 'CONTINUE' | 'COMPLETE' | 'FAILED' | ...
        }
    """
    logger = _get_logger()
    
    # ğŸ›¡ï¸ [v3.4 Deep Guard] None ë°©ì§€ - Immutable Empty Dict ì „ëµ
    # ì ˆëŒ€ë¡œ Noneì´ íŒŒì´í”„ë¼ì¸ì„ í†µê³¼í•˜ì§€ ëª»í•˜ê²Œ í•¨
    if base_state is None:
        logger.warning("ğŸš¨ [Deep Guard] base_state is None! Using empty dict.")
        base_state = {}
    
    if new_result is None:
        logger.warning("ğŸš¨ [Deep Guard] new_result is None! Using empty dict.")
        new_result = {}
    
    if context is None:
        context = {'action': 'sync'}
    
    action = context.get('action', 'sync') if context else 'sync'
    idempotency_key = base_state.get('idempotency_key', 'unknown') if isinstance(base_state, dict) else 'unknown'
    
    # ì»¨í…ìŠ¤íŠ¸ì— idempotency_key ì¶”ê°€
    if context:
        context['idempotency_key'] = idempotency_key
    else:
        context = {'action': action, 'idempotency_key': idempotency_key}
    
    logger.info(f"UniversalSyncCore v3.2: action={action}")
    
    # Step 1: ì…ë ¥ ì •ê·œí™” (ì•¡ì…˜ë³„ ìŠ¤ë§ˆíŠ¸ ì¶”ì¶œ)
    normalized_delta = flatten_result(new_result, context)
    
    # Step 2: ìƒíƒœ ë³‘í•© (Shallow Merge + CoW)
    updated_state = merge_logic(base_state, normalized_delta, context)
    
    # ğŸ” [Debug] Log loop_counter after merge for troubleshooting
    logger.info(f"[v3.14 Debug] After merge_logic: loop_counter={updated_state.get('loop_counter')}, "
               f"base_state.loop_counter={base_state.get('loop_counter') if isinstance(base_state, dict) else 'N/A'}")
    
    # Step 3: ê³µí†µ í•„ë“œ ì—…ë°ì´íŠ¸ (ë£¨í”„ ì¹´ìš´í„°, ì„¸ê·¸ë¨¼íŠ¸)
    # ğŸ›¡ï¸ [v3.14 Fix] loop_counter ì¦ê°€ëŠ” ASL IncrementLoopCounterì—ì„œë§Œ ìˆ˜í–‰
    # USCì—ì„œ ì¤‘ë³µ ì¦ê°€í•˜ë©´ ë¬´í•œ ë£¨í”„ ë°©ì§€ ë¡œì§ì´ ê¹¨ì§
    # should_increment_loop ë¡œì§ ì œê±° - ASLì´ loop_counter ì¦ê°€ ë‹´ë‹¹
    # 
    # REMOVED:
    # should_increment_loop = (action == 'sync' or normalized_delta.get('_increment_loop', False))
    # if should_increment_loop and action != 'init':
    #     updated_state['loop_counter'] = int(updated_state.get('loop_counter', 0)) + 1
    
    # ì„¸ê·¸ë¨¼íŠ¸ ì¦ê°€ (í”Œë˜ê·¸ê°€ ìˆëŠ” ê²½ìš°)
    if normalized_delta.get('_increment_segment', False):
        updated_state['segment_to_run'] = int(updated_state.get('segment_to_run', 0)) + 1
    
    # Step 4: ìë™ ìµœì í™” (P0~P2 í•´ê²°)
    optimized_state = optimize_and_offload(updated_state, context)
    
    # Step 5: next_action ê²°ì •
    next_action = _compute_next_action(optimized_state, normalized_delta, action)
    
    # pending_branches ì •ë¦¬ (aggregate_branches ì™„ë£Œ ì‹œ)
    if action == 'aggregate_branches' and normalized_delta.get('_aggregation_complete'):
        optimized_state.pop('pending_branches', None)
        optimized_state['segment_to_run'] = int(optimized_state.get('segment_to_run', 0)) + 1

    # [H-1 Delayed Deletion] íŒŒì´í”„ë¼ì¸ ë‚´ë¶€ ì œì–´ ì‹ í˜¸ë¥¼ Persist ì§ì „ì— ì œê±°
    # _compute_next_actionì´ normalized_deltaì—ì„œ ì½ìœ¼ë¯€ë¡œ stateì—ì„œ ì œê±°í•´ë„ ì•ˆì „
    # _failed_segments/_errorëŠ” ì¶”ì  ëª©ì ìœ¼ë¡œ ë³´ì¡´
    _PIPELINE_INTERNAL_KEYS = frozenset({
        '_status', '_is_init', '_increment_segment', '_increment_loop',
        '_aggregation_complete', '_is_pointer_mode', '_soft_fail_branches',
    })
    for _k in _PIPELINE_INTERNAL_KEYS:
        optimized_state.pop(_k, None)

    logger.info(f"UniversalSyncCore complete: action={action}, next={next_action}, size={optimized_state.get('payload_size_kb', 0)}KB")

    return {
        'state_data': optimized_state,
        'next_action': next_action
    }


def _compute_next_action(
    state: Dict[str, Any],
    delta: Dict[str, Any],
    action: str
) -> str:
    """
    ğŸ¯ next_action ê²°ì • ë¡œì§ (ì¤‘ì•™í™”)
    
    ëª¨ë“  ì•¡ì…˜ì˜ next_actionì„ ë‹¨ì¼ ë¡œì§ìœ¼ë¡œ ê²°ì •í•©ë‹ˆë‹¤.
    
    íƒ„ìƒ (init): 'STARTED' ë°˜í™˜
    
    ğŸ›¡ï¸ [v3.3] íƒ€ì… ì•ˆì „ì„± ê°•í™” - TypeError ë°©ì§€
    """
    # íƒ„ìƒ (init) - ì‹œì‘ ìƒíƒœ
    if action == 'init' or delta.get('_is_init'):
        return 'STARTED'
    
    # deltaì—ì„œ ìƒíƒœ ì¶”ì¶œ (ë¬¸ìì—´ ì •ê·œí™”)
    raw_status = delta.get('_status', 'CONTINUE')
    status = str(raw_status).upper() if raw_status is not None else 'CONTINUE'
    
    logger = _get_logger()
    logger.info(f"[_compute_next_action] action={action}, raw_status={raw_status}, normalized_status={status}")

    # [C-3 Fix] ë¸Œëœì¹˜ ì „ìš© íƒˆì¶œ ì¡°ê±´: _statusë§Œìœ¼ë¡œ ê²°ì •
    # main workflowì˜ total_segments/segment_to_run ì˜¤ì—¼ ë°©ì§€
    if action == 'sync_branch':
        if status in ('COMPLETE', 'SUCCESS', 'SUCCEEDED'):
            logger.info(f"[_compute_next_action sync_branch] Branch completed: status={status}")
            return 'COMPLETE'
        if status in ('FAILED', 'HALTED', 'SIGKILL', 'LOOP_LIMIT_EXCEEDED', 'PARTIAL_FAILURE'):
            logger.warning(f"[_compute_next_action sync_branch] Branch failed: status={status}")
            return 'FAILED'
        if status in ('PAUSED_FOR_HITP', 'PAUSE'):
            logger.info(f"[_compute_next_action sync_branch] Branch paused for HITP")
            return 'PAUSED_FOR_HITP'
        # CONTINUE ë˜ëŠ” ê¸°íƒ€ â†’ ë¸Œëœì¹˜ ë£¨í”„ ê³„ì†
        logger.info(f"[_compute_next_action sync_branch] Branch continues: status={status}")
        return 'CONTINUE'

    # ëª…ì‹œì  ì‹¤íŒ¨/ì¤‘ë‹¨ ìƒíƒœ (ğŸ›¡ï¸ [v3.16] HALTED/SIGKILL â†’ FAILED ì •ê·œí™”)
    if status in ('FAILED', 'HALTED', 'SIGKILL'):
        # ASLì—ëŠ” HALTED/SIGKILL caseê°€ ì—†ìœ¼ë¯€ë¡œ FAILEDë¡œ í†µì¼
        if status in ('HALTED', 'SIGKILL'):
            logger.warning(f"[_compute_next_action] Normalizing {status} to FAILED for ASL compatibility")
            return 'FAILED'
        logger.info(f"[_compute_next_action] Returning failure status: {status}")
        return status
    
    # ëª…ì‹œì  ì™„ë£Œ (SUCCESS, SUCCEEDEDë„ COMPLETEë¡œ ì²˜ë¦¬)
    if status in ('COMPLETE', 'SUCCESS', 'SUCCEEDED'):
        # ğŸ›¡ï¸ [v3.16 Fix] next_segmentê°€ ìˆìœ¼ë©´ COMPLETE ë¬´ì‹œ (ì¡°ê¸° ì¢…ë£Œ ë°©ì§€)
        if delta.get('segment_to_run') is not None:
            next_seg = delta.get('segment_to_run')
            logger.warning(
                f"[_compute_next_action] Status is {status} but next_segment={next_seg} exists. "
                f"This may indicate incorrect status. Treating as CONTINUE."
            )
            return 'CONTINUE'
        
        logger.info(f"[_compute_next_action] Workflow completed with status={status}, returning COMPLETE")
        return 'COMPLETE'
    
    # HITP ëŒ€ê¸°
    if status in ('PAUSED_FOR_HITP', 'PAUSE'):
        return 'PAUSED_FOR_HITP'
    
    # Distributed ì „ì²´ ì‹¤íŒ¨
    if delta.get('_aggregation_complete'):
        failed = delta.get('_failed_segments', [])
        chunk_summary = delta.get('distributed_chunk_summary')
        total = chunk_summary.get('total', 0) if isinstance(chunk_summary, dict) else 0
        if failed and len(failed) == total:
            return 'FAILED'
    
    # ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì—†ìœ¼ë©´ ì™„ë£Œ
    if delta.get('segment_to_run') is None and status == 'CONTINUE':
        # ğŸ›¡ï¸ [Guard] ì•ˆì „í•œ ìˆ«ì ë¹„êµ - TypeError ë°©ì§€
        try:
            current_segment = int(state.get('segment_to_run', 0) or 0)
            total_segments_raw = state.get('total_segments')
            
            if total_segments_raw is not None:
                total_segments = int(total_segments_raw)
                if current_segment >= total_segments - 1:
                    logger.info(f"[_compute_next_action] Last segment reached: {current_segment + 1}/{total_segments}, returning COMPLETE")
                    return 'COMPLETE'
        except (ValueError, TypeError) as e:
            logger.warning(
                f"[_compute_next_action] Invalid segment numbers: "
                f"segment_to_run={state.get('segment_to_run')}, "
                f"total_segments={state.get('total_segments')}. Error: {e}. Defaulting to CONTINUE."
            )
    
    # pending_branchesê°€ ìˆìœ¼ë©´ ë³‘ë ¬ ì²˜ë¦¬
    pending = state.get('pending_branches') or delta.get('pending_branches')
    if pending:
        # ğŸ›¡ï¸ [v3.16 Fix] ë¹ˆ ë°°ì—´ ì²´í¬ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        if isinstance(pending, list) and len(pending) > 0:
            logger.info(f"[_compute_next_action] Pending branches detected ({len(pending)}), returning PARALLEL_GROUP")
            return 'PARALLEL_GROUP'
        else:
            logger.warning(f"[_compute_next_action] pending_branches is empty or invalid: {type(pending).__name__}, treating as CONTINUE")
    
    logger.info(f"[_compute_next_action] No special conditions matched, returning CONTINUE")
    return 'CONTINUE'


# ============================================
# Backward Compatibility - ê¸°ì¡´ í•¨ìˆ˜ ë˜í¼
# ============================================

def load_from_s3_with_retry(
    s3_path: str,
    expected_checksum: Optional[str] = None
) -> Any:
    """
    ê¸°ì¡´ load_from_s3ì˜ ì¬ì‹œë„ + ì²´í¬ì„¬ ê²€ì¦ ë²„ì „
    
    backward compatible ë˜í¼ë¡œ, ê¸°ì¡´ ì½”ë“œì—ì„œ drop-in replacementë¡œ ì‚¬ìš© ê°€ëŠ¥
    """
    return get_default_hydrator().load_from_s3(s3_path, expected_checksum)
