"""
InfraSmokeTester: Infrastructure Health Check Lambda

ì´ í•¨ìˆ˜ëŠ” Analemma ì¸í”„ë¼ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œê°€ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ ì„ ì œì ìœ¼ë¡œ ê²€ì¦í•©ë‹ˆë‹¤.
5ë¶„ë§ˆë‹¤ EventBridgeë¡œ íŠ¸ë¦¬ê±°ë˜ë©°, ì‹¤íŒ¨ ì‹œ CloudWatch ì•ŒëŒì„ í†µí•´ ì¦‰ì‹œ ì•Œë¦¼ì„ ë°œì†¡í•©ë‹ˆë‹¤.

ì²´í¬ ëŒ€ìƒ:
- S3: smoketest/{request_id}.txt íŒŒì¼ ìƒì„±/ì½ê¸°/ì‚­ì œ (ë³‘ë ¬ ì•ˆì „)
- DynamoDB: Executions í…Œì´ë¸” ìƒíƒœ í™•ì¸
- Bedrock: ì‹¤ì œ 1-í† í° ì¶”ë¡  í…ŒìŠ¤íŠ¸ (Haiku ì‚¬ìš©)
- Step Functions: ìµœê·¼ 5ë¶„ ë‚´ ì‹¤í–‰ ì¤‘ FAILED ìƒíƒœ í™•ì¸
- PII Masking: ì´ë©”ì¼ ë§ˆìŠ¤í‚¹ & URL ë³´ì¡´ ê²€ì¦
- S3 Offloading: 256KB ì´ˆê³¼ ë°ì´í„° ìë™ ì˜¤í”„ë¡œë”© ê²€ì¦
"""
import json
import os
import logging
import time
from datetime import datetime, timezone, timedelta
from typing import Any

import boto3
from botocore.exceptions import ClientError

# Services for logic tests
from src.services.common.pii_masking_service import get_pii_masking_service
from src.services.state.state_persistence_service import StatePersistenceService

logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Environment Variables
DATA_BUCKET = os.environ.get("WORKFLOW_STATE_BUCKET", "")
EXECUTIONS_TABLE = os.environ.get("EXECUTIONS_TABLE", "")
ORCHESTRATOR_ARN = os.environ.get("WORKFLOW_ORCHESTRATOR_ARN", "")
DISTRIBUTED_ORCHESTRATOR_ARN = os.environ.get("WORKFLOW_DISTRIBUTED_ORCHESTRATOR_ARN", "")
# ğŸš¨ [Critical Fix] ê¸°ë³¸ê°’ì„ template.yamlê³¼ ì¼ì¹˜ì‹œí‚´
WORKFLOWS_TABLE = os.environ.get("WORKFLOWS_TABLE", "WorkflowsTableV3")
METRIC_NAMESPACE = "Analemma/Engine"


def put_custom_metric(metric_name: str, value: float, dimensions: list[dict] | None = None) -> None:
    """CloudWatchì— ì»¤ìŠ¤í…€ ë§¤íŠ¸ë¦­ì„ ë°œí–‰í•©ë‹ˆë‹¤."""
    try:
        cloudwatch = boto3.client("cloudwatch")
        metric_data = {
            "MetricName": metric_name,
            "Value": value,
            "Unit": "Count",
        }
        if dimensions:
            metric_data["Dimensions"] = dimensions
        
        cloudwatch.put_metric_data(
            Namespace=METRIC_NAMESPACE,
            MetricData=[metric_data]
        )
        logger.info(f"Metric emitted: {metric_name}={value}")
    except Exception as e:
        logger.error(f"Failed to emit metric {metric_name}: {e}")


def check_s3_permission(bucket_name: str, request_id: str) -> dict[str, Any]:
    """
    S3 ë²„í‚·ì— íŒŒì¼ì„ ìƒì„±/ì½ê¸°/ì‚­ì œí•˜ì—¬ ì—°ê²°ì„±ê³¼ ê¶Œí•œì„ í™•ì¸í•©ë‹ˆë‹¤.
    ë³‘ë ¬ ì‹¤í–‰ ì‹œ ì¶©ëŒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ request_id ê¸°ë°˜ ê³ ìœ  íŒŒì¼ëª…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    result = {"service": "S3", "bucket": bucket_name, "status": "OK", "details": None}
    
    if not bucket_name:
        result["status"] = "SKIPPED"
        result["details"] = "WORKFLOW_STATE_BUCKET not configured"
        return result
    
    s3 = boto3.client("s3")
    test_key = f"smoketest/{request_id}.txt"
    test_content = f"healthcheck-{int(time.time())}"
    
    try:
        # 1. Write
        s3.put_object(Bucket=bucket_name, Key=test_key, Body=test_content.encode())
        
        # 2. Read
        response = s3.get_object(Bucket=bucket_name, Key=test_key)
        read_content = response["Body"].read().decode()
        
        if read_content != test_content:
            raise ValueError("Content mismatch after read")
        
        # 3. Delete
        s3.delete_object(Bucket=bucket_name, Key=test_key)
        
        result["status"] = "OK"
    except ClientError as e:
        result["status"] = "ERROR"
        result["details"] = str(e)
        logger.error(f"S3 check failed: {e}")
    except Exception as e:
        result["status"] = "ERROR"
        result["details"] = str(e)
        logger.error(f"S3 check failed: {e}")
    
    return result


def check_dynamo_status(table_name: str) -> dict[str, Any]:
    """
    DynamoDB í…Œì´ë¸”ì´ ACTIVE ìƒíƒœì¸ì§€, GSIê°€ ì‚´ì•„ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    result = {"service": "DynamoDB", "table": table_name, "status": "OK", "details": None}
    
    if not table_name:
        result["status"] = "SKIPPED"
        result["details"] = "EXECUTIONS_TABLE not configured"
        return result
    
    dynamodb = boto3.client("dynamodb")
    
    try:
        response = dynamodb.describe_table(TableName=table_name)
        table_status = response["Table"]["TableStatus"]
        
        if table_status != "ACTIVE":
            result["status"] = "WARNING"
            result["details"] = f"Table status is {table_status}"
            return result
        
        # GSI ìƒíƒœ í™•ì¸
        gsi_list = response["Table"].get("GlobalSecondaryIndexes", [])
        inactive_gsi = [gsi["IndexName"] for gsi in gsi_list if gsi.get("IndexStatus") != "ACTIVE"]
        
        if inactive_gsi:
            result["status"] = "WARNING"
            result["details"] = f"Inactive GSIs: {inactive_gsi}"
        else:
            result["status"] = "OK"
            
    except ClientError as e:
        result["status"] = "ERROR"
        result["details"] = str(e)
        logger.error(f"DynamoDB check failed: {e}")
    
    return result


def check_bedrock_connectivity() -> dict[str, Any]:
    """
    Bedrockì— ì‹¤ì œ 1-í† í° ì¶”ë¡  ìš”ì²­ì„ ë³´ë‚´ ì—°ê²°ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤.
    ê°€ì¥ ì €ë ´í•œ Haiku ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³ , ì‘ë‹µ í…ìŠ¤íŠ¸ ìœ íš¨ì„±ë„ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    result = {"service": "Bedrock", "status": "OK", "details": None}
    
    # ì•Œë ¤ì§„ ì—ëŸ¬ íŒ¨í„´ ëª©ë¡
    ERROR_PATTERNS = [
        "error",
        "unable to",
        "cannot process",
        "rate limit",
        "quota exceeded"
    ]
    
    try:
        bedrock_runtime = boto3.client("bedrock-runtime")
        
        # Claude 3 Haiku - ê°€ì¥ ì €ë ´í•œ ì˜µì…˜
        model_id = "anthropic.claude-3-haiku-20240307-v1:0"
        
        request_body = json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 1,
            "messages": [{"role": "user", "content": "Hi"}]
        })
        
        response = bedrock_runtime.invoke_model(
            modelId=model_id,
            body=request_body,
            contentType="application/json",
            accept="application/json"
        )
        
        # ì‘ë‹µ êµ¬ì¡° ë° í…ìŠ¤íŠ¸ ìœ íš¨ì„± ê²€ì¦
        response_body = json.loads(response["body"].read())
        
        # 1. content í•„ë“œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if "content" not in response_body:
            result["status"] = "WARNING"
            result["details"] = "Response missing 'content' field"
            return result
        
        # 2. contentê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        content_list = response_body.get("content", [])
        if not content_list:
            result["status"] = "WARNING"
            result["details"] = "Response content is empty"
            return result
        
        # 3. ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì—ëŸ¬ íŒ¨í„´ ê²€ì‚¬
        response_text = ""
        for item in content_list:
            if item.get("type") == "text":
                response_text += item.get("text", "")
        
        response_text_lower = response_text.lower()
        for pattern in ERROR_PATTERNS:
            if pattern in response_text_lower:
                result["status"] = "WARNING"
                result["details"] = f"Response contains error pattern: '{pattern}'"
                return result
        
        # ëª¨ë“  ê²€ì¦ í†µê³¼
        result["status"] = "OK"
        result["details"] = f"Model {model_id} responded: '{response_text[:50]}...'"
            
    except ClientError as e:
        error_code = e.response.get("Error", {}).get("Code", "Unknown")
        if error_code == "AccessDeniedException":
            result["status"] = "ERROR"
            result["details"] = "Bedrock access denied - check IAM permissions"
        elif error_code == "ThrottlingException":
            result["status"] = "WARNING"
            result["details"] = "Bedrock throttled - may indicate high load"
        else:
            result["status"] = "ERROR"
            result["details"] = str(e)
        logger.error(f"Bedrock check failed: {e}")
    except Exception as e:
        result["status"] = "ERROR"
        result["details"] = str(e)
        logger.error(f"Bedrock check failed: {e}")
    
    return result



def check_step_functions(orchestrator_arn: str) -> dict[str, Any]:
    """
    Step Functions ìƒíƒœ ë¨¸ì‹  ìƒíƒœ ë° ìµœê·¼ ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    maxResults=10ìœ¼ë¡œ ì œí•œí•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì´ê³ , ì‹¤íŒ¨ìœ¨ ë§¤íŠ¸ë¦­ì„ ë°œí–‰í•©ë‹ˆë‹¤.
    """
    result = {"service": "StepFunctions", "arn": orchestrator_arn, "status": "OK", "details": None}
    
    if not orchestrator_arn:
        result["status"] = "SKIPPED"
        result["details"] = "WORKFLOW_ORCHESTRATOR_ARN not configured"
        return result
    
    sfn = boto3.client("stepfunctions")
    
    try:
        # 1. ìƒíƒœ ë¨¸ì‹  ì •ì˜ í™•ì¸
        sfn.describe_state_machine(stateMachineArn=orchestrator_arn)
        
        # 2. ìµœê·¼ 10ê°œ ì‹¤í–‰ ì¤‘ FAILEDê°€ ìˆëŠ”ì§€ í™•ì¸ (íš¨ìœ¨ì„±ì„ ìœ„í•´ ì œí•œ)
        executions = sfn.list_executions(
            stateMachineArn=orchestrator_arn,
            maxResults=10  # ì„±ëŠ¥ ìµœì í™”: ìµœê·¼ 10ê°œë§Œ ì¡°íšŒ
        )
        
        recent_executions = executions.get("executions", [])
        total_count = len(recent_executions)
        failed_count = sum(1 for ex in recent_executions if ex.get("status") == "FAILED")
        
        # ğŸ“Š ì‹¤íŒ¨ìœ¨ ë§¤íŠ¸ë¦­ ë°œí–‰ (0.0 ~ 1.0)
        if total_count > 0:
            failure_rate = failed_count / total_count
            # State Machine ì´ë¦„ ì¶”ì¶œ (ARNì—ì„œ)
            sm_name = orchestrator_arn.split(":")[-1] if ":" in orchestrator_arn else "unknown"
            put_custom_metric(
                "StepFunctionsFailureRate",
                failure_rate,
                dimensions=[{"Name": "StateMachine", "Value": sm_name}]
            )
        
        if failed_count > 0:
            result["status"] = "WARNING"
            result["details"] = f"{failed_count}/{total_count} executions failed ({int(failed_count/total_count*100)}%)"
        else:
            result["status"] = "OK"
            result["details"] = f"Last {total_count} executions OK"
            
    except ClientError as e:
        result["status"] = "ERROR"
        result["details"] = str(e)
        logger.error(f"Step Functions check failed: {e}")
    
    return result


# =============================================================================
# LOGIC CHECKS (verify internal services)
# =============================================================================

def verify_pii_masking() -> dict[str, Any]:
    """
    PII ë§ˆìŠ¤í‚¹ ì„œë¹„ìŠ¤ê°€ ì´ë©”ì¼ì„ ë§ˆìŠ¤í‚¹í•˜ê³  URLì„ ë³´ì¡´í•˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    result = {"service": "PII_Masking", "status": "OK", "details": None}
    
    try:
        service = get_pii_masking_service()
        test_url = "https://s3.aws.com/report.pdf"
        test_input = f"Contact dev@analemma.ai or visit {test_url}"
        masked_result = service.mask(test_input)
        
        # ì´ë©”ì¼ ë§ˆìŠ¤í‚¹ í™•ì¸
        email_masked = "[EMAIL_REDACTED]" in masked_result
        # URL ì™„ì „ ë³´ì¡´ í™•ì¸ (ë¶€ë¶„ ì¼ì¹˜ ì•„ë‹Œ ì •í™•í•œ ë¬¸ìì—´ í¬í•¨ ì—¬ë¶€)
        url_preserved = test_url in masked_result
        
        if email_masked and url_preserved:
            result["status"] = "OK"
            result["details"] = f"Email masked, URL preserved. Sample: {masked_result[:60]}..."
        else:
            result["status"] = "ERROR"
            issues = []
            if not email_masked:
                issues.append("Email not masked")
            if not url_preserved:
                issues.append("URL not preserved")
            result["details"] = f"Failed: {', '.join(issues)}. Result: {masked_result[:80]}"
            
    except Exception as e:
        result["status"] = "ERROR"
        result["details"] = str(e)
        logger.error(f"PII Masking check failed: {e}")
    
    return result


def _cleanup_test_data(execution_id: str, owner_id: str, workflow_id: str) -> None:
    """í…ŒìŠ¤íŠ¸ ë°ì´í„° ì™„ì „ ì‚­ì œ (DDB + S3)"""
    try:
        persistence = StatePersistenceService()
        result = persistence.delete_state(execution_id, owner_id, workflow_id)
        logger.info(f"Cleanup result for {execution_id}: {result}")
    except Exception as e:
        logger.warning(f"Cleanup failed for {execution_id}: {e}")


def verify_s3_offloading(request_id: str) -> dict[str, Any]:
    """
    256KB ì´ˆê³¼ ë°ì´í„°ê°€ S3ë¡œ ìë™ ì˜¤í”„ë¡œë”©ë˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
    try-finallyë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë°˜ë“œì‹œ ì •ë¦¬í•©ë‹ˆë‹¤.
    """
    result = {"service": "S3_Offloading", "status": "OK", "details": None}
    
    test_exec_id = f"smoke-{request_id}"
    test_owner_id = "system"
    test_workflow_id = "smoke-test"
    
    try:
        persistence = StatePersistenceService(
            state_bucket=DATA_BUCKET,
            workflows_table=WORKFLOWS_TABLE
        )
        
        # 300KB í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (256KB ì„ê³„ì¹˜ ì´ˆê³¼)
        large_data = {"test_payload": "x" * 300_000, "request_id": request_id}
        payload_size_kb = len(json.dumps(large_data)) // 1024
        
        # ì €ì¥ ì‹œë„ (ë‚´ë¶€ì ìœ¼ë¡œ S3 ì˜¤í”„ë¡œë”© ë°œìƒí•´ì•¼ í•¨)
        save_result = persistence.save_state(
            execution_id=test_exec_id,
            owner_id=test_owner_id,
            workflow_id=test_workflow_id,
            chunk_id="0",
            segment_id=0,
            state_data=large_data
        )
        
        if not save_result.get("saved"):
            result["status"] = "ERROR"
            result["details"] = f"Save failed: {save_result.get('error')}"
            return result
        
        # ë¡œë“œ ë° ê²€ì¦
        load_result = persistence.load_state(
            execution_id=test_exec_id,
            owner_id=test_owner_id,
            workflow_id=test_workflow_id,
            chunk_index=1  # chunk_index > 0 to trigger load
        )
        
        if load_result.get("state_loaded"):
            loaded_data = load_result.get("previous_state", {})
            # ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ (request_id í•„ë“œ ë¹„êµ)
            if loaded_data.get("request_id") == request_id:
                result["status"] = "OK"
                result["details"] = f"Saved and loaded {payload_size_kb}KB successfully"
            else:
                result["status"] = "WARNING"
                result["details"] = "Data mismatch after load"
        else:
            result["status"] = "WARNING"
            result["details"] = f"Load issue: {load_result.get('reason', 'unknown')}"
            
    except Exception as e:
        result["status"] = "ERROR"
        result["details"] = str(e)
        logger.error(f"S3 Offloading check failed: {e}")
    finally:
        # ì„±ê³µ/ì‹¤íŒ¨ ë¬´ê´€í•˜ê²Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
        _cleanup_test_data(test_exec_id, test_owner_id, test_workflow_id)
    
    return result


def lambda_handler(event: dict, context: Any) -> dict:
    """
    ë©”ì¸ í—¬ìŠ¤ì²´í¬ í•¸ë“¤ëŸ¬ - Environment Validation Engine.
    ì¸í”„ë¼ êµ¬ì„± ìš”ì†Œì™€ ë‚´ë¶€ ë¡œì§ì„ ëª¨ë‘ ê²€ì‚¬í•˜ê³  ê²°ê³¼ë¥¼ CloudWatch ë§¤íŠ¸ë¦­ìœ¼ë¡œ ë°œí–‰í•©ë‹ˆë‹¤.
    """
    request_id = context.aws_request_id if context else f"local-{int(time.time())}"
    
    logger.info(f"Starting environment validation (request_id: {request_id})")
    
    # Infrastructure checks
    infra_checks = {
        "S3_DataBucket": check_s3_permission(DATA_BUCKET, request_id),
        "DynamoDB_ExecTable": check_dynamo_status(EXECUTIONS_TABLE),
        "Bedrock_Runtime": check_bedrock_connectivity(),
        "StepFunctions_Orchestrator": check_step_functions(ORCHESTRATOR_ARN),
        "StepFunctions_Distributed": check_step_functions(DISTRIBUTED_ORCHESTRATOR_ARN),
    }
    
    # Logic checks (internal services)
    logic_checks = {
        "PII_Masking": verify_pii_masking(),
        "S3_Offloading": verify_s3_offloading(request_id),
    }
    
    # Combine all results
    all_results = {**infra_checks, **logic_checks}

    # ì‹¤íŒ¨/ê²½ê³  í•­ëª© ì§‘ê³„
    failed_services = [k for k, v in all_results.items() if v.get("status") == "ERROR"]
    warning_services = [k for k, v in all_results.items() if v.get("status") == "WARNING"]
    
    # CloudWatch ë§¤íŠ¸ë¦­ ë°œí–‰
    if failed_services:
        put_custom_metric("InfraHealthStatus", 0)
        overall_status = "ERROR"
    elif warning_services:
        put_custom_metric("InfraHealthStatus", 0.5)  # ê²½ê³  ìƒíƒœ
        overall_status = "WARNING"
    else:
        put_custom_metric("InfraHealthStatus", 1)
        overall_status = "OK"
    
    response = {
        "status": overall_status,
        "timestamp": int(time.time()),
        "request_id": request_id,
        "infra_checks": infra_checks,
        "logic_checks": logic_checks,
    }
    
    if failed_services:
        response["failed"] = failed_services
        logger.error(f"Environment validation FAILED: {failed_services}")
    
    if warning_services:
        response["warnings"] = warning_services
        logger.warning(f"Environment validation WARNINGS: {warning_services}")
    
    logger.info(f"Environment validation completed: {overall_status}")
    
    return response
