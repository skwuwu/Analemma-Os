"""
Slop Detector - ì›Œí¬ìŠ¬ë¡­ íŒ¨í„´ íƒì§€ê¸°
====================================

LLMì´ ìƒì„±í•˜ëŠ” ì €í’ˆì§ˆ "ìŠ¬ë¡­(Slop)" íŒ¨í„´ì„ íƒì§€í•©ë‹ˆë‹¤.

ìŠ¬ë¡­ì˜ íŠ¹ì§•:
    1. ìƒíˆ¬ì  ë¬¸êµ¬ (Boilerplate phrases)
    2. ê³¼ë„í•œ í—¤ì§• (Excessive hedging)
    3. ë¹ˆì•½í•œ ë‚´ìš©ì˜ ì¥í™©í•œ í‘œí˜„ (Verbose emptiness)
    4. ë°˜ë³µì  êµ¬ì¡° (Repetitive structures)
    5. ë©”íƒ€ ì–¸ê¸‰ (Self-referential meta statements)

íƒì§€ ë°©ë²•:
    - íŒ¨í„´ ë§¤ì¹­ (ì •ê·œì‹ ê¸°ë°˜)
    - N-gram ë¹ˆë„ ë¶„ì„
    - ë¬¸ì¥ êµ¬ì¡° ìœ ì‚¬ë„
    - ì •ë³´ ë°€ë„ ì¸¡ì •
"""

import re
from dataclasses import dataclass, field
from typing import List, Dict, Set, Tuple, Optional
from enum import Enum


class SlopCategory(Enum):
    """ìŠ¬ë¡­ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    BOILERPLATE = "boilerplate"           # ìƒíˆ¬ì  ë¬¸êµ¬
    HEDGING = "hedging"                    # ê³¼ë„í•œ í—¤ì§•
    VERBOSE_EMPTINESS = "verbose_emptiness"  # ì¥í™©í•œ ê³µí—ˆí•¨
    REPETITION = "repetition"              # ë°˜ë³µ
    META_STATEMENT = "meta_statement"      # ë©”íƒ€ ì–¸ê¸‰
    FILLER = "filler"                      # ì±„ìš°ê¸° í‘œí˜„
    FALSE_DEPTH = "false_depth"            # ê±°ì§“ ê¹Šì´
    EMOJI_OVERLOAD = "emoji_overload"      # ê³¼ë„í•œ ì´ëª¨í‹°ì½˜ ì‚¬ìš©


@dataclass
class SlopPattern:
    """ìŠ¬ë¡­ íŒ¨í„´ ì •ì˜"""
    pattern: str
    category: SlopCategory
    severity: float  # 0.0 ~ 1.0
    description: str
    language: str = "en"  # en, ko, universal
    whitelist_domains: List[str] = field(default_factory=list)  # ì´ ë„ë©”ì¸ì—ì„œëŠ” ì‹¬ê°ë„ ê°ì†Œ


@dataclass
class EmojiAnalysisResult:
    """ì´ëª¨í‹°ì½˜ ë¶„ì„ ê²°ê³¼"""
    emoji_count: int = 0
    emoji_ratio: float = 0.0  # ë‹¨ì–´ ëŒ€ë¹„ ë¹„ìœ¨
    consecutive_emoji_count: int = 0  # ì—°ì† ì´ëª¨í‹°ì½˜ ìˆ˜
    penalty: float = 0.0
    is_overload: bool = False
    
    def to_dict(self) -> Dict:
        return {
            'emoji_count': self.emoji_count,
            'emoji_ratio': round(self.emoji_ratio, 4),
            'consecutive_emoji_count': self.consecutive_emoji_count,
            'penalty': round(self.penalty, 4),
            'is_overload': self.is_overload
        }


@dataclass
class SlopDetectionResult:
    """ìŠ¬ë¡­ íƒì§€ ê²°ê³¼"""
    is_slop: bool
    slop_score: float  # 0.0 (clean) ~ 1.0 (pure slop)
    detected_patterns: List[Dict]
    category_breakdown: Dict[str, int]
    recommendation: str
    requires_llm_verification: bool
    emoji_analysis: Optional[EmojiAnalysisResult] = None
    domain_adjustments: Dict[str, float] = field(default_factory=dict)
    
    def to_dict(self) -> Dict:
        result = {
            'is_slop': self.is_slop,
            'slop_score': round(self.slop_score, 4),
            'detected_patterns': self.detected_patterns[:10],
            'category_breakdown': self.category_breakdown,
            'recommendation': self.recommendation,
            'requires_llm_verification': self.requires_llm_verification
        }
        if self.emoji_analysis:
            result['emoji_analysis'] = self.emoji_analysis.to_dict()
        if self.domain_adjustments:
            result['domain_adjustments'] = self.domain_adjustments
        return result


class SlopDetector:
    """
    ì›Œí¬ìŠ¬ë¡­ íŒ¨í„´ íƒì§€ê¸°
    
    Kernel Level: RING_1_QUALITY
    Cost: $0 (ë¡œì»¬ íŒ¨í„´ ë§¤ì¹­)
    
    Usage:
        detector = SlopDetector()
        result = detector.detect("In conclusion, it is important to note that...")
        
        if result.is_slop:
            # í’ˆì§ˆ ê²Œì´íŠ¸ì—ì„œ ì°¨ë‹¨ ë˜ëŠ” LLM ê²€ì¦ ìš”ì²­
            pass
    """
    
    # ========================================
    # ì˜ì–´ ìŠ¬ë¡­ íŒ¨í„´
    # ========================================
    # ë„ë©”ì¸ë³„ ì´ëª¨í‹°ì½˜ í—ˆìš© ì •ì±…
    # ========================================
    EMOJI_POLICY = {
        'TECHNICAL_REPORT': {'max_ratio': 0.0, 'severity_multiplier': 2.0},
        'CODE_DOCUMENTATION': {'max_ratio': 0.0, 'severity_multiplier': 2.0},
        'LEGAL_DOCUMENT': {'max_ratio': 0.0, 'severity_multiplier': 2.5},
        'FINANCIAL_REPORT': {'max_ratio': 0.0, 'severity_multiplier': 2.0},
        'MARKETING_COPY': {'max_ratio': 0.05, 'severity_multiplier': 0.5},
        'SOCIAL_MEDIA': {'max_ratio': 0.1, 'severity_multiplier': 0.3},
        'GENERAL_CHAT': {'max_ratio': 0.15, 'severity_multiplier': 0.2},
        'GENERAL_TEXT': {'max_ratio': 0.03, 'severity_multiplier': 1.0},
    }
    
    # ========================================
    # ë„ë©”ì¸ë³„ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸
    # ========================================
    DOMAIN_WHITELIST = {
        'CODE_DOCUMENTATION': [
            r"\b(note that|in this function|returns|parameters)\b",
            r"\b(for example|e\.g\.|i\.e\.)\b",
        ],
        'TECHNICAL_REPORT': [
            r"\b(in summary|to summarize)\b",  # ê¸°ìˆ  ë¦¬í¬íŠ¸ì—ì„œëŠ” ìš”ì•½ í—ˆìš©
            r"\b(it is important to note)\b",
        ],
        'ACADEMIC_PAPER': [
            r"\b(in conclusion|to summarize|furthermore|moreover)\b",
            r"\b(as discussed|as mentioned)\b",
        ],
    }
    
    # ========================================
    # ì˜ì–´ ìŠ¬ë¡­ íŒ¨í„´ (í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë„ë©”ì¸ í¬í•¨)
    # ========================================
    ENGLISH_SLOP_PATTERNS: List[SlopPattern] = [
        # Boilerplate - ìƒíˆ¬ì  ë„ì…/ê²°ë¡ 
        SlopPattern(
            r"\b(in conclusion|to summarize|in summary|to conclude)\b",
            SlopCategory.BOILERPLATE, 0.6,
            "Generic conclusion opener",
            whitelist_domains=['ACADEMIC_PAPER', 'TECHNICAL_REPORT']
        ),
        SlopPattern(
            r"\bit is important to (note|remember|understand|consider) that\b",
            SlopCategory.BOILERPLATE, 0.7,
            "Importance hedging pattern",
            whitelist_domains=['CODE_DOCUMENTATION', 'TECHNICAL_REPORT']
        ),
        SlopPattern(
            r"\b(as (we|I) (can see|have seen|discussed|mentioned))\b",
            SlopCategory.BOILERPLATE, 0.5,
            "Self-referential recap",
            whitelist_domains=['ACADEMIC_PAPER']
        ),
        SlopPattern(
            r"\b(first and foremost|last but not least|at the end of the day)\b",
            SlopCategory.BOILERPLATE, 0.6,
            "ClichÃ© transition phrase"
        ),
        SlopPattern(
            r"\b(it goes without saying|needless to say)\b",
            SlopCategory.BOILERPLATE, 0.7,
            "Unnecessary meta statement"
        ),
        
        # Hedging - ê³¼ë„í•œ ëª¨í˜¸í™”
        SlopPattern(
            r"\b(may or may not|could potentially|might possibly)\b",
            SlopCategory.HEDGING, 0.8,
            "Excessive hedging"
        ),
        SlopPattern(
            r"\b(to some extent|in some ways|in a sense|somewhat)\b",
            SlopCategory.HEDGING, 0.4,
            "Vague qualification"
        ),
        SlopPattern(
            r"\b(it (depends|varies|can vary)|there are (many|various) factors)\b",
            SlopCategory.HEDGING, 0.5,
            "Non-committal response"
        ),
        
        # Verbose Emptiness - ì¥í™©í•œ ê³µí—ˆí•¨
        SlopPattern(
            r"\b(in terms of|with regard to|with respect to|in relation to)\b",
            SlopCategory.VERBOSE_EMPTINESS, 0.3,
            "Bureaucratic filler"
        ),
        SlopPattern(
            r"\b(the fact that|due to the fact that|despite the fact that)\b",
            SlopCategory.VERBOSE_EMPTINESS, 0.4,
            "Wordy fact reference"
        ),
        SlopPattern(
            r"\b(at this point in time|at the present time|in today's world)\b",
            SlopCategory.VERBOSE_EMPTINESS, 0.5,
            "Temporal padding"
        ),
        
        # Meta Statements - AI ìê¸° ì–¸ê¸‰
        SlopPattern(
            r"\b(as an AI|as a language model|I don't have personal)\b",
            SlopCategory.META_STATEMENT, 0.9,
            "AI self-reference (should be filtered at Ring 0)"
        ),
        SlopPattern(
            r"\b(I (cannot|can't|am unable to) (provide|give|offer))\b",
            SlopCategory.META_STATEMENT, 0.7,
            "Capability disclaimer"
        ),
        SlopPattern(
            r"\b(based on (my|the) (training|knowledge|information))\b",
            SlopCategory.META_STATEMENT, 0.6,
            "Training data reference"
        ),
        
        # Filler - ì±„ìš°ê¸° í‘œí˜„
        SlopPattern(
            r"\b(basically|essentially|fundamentally|ultimately)\b",
            SlopCategory.FILLER, 0.3,
            "Emphasis filler"
        ),
        SlopPattern(
            r"\b(very|really|quite|rather|fairly|pretty much)\b",
            SlopCategory.FILLER, 0.2,
            "Intensity modifier (low severity)"
        ),
        
        # False Depth - ê±°ì§“ ê¹Šì´
        SlopPattern(
            r"\b(it's (worth|important to) (noting|mentioning|considering))\b",
            SlopCategory.FALSE_DEPTH, 0.6,
            "False importance signaling"
        ),
        SlopPattern(
            r"\b(this (raises|brings up|highlights) (important|interesting|key))\b",
            SlopCategory.FALSE_DEPTH, 0.5,
            "Pseudo-analytical phrase"
        ),
    ]
    
    # ========================================
    # í•œêµ­ì–´ ìŠ¬ë¡­ íŒ¨í„´
    # ========================================
    KOREAN_SLOP_PATTERNS: List[SlopPattern] = [
        # Boilerplate - ìƒíˆ¬ì  ë¬¸êµ¬
        SlopPattern(
            r"(ê²°ë¡ ì ìœ¼ë¡œ|ìš”ì•½í•˜ìë©´|ì •ë¦¬í•˜ë©´|ë§ˆë¬´ë¦¬í•˜ìë©´)",
            SlopCategory.BOILERPLATE, 0.6,
            "ì¼ë°˜ì ì¸ ê²°ë¡  ì‹œì‘ì–´", "ko"
        ),
        SlopPattern(
            r"(ì£¼ëª©í•  (í•„ìš”ê°€|ì ì´|ë§Œí•œ|ê°€ì¹˜ê°€) ìˆ)",
            SlopCategory.BOILERPLATE, 0.5,
            "ì£¼ëª© ê°•ì¡° íŒ¨í„´", "ko"
        ),
        SlopPattern(
            r"(ë§ì”€ë“œë¦° (ë°”ì™€|ê²ƒì²˜ëŸ¼|ëŒ€ë¡œ))",
            SlopCategory.BOILERPLATE, 0.5,
            "ìê¸° ì°¸ì¡° ë°˜ë³µ", "ko"
        ),
        
        # Hedging - ê³¼ë„í•œ í—¤ì§•
        SlopPattern(
            r"(ì¼ ìˆ˜ë„ ìˆê³  ì•„ë‹ ìˆ˜ë„|ê²½ìš°ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜)",
            SlopCategory.HEDGING, 0.8,
            "ê³¼ë„í•œ ëª¨í˜¸í™”", "ko"
        ),
        SlopPattern(
            r"(ì–´ëŠ ì •ë„|ë‹¤ì†Œ|ì•½ê°„|ì¼ë¶€)",
            SlopCategory.HEDGING, 0.3,
            "ëª¨í˜¸í•œ ìˆ˜ì‹ì–´", "ko"
        ),
        
        # Meta Statements
        SlopPattern(
            r"(AIë¡œì„œ|ì–¸ì–´ ëª¨ë¸ë¡œì„œ|ì œê°€ í•™ìŠµí•œ)",
            SlopCategory.META_STATEMENT, 0.9,
            "AI ìê¸° ì–¸ê¸‰", "ko"
        ),
        SlopPattern(
            r"(ì €ëŠ” (í•  ìˆ˜ ì—†|ë“œë¦¬ê¸° ì–´ë ¤|ì œê³µí•˜ê¸° ì–´ë ¤))",
            SlopCategory.META_STATEMENT, 0.7,
            "ëŠ¥ë ¥ ë¶€ì¸", "ko"
        ),
        
        # Filler
        SlopPattern(
            r"(ê¸°ë³¸ì ìœ¼ë¡œ|ë³¸ì§ˆì ìœ¼ë¡œ|ê¶ê·¹ì ìœ¼ë¡œ|ê²°êµ­)",
            SlopCategory.FILLER, 0.3,
            "ì±„ìš°ê¸° í‘œí˜„", "ko"
        ),
        
        # False Depth
        SlopPattern(
            r"(ì¤‘ìš”í•œ (ì ì€|ê²ƒì€|ì‚¬ì‹¤ì€))",
            SlopCategory.FALSE_DEPTH, 0.5,
            "ê±°ì§“ ì¤‘ìš”ì„± ì‹ í˜¸", "ko"
        ),
        SlopPattern(
            r"(í¥ë¯¸ë¡œìš´ (ì |ë¶€ë¶„|ì‚¬ì‹¤))",
            SlopCategory.FALSE_DEPTH, 0.4,
            "ì˜ì‚¬ ë¶„ì„ í‘œí˜„", "ko"
        ),
    ]
    
    # ========================================
    # ì´ëª¨í‹°ì½˜ ìŠ¬ë¡­ íŒ¨í„´
    # ========================================
    EMOJI_SLOP_PATTERNS: List[SlopPattern] = [
        # ì—°ì†ëœ ì´ëª¨í‹°ì½˜ (2ê°œ ì´ìƒ)
        SlopPattern(
            r"[\u2600-\u27BF\U0001f300-\U0001faff]{2,}",
            SlopCategory.EMOJI_OVERLOAD, 0.7,
            "Consecutive emoji usage"
        ),
        # ìŠ¤íŒŒí´ ì´ëª¨í‹°ì½˜ ë‚¨ìš© (AI ìŠ¬ë¡­ì˜ ëŒ€í‘œì  ì‹ í˜¸)
        SlopPattern(
            r"[âœ¨ğŸŒŸğŸ’«â­]{2,}",
            SlopCategory.EMOJI_OVERLOAD, 0.8,
            "Sparkle emoji overload (AI signature)"
        ),
        # ê¸€ë¨¸ë¦¬ ê¸°í˜¸ + ì´ëª¨í‹°ì½˜ ì¡°í•© (AI ë¦¬ìŠ¤íŠ¸ íŒ¨í„´)
        SlopPattern(
            r"^[\-\*â€¢]\s*[\u2600-\u27BF\U0001f300-\U0001faff]",
            SlopCategory.EMOJI_OVERLOAD, 0.6,
            "Bullet + emoji combo (AI list pattern)"
        ),
        # ë¡œì¼“/í™”ì¬ ì´ëª¨í‹°ì½˜ ë‚¨ìš©
        SlopPattern(
            r"[ğŸš€ğŸ”¥ğŸ’¥]{2,}",
            SlopCategory.EMOJI_OVERLOAD, 0.7,
            "Hype emoji overload"
        ),
    ]
    
    def __init__(
        self,
        slop_threshold: float = 0.5,
        enable_korean: bool = True,
        enable_emoji_detection: bool = True,
        domain: str = "GENERAL_TEXT",
        custom_patterns: Optional[List[SlopPattern]] = None
    ):
        """
        Args:
            slop_threshold: ìŠ¬ë¡­ íŒì • ì„ê³„ê°’ (0.0 ~ 1.0)
            enable_korean: í•œêµ­ì–´ íŒ¨í„´ í™œì„±í™” ì—¬ë¶€
            enable_emoji_detection: ì´ëª¨í‹°ì½˜ íƒì§€ í™œì„±í™” ì—¬ë¶€
            domain: ì½˜í…ì¸  ë„ë©”ì¸ (í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë° ì´ëª¨í‹°ì½˜ ì •ì±… ì ìš©)
            custom_patterns: ì¶”ê°€ ì»¤ìŠ¤í…€ íŒ¨í„´
        """
        self.slop_threshold = slop_threshold
        self.domain = domain.upper()
        self.enable_emoji_detection = enable_emoji_detection
        
        # ë„ë©”ì¸ë³„ ì´ëª¨í‹°ì½˜ ì •ì±…
        self.emoji_policy = self.EMOJI_POLICY.get(
            self.domain,
            self.EMOJI_POLICY['GENERAL_TEXT']
        )
        
        # ë„ë©”ì¸ë³„ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ íŒ¨í„´ ì»´íŒŒì¼
        self.whitelist_patterns: List[re.Pattern] = []
        if self.domain in self.DOMAIN_WHITELIST:
            for pattern_str in self.DOMAIN_WHITELIST[self.domain]:
                self.whitelist_patterns.append(
                    re.compile(pattern_str, re.IGNORECASE)
                )
        
        # íŒ¨í„´ ì»´íŒŒì¼
        self.patterns: List[Tuple[re.Pattern, SlopPattern]] = []
        
        for pattern in self.ENGLISH_SLOP_PATTERNS:
            self.patterns.append((
                re.compile(pattern.pattern, re.IGNORECASE),
                pattern
            ))
        
        if enable_korean:
            for pattern in self.KOREAN_SLOP_PATTERNS:
                self.patterns.append((
                    re.compile(pattern.pattern),
                    pattern
                ))
        
        if enable_emoji_detection:
            for pattern in self.EMOJI_SLOP_PATTERNS:
                self.patterns.append((
                    re.compile(pattern.pattern, re.MULTILINE),
                    pattern
                ))
        
        if custom_patterns:
            for pattern in custom_patterns:
                self.patterns.append((
                    re.compile(pattern.pattern, re.IGNORECASE if pattern.language == "en" else 0),
                    pattern
                ))
    
    def detect(self, text: str) -> SlopDetectionResult:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ìŠ¬ë¡­ íŒ¨í„´ íƒì§€
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            SlopDetectionResult: íƒì§€ ê²°ê³¼
        """
        if not text or len(text.strip()) < 20:
            return self._clean_result()
        
        detected_patterns: List[Dict] = []
        category_counts: Dict[str, int] = {}
        total_severity = 0.0
        domain_adjustments: Dict[str, float] = {}
        
        for compiled_pattern, slop_pattern in self.patterns:
            matches = compiled_pattern.findall(text)
            
            if matches:
                # ë„ë©”ì¸ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì²´í¬ - ì‹¬ê°ë„ ì¡°ì •
                severity = slop_pattern.severity
                is_whitelisted = self.domain in slop_pattern.whitelist_domains
                
                if is_whitelisted:
                    # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë„ë©”ì¸ì—ì„œëŠ” ì‹¬ê°ë„ 70% ê°ì†Œ
                    severity *= 0.3
                    domain_adjustments[slop_pattern.pattern[:30]] = -0.7
                
                for match in matches:
                    detected_patterns.append({
                        'pattern': slop_pattern.pattern[:50],
                        'matched': match if isinstance(match, str) else match[0],
                        'category': slop_pattern.category.value,
                        'severity': severity,
                        'original_severity': slop_pattern.severity,
                        'description': slop_pattern.description,
                        'whitelisted': is_whitelisted
                    })
                    
                    total_severity += severity * len(matches)
                    
                    cat = slop_pattern.category.value
                    category_counts[cat] = category_counts.get(cat, 0) + len(matches)
        
        # ìŠ¬ë¡­ ì ìˆ˜ ê³„ì‚° (ì •ê·œí™”)
        text_length_factor = max(1, len(text) / 500)  # 500ì ê¸°ì¤€ ì •ê·œí™”
        slop_score = min(1.0, total_severity / (text_length_factor * 5))
        
        # ë°˜ë³µ êµ¬ì¡° ë¶„ì„ìœ¼ë¡œ ì¶”ê°€ ì ìˆ˜
        repetition_penalty = self._analyze_sentence_repetition(text)
        slop_score = min(1.0, slop_score + repetition_penalty)
        
        # ========================================
        # ì´ëª¨í‹°ì½˜ ë°€ë„ ë¶„ì„ (ë„ë©”ì¸ë³„ ì •ì±… ì ìš©)
        # ========================================
        emoji_result = None
        if self.enable_emoji_detection:
            emoji_result = self._analyze_emoji_density(text)
            if emoji_result.penalty > 0:
                slop_score = min(1.0, slop_score + emoji_result.penalty)
                if emoji_result.is_overload:
                    category_counts[SlopCategory.EMOJI_OVERLOAD.value] = emoji_result.emoji_count
        
        is_slop = slop_score >= self.slop_threshold
        
        # ê¶Œì¥ ì‚¬í•­ ê²°ì •
        if slop_score < 0.3:
            recommendation = "PASS: Content appears to have good information density"
        elif slop_score < 0.5:
            recommendation = "REVIEW: Some slop patterns detected, consider LLM verification"
        elif slop_score < 0.7:
            recommendation = "WARN: Significant slop detected, LLM verification recommended"
        else:
            recommendation = "REJECT: High slop concentration, content should be regenerated"
        
        # LLM ê²€ì¦ í•„ìš” ì—¬ë¶€ (0.3 ~ 0.7 êµ¬ê°„ì€ ë¶ˆí™•ì‹¤)
        requires_llm = 0.3 <= slop_score <= 0.7
        
        return SlopDetectionResult(
            is_slop=is_slop,
            slop_score=slop_score,
            detected_patterns=detected_patterns,
            category_breakdown=category_counts,
            recommendation=recommendation,
            requires_llm_verification=requires_llm,
            emoji_analysis=emoji_result,
            domain_adjustments=domain_adjustments
        )
    
    def _analyze_emoji_density(self, text: str) -> EmojiAnalysisResult:
        """
        ì´ëª¨í‹°ì½˜ ë°€ë„ ë¶„ì„ ë° í˜ë„í‹° ê³„ì‚°
        
        ë„ë©”ì¸ë³„ ì •ì±… ì ìš©:
        - TECHNICAL_REPORT: 0ê°œ ê¶Œì¥ (ë§¤ìš° ì—„ê²©)
        - MARKETING_COPY: 5% í—ˆìš©
        - GENERAL_CHAT: 15% í—ˆìš©
        """
        # ëª¨ë“  ì´ëª¨í‹°ì½˜ ì¶”ì¶œ (ìœ ë‹ˆì½”ë“œ ë²”ìœ„)
        emoji_pattern = r"[\u2600-\u27BF\U0001f300-\U0001faff\U0001f600-\U0001f64f\U0001f680-\U0001f6ff]"
        emojis = re.findall(emoji_pattern, text)
        
        if not emojis:
            return EmojiAnalysisResult()
        
        emoji_count = len(emojis)
        word_count = len(text.split())
        
        # ì—°ì† ì´ëª¨í‹°ì½˜ íƒì§€
        consecutive_pattern = r"[\u2600-\u27BF\U0001f300-\U0001faff\U0001f600-\U0001f64f\U0001f680-\U0001f6ff]{2,}"
        consecutive_matches = re.findall(consecutive_pattern, text)
        consecutive_count = sum(len(m) for m in consecutive_matches)
        
        # ì´ëª¨í‹°ì½˜ ë¹„ìœ¨ ê³„ì‚°
        emoji_ratio = emoji_count / word_count if word_count > 0 else 0
        
        # ë„ë©”ì¸ ì •ì±… ì ìš©
        max_ratio = self.emoji_policy['max_ratio']
        severity_multiplier = self.emoji_policy['severity_multiplier']
        
        # í˜ë„í‹° ê³„ì‚°
        penalty = 0.0
        is_overload = False
        
        if emoji_ratio > max_ratio:
            excess_ratio = emoji_ratio - max_ratio
            
            if excess_ratio > 0.15:  # ë§¤ìš° ì‹¬ê° (í—ˆìš©ì¹˜ + 15% ì´ˆê³¼)
                penalty = 0.4 * severity_multiplier
                is_overload = True
            elif excess_ratio > 0.08:  # ì‹¬ê°
                penalty = 0.25 * severity_multiplier
                is_overload = True
            elif excess_ratio > 0.03:  # ì£¼ì˜
                penalty = 0.15 * severity_multiplier
            else:  # ê²½ë¯¸
                penalty = 0.05 * severity_multiplier
        
        # ì—°ì† ì´ëª¨í‹°ì½˜ ì¶”ê°€ í˜ë„í‹°
        if consecutive_count >= 3:
            penalty += 0.1 * severity_multiplier
            is_overload = True
        
        penalty = min(0.5, penalty)  # ìµœëŒ€ 0.5
        
        return EmojiAnalysisResult(
            emoji_count=emoji_count,
            emoji_ratio=emoji_ratio,
            consecutive_emoji_count=consecutive_count,
            penalty=penalty,
            is_overload=is_overload
        )
    
    def _analyze_sentence_repetition(self, text: str) -> float:
        """ë¬¸ì¥ êµ¬ì¡° ë°˜ë³µ ë¶„ì„"""
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        if len(sentences) < 3:
            return 0.0
        
        # ë¬¸ì¥ ì‹œì‘ íŒ¨í„´ ë¶„ì„
        starters = [s.split()[0].lower() if s.split() else "" for s in sentences]
        starter_counts = {}
        for starter in starters:
            starter_counts[starter] = starter_counts.get(starter, 0) + 1
        
        # ê°™ì€ ì‹œì‘ì–´ê°€ 30% ì´ìƒì´ë©´ í˜ë„í‹°
        max_repetition = max(starter_counts.values()) / len(starters)
        
        if max_repetition > 0.5:
            return 0.2
        elif max_repetition > 0.3:
            return 0.1
        
        return 0.0
    
    def _clean_result(self) -> SlopDetectionResult:
        """ê¹¨ë—í•œ ê²°ê³¼ (ìŠ¬ë¡­ ì—†ìŒ)"""
        return SlopDetectionResult(
            is_slop=False,
            slop_score=0.0,
            detected_patterns=[],
            category_breakdown={},
            recommendation="PASS: Text too short to analyze or clean",
            requires_llm_verification=False
        )
    
    @staticmethod
    def quick_slop_check(text: str) -> bool:
        """
        ë¹ ë¥¸ ìŠ¬ë¡­ ì²´í¬ (ê°„ëµí™”ëœ ë²„ì „)
        
        Cost: $0
        Latency: < 1ms
        
        Returns:
            bool: True if likely contains slop
        """
        if not text or len(text) < 50:
            return False
        
        # í•µì‹¬ ìŠ¬ë¡­ íŒ¨í„´ë§Œ ì²´í¬
        critical_patterns = [
            r"\bit is important to note that\b",
            r"\bin conclusion\b",
            r"\bas an AI\b",
            r"\bto summarize\b",
            r"\bfirst and foremost\b",
            r"ê²°ë¡ ì ìœ¼ë¡œ",
            r"AIë¡œì„œ",
        ]
        
        text_lower = text.lower()
        matches = sum(1 for p in critical_patterns if re.search(p, text_lower, re.IGNORECASE))
        
        return matches >= 2  # 2ê°œ ì´ìƒ ë§¤ì¹­ ì‹œ ìŠ¬ë¡­ ì˜ì‹¬
