import heapq
import os
import json
import logging
import threading
from typing import Dict, Any, List, Set, Optional, Tuple, FrozenSet

logger = logging.getLogger(__name__)

# ============================================================================
# [v2.0 Production Hardening] ìƒìˆ˜ ë° ì„¤ì •
# ============================================================================

# ìµœëŒ€ ì¬ê·€ ê¹Šì´ ì œí•œ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
MAX_PARTITION_DEPTH = int(os.environ.get("MAX_PARTITION_DEPTH", "50"))

# ìµœëŒ€ ë…¸ë“œ ìˆ˜ ì œí•œ (ëŒ€ê·œëª¨ ê·¸ë˜í”„ ë³´í˜¸)
MAX_NODES_LIMIT = int(os.environ.get("MAX_NODES_LIMIT", "500"))

# ì„±ëŠ¥ ê²½ê³  ì„ê³„ê°’ (100ê°œ ë…¸ë“œ ì´ˆê³¼ ì‹œ ê²½ê³ )
# ë³µì¡í•œ ê·¸ë˜í”„ì—ì„œ ìœ„ìƒ ì •ë ¬/ì‚¬ì´í´ ê°ì§€ latency ì¦ê°€
PERFORMANCE_WARNING_NODE_COUNT = int(os.environ.get("PERFORMANCE_WARNING_NODE_COUNT", "100"))

# LLM ë…¸ë“œ íƒ€ì…ë“¤ - ì´ íƒ€ì…ë“¤ì„ ë§Œë‚  ë•Œë§ˆë‹¤ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤
# Note: Specific vendor types (openai_chat, anthropic_chat, etc.) are mapped to llm_chat via NODE_TYPE_ALIASES
LLM_NODE_TYPES: FrozenSet[str] = frozenset({
    "llm_chat",
    "aiModel"  # ë²”ìš© AI ëª¨ë¸ ë…¸ë“œ íƒ€ì… (llm_chatì˜ ë³„ì¹­)
})

# HITP (Human in the Loop) ì—£ì§€ íƒ€ì…ë“¤
HITP_EDGE_TYPES: FrozenSet[str] = frozenset({"hitp", "human_in_the_loop", "pause"})

# ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì…ë“¤
SEGMENT_TYPES: FrozenSet[str] = frozenset({
    "normal", "llm", "hitp", "isolated", "complete", "parallel_group", "aggregator"
})


# ============================================================================
# [v3.17] Loop Limit Constants â€” Complexity-Budget Counting
# ============================================================================
#
# max_loop_iterationsëŠ” ë‘ ê°€ì§€ ì—­í• ì„ í•©ë‹ˆë‹¤:
#   1. SFN loop_counterê°€ ì´ ê°’ì„ ì´ˆê³¼í•˜ë©´ LoopLimitExceeded ë°œìƒ (ì•ˆì „ ì¥ì¹˜)
#   2. ì›Œí¬í”Œë¡œìš° ë³µì¡ë„ì— ë¹„ë¡€í•˜ëŠ” ì¶©ë¶„í•œ ì‹¤í–‰ ì—¬ìœ ë¥¼ ì œê³µ (ì˜ˆì‚° ì—­í• )
#
# [ë¶„ì„] for_each ì²˜ë¦¬ ë°©ì‹:
#   - for_each_runnerëŠ” Lambda ë‚´ë¶€ ThreadPoolExecutorë¡œ ëª¨ë“  ì•„ì´í…œ ì²˜ë¦¬
#   - SFN loop_counter ê´€ì : parallel_group(PARALLEL_GROUP ê²½ë¡œ, +0) +
#     aggregator(CONTINUE ê²½ë¡œ, +1) = ì´ 1~2íšŒ ì¦ê°€ (SFN ì „ì´ ë¬´ê´€)
#   - ê·¸ëŸ¬ë‚˜ Lambda ë‚´ë¶€ì—ì„œ sub_node_count Ã— max_iterations ë§Œí¼ ì‹¤í–‰ ë°œìƒ
#   - ì´ ë‚´ë¶€ ë³µì¡ë„ë¥¼ ë¬´ì‹œí•˜ë©´ limitì´ ë„ˆë¬´ ë‚®ì•„ í…ŒìŠ¤íŠ¸/ì•ˆì „ ê¸°ì¤€ ë¯¸ë‹¬
#   - ë”°ë¼ì„œ Lambda-internal ë³µì¡ë„ ì˜ˆì‚°: sub_node_count Ã— max_iterations
#
# [ë¶„ì„] sequential loop ì²˜ë¦¬ ë°©ì‹:
#   - ê° ë°˜ë³µë§ˆë‹¤ loop body ì„¸ê·¸ë¨¼íŠ¸ë“¤ì´ ì‹¤ì œ SFN CONTINUE ì „ì´ ë°œìƒ
#   - weight = segment_count Ã— (max_iter - 1) (ì²« ë²ˆì§¸ ë°˜ë³µì€ base countì— í¬í•¨)
#
# [ê³µì‹] loop_limit = max(int(raw * 1.5) + 20, 50)
#   - raw = base_segments + sequential_loop_weight + for_each_complexity_budget
#   - 1.5x: API ì¬ì‹œë„Â·ë§ˆì´ë„ˆ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  ì—¬ìœ 
#   - +20: ê·œëª¨ ë¬´ê´€ ìµœì†Œ ì™„ì¶©
#   - floor=50: ì‹¤ì§ˆì  ë¬´í•œë£¨í”„ ì°¨ë‹¨

# ìµœì¢… estimated_executionsì— ê³±í•  ì•ˆì „ ë°°ìˆ˜ (1.5x)
LOOP_LIMIT_SAFETY_MULTIPLIER: float = float(os.environ.get("LOOP_LIMIT_SAFETY_MULTIPLIER", "1.5"))

# loop_limit ê³ ì • ë³´ë„ˆìŠ¤ â€” ê·œëª¨ ë¬´ê´€í•˜ê²Œ ìµœì†Œ ì™„ì¶© ë³´ì¥
LOOP_LIMIT_FLAT_BONUS: int = int(os.environ.get("LOOP_LIMIT_FLAT_BONUS", "20"))

# loop_limit ì ˆëŒ€ í•˜í•œì„  â€” ì–´ë–¤ ê²½ìš°ì—ë„ ì´ ê°’ ì´ìƒì„ ë³´ì¥
LOOP_LIMIT_FLOOR: int = int(os.environ.get("LOOP_LIMIT_FLOOR", "50"))


# ============================================================================
# [Critical Fix #1] ì‚¬ì´í´ ê°ì§€ ì˜ˆì™¸ ë° DAG ê²€ì¦
# ============================================================================

class CycleDetectedError(Exception):
    """ê·¸ë˜í”„ì—ì„œ ì‚¬ì´í´(ìˆœí™˜ ì°¸ì¡°)ì´ ê°ì§€ë˜ì—ˆì„ ë•Œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
    def __init__(self, cycle_path: List[str]):
        self.cycle_path = cycle_path
        super().__init__(
            f"Cycle detected in workflow graph: {' -> '.join(cycle_path)}. "
            f"Workflows must be DAGs (Directed Acyclic Graphs)."
        )


class PartitionDepthExceededError(Exception):
    """íŒŒí‹°ì…”ë‹ ì¬ê·€ ê¹Šì´ ì´ˆê³¼ ì‹œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
    def __init__(self, depth: int, max_depth: int):
        self.depth = depth
        self.max_depth = max_depth
        super().__init__(
            f"Partition recursion depth ({depth}) exceeded maximum ({max_depth}). "
            f"Consider simplifying the workflow or increasing MAX_PARTITION_DEPTH."
        )


class BranchTerminationError(Exception):
    """ë¸Œëœì¹˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì¢…ë£Œë˜ì§€ ì•Šì•˜ì„ ë•Œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
    def __init__(self, branch_id: str, message: str):
        self.branch_id = branch_id
        super().__init__(f"Branch '{branch_id}' termination error: {message}")


class AtomicGroupTimeoutError(Exception):
    """Atomic Groupì˜ ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ì´ Lambda ì œí•œì„ ì´ˆê³¼í•  ë•Œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸"""
    def __init__(self, group_id: str, estimated_duration: float, lambda_timeout: float):
        self.group_id = group_id
        self.estimated_duration = estimated_duration
        self.lambda_timeout = lambda_timeout
        super().__init__(
            f"Atomic Group '{group_id}' estimated duration ({estimated_duration:.1f}s) "
            f"exceeds safe limit ({lambda_timeout * 0.7:.1f}s = 70% of Lambda timeout {lambda_timeout}s). "
            f"Consider splitting the group or reducing node execution times."
        )


def validate_dag(
    nodes: Dict[str, Any], 
    outgoing_edges: Dict[str, List[Dict[str, Any]]]
) -> Tuple[bool, Optional[List[str]]]:
    """
    [Critical Fix #1] ê·¸ë˜í”„ê°€ DAG(Directed Acyclic Graph)ì¸ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
    
    Kahn's Algorithm (ìœ„ìƒ ì •ë ¬) ê¸°ë°˜ ì‚¬ì´í´ ê°ì§€.
    
    Args:
        nodes: ë…¸ë“œ ID -> ë…¸ë“œ ì •ì˜ ë§µ
        outgoing_edges: ë…¸ë“œ ID -> ë‚˜ê°€ëŠ” ì—£ì§€ ë¦¬ìŠ¤íŠ¸ ë§µ
        
    Returns:
        Tuple[is_dag, cycle_path]: DAGì´ë©´ (True, None), ì•„ë‹ˆë©´ (False, cycle_path)
    """
    if not nodes:
        return True, None
    
    # ì§„ì… ì°¨ìˆ˜(in-degree) ê³„ì‚°
    in_degree: Dict[str, int] = {nid: 0 for nid in nodes}
    
    for source_id, edges in outgoing_edges.items():
        for edge in edges:
            target = edge.get("target")
            if target and target in in_degree:
                in_degree[target] += 1
    
    # ì§„ì… ì°¨ìˆ˜ê°€ 0ì¸ ë…¸ë“œë“¤ë¡œ ì‹œì‘
    queue = [nid for nid, deg in in_degree.items() if deg == 0]
    visited_count = 0
    
    while queue:
        node_id = queue.pop(0)
        visited_count += 1
        
        for edge in outgoing_edges.get(node_id, []):
            target = edge.get("target")
            if target and target in in_degree:
                in_degree[target] -= 1
                if in_degree[target] == 0:
                    queue.append(target)
    
    # ëª¨ë“  ë…¸ë“œë¥¼ ë°©ë¬¸í•˜ì§€ ëª»í–ˆë‹¤ë©´ ì‚¬ì´í´ ì¡´ì¬
    if visited_count < len(nodes):
        # ì‚¬ì´í´ ê²½ë¡œ ì¶”ì  (DFSë¡œ ì‹¤ì œ ì‚¬ì´í´ ì°¾ê¸°)
        cycle_path = _find_cycle_path(nodes, outgoing_edges)
        return False, cycle_path
    
    return True, None


def _find_cycle_path(
    nodes: Dict[str, Any], 
    outgoing_edges: Dict[str, List[Dict[str, Any]]]
) -> List[str]:
    """DFSë¡œ ì‹¤ì œ ì‚¬ì´í´ ê²½ë¡œë¥¼ ì¶”ì í•©ë‹ˆë‹¤."""
    WHITE, GRAY, BLACK = 0, 1, 2
    color: Dict[str, int] = {nid: WHITE for nid in nodes}
    parent: Dict[str, Optional[str]] = {nid: None for nid in nodes}
    
    def dfs(node_id: str, path: List[str]) -> Optional[List[str]]:
        color[node_id] = GRAY
        path.append(node_id)
        
        for edge in outgoing_edges.get(node_id, []):
            target = edge.get("target")
            if target and target in color:
                if color[target] == GRAY:
                    # ì‚¬ì´í´ ë°œê²¬ - ê²½ë¡œ ì¶”ì¶œ
                    cycle_start = path.index(target)
                    return path[cycle_start:] + [target]
                elif color[target] == WHITE:
                    result = dfs(target, path)
                    if result:
                        return result
        
        color[node_id] = BLACK
        path.pop()
        return None
    
    for nid in nodes:
        if color[nid] == WHITE:
            result = dfs(nid, [])
            if result:
                return result
    
    return ["unknown_cycle"]  # í´ë°±


# ============================================================================
# [Critical Fix #3] Atomic Group íƒ€ì„ì•„ì›ƒ ê²€ì¦
# ============================================================================

# Lambda ì œí•œ (ì´ˆ)
LAMBDA_TIMEOUT_SECONDS = int(os.environ.get("LAMBDA_TIMEOUT_SECONDS", "900"))  # 15ë¶„

# ë…¸ë“œ íƒ€ì…ë³„ í‰ê·  ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
NODE_EXECUTION_ESTIMATES = {
    "llm_chat": 10.0,       # LLM í˜¸ì¶œ: í‰ê·  10ì´ˆ
    "aiModel": 10.0,        # AI ëª¨ë¸: í‰ê·  10ì´ˆ
    "api_call": 2.0,        # API í˜¸ì¶œ: í‰ê·  2ì´ˆ
    "db_query": 1.0,        # DB ì¿¼ë¦¬: í‰ê·  1ì´ˆ
    "operator": 0.5,        # Operator: í‰ê·  0.5ì´ˆ
    "safe_operator": 0.5,   # Safe Operator: í‰ê·  0.5ì´ˆ
    "loop": 5.0,            # Loop: í‰ê·  5ì´ˆ (ë‚´ë¶€ ë…¸ë“œ ë³„ë„ ê³„ì‚°)
    "for_each": 8.0,        # For Each: í‰ê·  8ì´ˆ (ë³‘ë ¬ ì²˜ë¦¬)
    "parallel_group": 5.0,  # Parallel Group: í‰ê·  5ì´ˆ
    "aggregator": 0.3,      # Aggregator: í‰ê·  0.3ì´ˆ
    "route_condition": 0.2, # Route Condition: í‰ê·  0.2ì´ˆ
    "default": 1.0          # ê¸°íƒ€: í‰ê·  1ì´ˆ
}


def estimate_node_duration(node: Dict[str, Any]) -> float:
    """
    ë…¸ë“œì˜ ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ ì¶”ì •
    
    Args:
        node: ë…¸ë“œ ì„¤ì •
    
    Returns:
        ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
    """
    node_type = node.get("type", "default")
    config = node.get("config", {})
    
    # ê¸°ë³¸ ì‹¤í–‰ ì‹œê°„
    base_duration = NODE_EXECUTION_ESTIMATES.get(node_type, NODE_EXECUTION_ESTIMATES["default"])
    
    # íƒ€ì…ë³„ ë³´ì •
    if node_type == "loop":
        # Loop: max_iterations ê³ ë ¤
        max_iterations = config.get("max_iterations", 5)
        sub_nodes = config.get("nodes", [])
        sub_duration = sum(estimate_node_duration(n) for n in sub_nodes)
        return max_iterations * sub_duration
    
    elif node_type == "for_each":
        # For Each: max_iterations ê³ ë ¤ (ë³‘ë ¬ ì²˜ë¦¬)
        max_iterations = config.get("max_iterations", 20)
        sub_workflow = config.get("sub_workflow", {})
        sub_nodes = sub_workflow.get("nodes", [])
        sub_duration = max(
            (estimate_node_duration(n) for n in sub_nodes),
            default=1.0
        )
        # ë³‘ë ¬ ì²˜ë¦¬ì´ë¯€ë¡œ ê°€ì¥ ê¸´ ë…¸ë“œ ì‹œê°„ë§Œ ê³ ë ¤
        return sub_duration
    
    elif node_type in ("llm_chat", "aiModel"):
        # LLM: max_tokens ê¸°ë°˜ ë³´ì •
        max_tokens = config.get("max_tokens", 256)
        # í† í°ë‹¹ ~0.01ì´ˆ ì¶”ì • (GPT-4 ê¸°ì¤€)
        token_penalty = max_tokens * 0.01
        
        # Extended Thinking í™œì„±í™” ì‹œ ì¶”ê°€ ì‹œê°„
        if config.get("enable_thinking", False):
            thinking_budget = config.get("thinking_budget_tokens", 4096)
            token_penalty += thinking_budget * 0.01
        
        return base_duration + token_penalty
    
    elif node_type == "api_call":
        # API Call: timeout ì„¤ì • ê³ ë ¤
        timeout = config.get("timeout", 10)
        return min(timeout, base_duration)
    
    return base_duration


def analyze_loop_structures(nodes: List[Dict[str, Any]], node_to_seg_map: Dict[str, int] = None) -> Dict[str, Any]:
    """
    Analyze loop structures to estimate weighted execution count.
    
    ğŸ›¡ï¸ [Dynamic Loop Limit] Segment-based iteration counting
    - for_each: Adds 2 base segments (parallel_group + aggregator)
      * PLUS runtime sub_workflow execution: estimated_sub_segments Ã— max_iterations
      * PLUS nested loop weights from sub_workflow
      * Critical: sub_workflow is PARTITIONED at runtime, creating additional segments
    - Sequential loop: Adds (internal_segment_count Ã— (max_iterations - 1))
      * First iteration included in base segment count
      * Additional iterations = (max_iter - 1) Ã— segment_count
    - Formula: Î£(2 + sub_segments Ã— max_iter + nested_weights) for for_each + Î£(seg_count Ã— (max_iter - 1)) for loops
    
    Args:
        nodes: List of workflow nodes
        node_to_seg_map: Mapping of node_id â†’ segment_id (optional)
        
    Returns:
        {
            "loop_nodes": [...],
            "total_loop_weighted_segments": int,  # Weighted segment count
            "loop_count": int
        }
    """
    loop_nodes = []
    total_weighted = 0
    
    for node in nodes:
        node_type = node.get("type", "")
        config = node.get("config", {})
        
        if node_type == "loop":
            max_iter = config.get("max_iterations", 5)
            sub_nodes = config.get("nodes", [])
            
            # Calculate how many segments this loop's internal nodes span
            if node_to_seg_map:
                sub_node_ids = [n.get("id") for n in sub_nodes if n.get("id")]
                sub_segments = set(node_to_seg_map.get(nid) for nid in sub_node_ids if node_to_seg_map.get(nid) is not None)
                segment_count = len(sub_segments) if sub_segments else len(sub_nodes)
            else:
                # Fallback: estimate based on node count
                segment_count = max(1, len(sub_nodes))
            
            # Recursive analysis for nested loops
            sub_analysis = analyze_loop_structures(sub_nodes, node_to_seg_map)
            
            loop_nodes.append({
                "node_id": node.get("id"),
                "type": "loop",
                "max_iterations": max_iter,
                "sub_node_count": len(sub_nodes),
                "sub_segment_count": segment_count,
                "nested_loops": sub_analysis["loop_nodes"]
            })
            
            # âœ… [FIX] Segment-based counting: segment_count Ã— (max_iter - 1)
            # Subtract 1 because total_segments already includes the first execution
            total_weighted += segment_count * (max_iter - 1) + sub_analysis["total_loop_weighted_segments"]
            
        elif node_type == "parallel_group":
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # [v3.18 Fix] Inline Parallel Group â†’ branches ì¬ê·€ íƒìƒ‰
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # for_each / loop ë…¸ë“œê°€ parallel_group.branches[].nodes[] ì•ˆì—
            # ì¤‘ì²©ëœ ê²½ìš°, ìƒìœ„ ë ˆë²¨ ìŠ¤ìº”ì—ì„œëŠ” ì™„ì „íˆ ëˆ„ë½ëœë‹¤.
            # â†’ branchesë¥¼ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰í•´ weighted í•©ì‚°.
            #
            # [v3.18.1 Fix] branches ìœ„ì¹˜ ì´ì¤‘ íƒìƒ‰:
            #   - ì¼ë¶€ ì›Œí¬í”Œë¡œìš°: branchesê°€ ë…¸ë“œ ìµœìƒìœ„ì— ìœ„ì¹˜ (STRESS ìŠ¤íƒ€ì¼)
            #   - ì¼ë¶€ ì›Œí¬í”Œë¡œìš°: branchesê°€ config í•˜ìœ„ì— ìœ„ì¹˜ (MAP_AGGREGATOR ìŠ¤íƒ€ì¼)
            #   â†’ ë‘˜ ë‹¤ í™•ì¸í•˜ì§€ ì•Šìœ¼ë©´ config.branches ì•ˆì˜ for_eachê°€ ëˆ„ë½ë¨
            branches = node.get("branches") or node.get("config", {}).get("branches") or []
            for branch in branches:
                branch_nodes = branch.get("nodes", [])
                if branch_nodes:
                    branch_analysis = analyze_loop_structures(branch_nodes, node_to_seg_map)
                    total_weighted += branch_analysis["total_loop_weighted_segments"]
                    loop_nodes.extend(branch_analysis["loop_nodes"])
                    logger.debug(
                        f"[Loop Analysis] parallel_group '{node.get('id')}' "
                        f"branch '{branch.get('id', '?')}': "
                        f"nested_weight={branch_analysis['total_loop_weighted_segments']}, "
                        f"nested_loops={branch_analysis['loop_count']}"
                    )

        elif node_type == "for_each":
            max_iter = config.get("max_iterations", 20)
            sub_workflow = config.get("sub_workflow", {})
            sub_nodes = sub_workflow.get("nodes", [])
            
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # [v3.17] for_each Lambda-Internal Complexity Budget
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # for_each_runnerëŠ” Lambda ë‚´ë¶€ ThreadPoolExecutorë¡œ ì‹¤í–‰ë¨.
            # SFN loop_counter ì¦ê°€: ìµœëŒ€ 2íšŒ (parallel_group + aggregator).
            # ê·¸ëŸ¬ë‚˜ Lambda ë‚´ë¶€ì—ì„œ sub_node_count Ã— max_iterations íšŒ ì‹¤í–‰ ë°œìƒ.
            # max_loop_iterationsëŠ” SFN ì „ì´ ì¹´ìš´í„°ì´ì ë³µì¡ë„ ì˜ˆì‚°ì´ë¯€ë¡œ,
            # Lambda ë‚´ë¶€ ì‹¤í–‰ëŸ‰ì„ complexity_budgetìœ¼ë¡œ ë°˜ì˜:
            #   self_weight = sub_node_count Ã— max_iterations
            sub_node_count = len(sub_nodes)
            for_each_self_weight = sub_node_count * max_iter
            
            # Recursive analysis for nested loops (sequential loop inside for_each ëŒ€ë¹„)
            sub_analysis = analyze_loop_structures(sub_nodes, node_to_seg_map)
            
            loop_nodes.append({
                "node_id": node.get("id"),
                "type": "for_each",
                "max_iterations": max_iter,
                "sub_node_count": sub_node_count,
                "self_weight": for_each_self_weight,  # Lambda-internal complexity budget
                "nested_loops": sub_analysis["loop_nodes"]
            })
            
            # for_each ë³µì¡ë„ ì˜ˆì‚° + ì¤‘ì²© sequential loop ê°€ì¤‘ì¹˜
            for_each_weighted = for_each_self_weight + sub_analysis["total_loop_weighted_segments"]
            total_weighted += for_each_weighted
            
            logger.debug(
                f"[Loop Analysis] for_each '{node.get('id')}' (v3.17 complexity-budget): "
                f"max_iter={max_iter}, sub_nodes={sub_node_count}, "
                f"self_weight={for_each_self_weight} (Lambda-internal budget), "
                f"nested_weight={sub_analysis['total_loop_weighted_segments']}"
            )
    
    return {
        "loop_nodes": loop_nodes,
        "total_loop_weighted_segments": total_weighted,
        "loop_count": len(loop_nodes)
    }


def validate_atomic_group_timeout(
    group_id: str,
    nodes: List[Dict[str, Any]],
    lambda_timeout: float = LAMBDA_TIMEOUT_SECONDS
) -> None:
    """
    Atomic Groupì˜ ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ ê²€ì¦
    
    Args:
        group_id: ê·¸ë£¹ ID
        nodes: ê·¸ë£¹ ë‚´ ë…¸ë“œ ëª©ë¡
        lambda_timeout: Lambda íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    
    Raises:
        AtomicGroupTimeoutError: ì˜ˆìƒ ì‹œê°„ì´ ì•ˆì „ ì œí•œ(70%)ì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš°
    """
    # ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ í•©ì‚°
    total_duration = sum(estimate_node_duration(node) for node in nodes)
    
    # ì•ˆì „ ì œí•œ: Lambda íƒ€ì„ì•„ì›ƒì˜ 70%
    safe_limit = lambda_timeout * 0.7
    
    logger.info(
        f"[ATOMIC_GROUP_VALIDATION] {group_id}: "
        f"{len(nodes)} nodes, estimated {total_duration:.1f}s "
        f"(limit: {safe_limit:.1f}s)"
    )
    
    if total_duration > safe_limit:
        logger.warning(
            f"[ATOMIC_GROUP_TIMEOUT_RISK] {group_id} exceeds safe limit: "
            f"{total_duration:.1f}s > {safe_limit:.1f}s"
        )
        raise AtomicGroupTimeoutError(group_id, total_duration, lambda_timeout)


def extract_atomic_groups(workflow_config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    ì›Œí¬í”Œë¡œìš°ì—ì„œ Atomic Group ì¶”ì¶œ
    
    ëª…ì‹œì  ê·¸ë£¹:
    - type="group", atomic=true
    
    ì•”ë¬µì  ê·¸ë£¹:
    - DB íŠ¸ëœì­ì…˜ íŒ¨í„´ (BEGIN ... COMMIT)
    - HTTP ì„¸ì…˜ ìœ ì§€ íŒ¨í„´
    
    Args:
        workflow_config: ì›Œí¬í”Œë¡œìš° ì„¤ì •
    
    Returns:
        Atomic Group ëª©ë¡
    """
    nodes = workflow_config.get("nodes", [])
    edges = workflow_config.get("edges", [])
    
    atomic_groups = []
    
    # 1. ëª…ì‹œì  ê·¸ë£¹ ì¶”ì¶œ
    for node in nodes:
        if node.get("type") == "group" and node.get("data", {}).get("atomic"):
            group_nodes = node.get("data", {}).get("nodes", [])
            atomic_groups.append({
                "group_id": node["id"],
                "nodes": [n for n in nodes if n["id"] in group_nodes],
                "is_explicit": True
            })
    
    # 2. ì•”ë¬µì  ê·¸ë£¹ ê°ì§€ (DB íŠ¸ëœì­ì…˜ íŒ¨í„´)
    implicit_groups = _detect_transaction_patterns(nodes, edges)
    atomic_groups.extend(implicit_groups)
    
    logger.info(
        f"[ATOMIC_GROUPS] Extracted {len(atomic_groups)} groups "
        f"({sum(1 for g in atomic_groups if g['is_explicit'])} explicit, "
        f"{sum(1 for g in atomic_groups if not g['is_explicit'])} implicit)"
    )
    
    return atomic_groups


def _detect_transaction_patterns(
    nodes: List[Dict[str, Any]],
    edges: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    DB íŠ¸ëœì­ì…˜ íŒ¨í„´ ìë™ ê°ì§€
    
    íŒ¨í„´:
    1. BEGIN TRANSACTION â†’ INSERT/UPDATE/DELETE â†’ COMMIT
    2. START SESSION â†’ API CALL â†’ END SESSION
    
    Args:
        nodes: ë…¸ë“œ ëª©ë¡
        edges: ì—£ì§€ ëª©ë¡
    
    Returns:
        ì•”ë¬µì  Atomic Group ëª©ë¡
    """
    implicit_groups = []
    node_map = {n["id"]: n for n in nodes}
    
    # ì—£ì§€ ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    adjacency = {n["id"]: [] for n in nodes}
    for edge in edges:
        source = edge.get("source")
        target = edge.get("target")
        if source and target:
            adjacency[source].append(target)
    
    # BEGIN â†’ ... â†’ COMMIT íŒ¨í„´ ê°ì§€
    for node in nodes:
        if node.get("type") == "db_query":
            query = node.get("config", {}).get("query", "").strip().upper()
            
            # BEGIN TRANSACTION ê°ì§€
            if "BEGIN" in query or "START TRANSACTION" in query:
                # ì—°ê²°ëœ ë…¸ë“œ ì¶”ì 
                group_nodes = []
                visited = set()
                queue = [node["id"]]
                
                while queue:
                    current_id = queue.pop(0)
                    if current_id in visited:
                        continue
                    visited.add(current_id)
                    
                    current_node = node_map.get(current_id)
                    if not current_node:
                        continue
                    
                    group_nodes.append(current_node)
                    
                    # COMMIT ë°œê²¬ ì‹œ ì¢…ë£Œ
                    if current_node.get("type") == "db_query":
                        current_query = current_node.get("config", {}).get("query", "").upper()
                        if "COMMIT" in current_query:
                            break
                    
                    # ë‹¤ìŒ ë…¸ë“œ ì¶”ê°€
                    for next_id in adjacency.get(current_id, []):
                        if next_id not in visited:
                            queue.append(next_id)
                
                # ê·¸ë£¹ ë“±ë¡ (COMMIT ë°œê²¬ ì‹œë§Œ)
                if len(group_nodes) > 1:
                    last_node = group_nodes[-1]
                    last_query = last_node.get("config", {}).get("query", "").upper()
                    if "COMMIT" in last_query:
                        implicit_groups.append({
                            "group_id": f"tx_{node['id']}",
                            "nodes": group_nodes,
                            "is_explicit": False,
                            "pattern": "db_transaction"
                        })
                        logger.info(
                            f"[TRANSACTION_PATTERN] Detected DB transaction group: "
                            f"{node['id']} â†’ {last_node['id']} ({len(group_nodes)} nodes)"
                        )
    
    return implicit_groups


def partition_workflow_advanced(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    ê³ ê¸‰ ì›Œí¬í”Œë¡œìš° ë¶„í• : HITP ì—£ì§€ì™€ LLM ë…¸ë“œ ê¸°ë°˜ìœ¼ë¡œ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    [v2.0 Production Hardening]
    - DAG(Directed Acyclic Graph) ì‚¬ì „ ê²€ì¦
    - ì¬ê·€ ê¹Šì´ ì œí•œ (MAX_PARTITION_DEPTH)
    - í•©ë¥˜ì (Convergence Node) ê°•ì œ ë¶„í• 
    - ë¸Œëœì¹˜ ì¢…ë£Œ ê²€ì¦
    - Thread-safe ID ìƒì„±
    
    ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜:
    - ë³‘í•© ì§€ì (Merge Point) ê°ì§€ ë° ì²˜ë¦¬
    - ë³‘ë ¬ ê·¸ë£¹(Parallel Group) ìƒì„± ë° ì¬ê·€ì  íŒŒí‹°ì…”ë‹
    - Convergence Node ì°¾ê¸° ë° ë¸Œëœì¹˜ ì œí•œ
    - ì¬ê·€ì  Node-to-Segment ë§¤í•‘
    
    Raises:
        CycleDetectedError: ê·¸ë˜í”„ì— ì‚¬ì´í´ì´ ìˆëŠ” ê²½ìš°
        PartitionDepthExceededError: ì¬ê·€ ê¹Šì´ ì´ˆê³¼ ì‹œ
        ValueError: ë…¸ë“œ ìˆ˜ ì œí•œ ì´ˆê³¼ ì‹œ
    """
    # ğŸ›¡ï¸ [v3.8] None defense: filter out None elements from nodes list
    raw_nodes = config.get("nodes", [])
    nodes = {n["id"]: n for n in raw_nodes if n is not None and isinstance(n, dict) and "id" in n}
    edges = config.get("edges", []) if config.get("edges") else []
    
    # [Critical Fix] ë…¸ë“œ ìˆ˜ ì œí•œ ê²€ì¦
    if len(nodes) > MAX_NODES_LIMIT:
        raise ValueError(
            f"Workflow has {len(nodes)} nodes, exceeding maximum limit of {MAX_NODES_LIMIT}. "
            f"Consider splitting into subgraphs."
        )
    
    # ğŸš¨ [Performance Warning] 100ê°œ ë…¸ë“œ ì´ˆê³¼ ì‹œ ê²½ê³ 
    # Lambda ì‹¤í–‰ ì‹œê°„(15ë¶„)ë³´ë‹¤ latencyê°€ ë¨¼ì € ë¬¸ì œë  ìˆ˜ ìˆìŒ
    # ë³µì¡í•œ ê·¸ë˜í”„ì¼ ê²½ìš° ìœ„ìƒ ì •ë ¬/ì‚¬ì´í´ ê°ì§€ ë‹¨ê³„ì—ì„œ ì§€ì—° ë°œìƒ
    performance_warnings = []
    if len(nodes) > PERFORMANCE_WARNING_NODE_COUNT:
        warning_msg = (
            f"âš ï¸ Workflow has {len(nodes)} nodes (threshold: {PERFORMANCE_WARNING_NODE_COUNT}). "
            f"Complex graphs may experience increased latency during topological sort and cycle detection. "
            f"Consider splitting into smaller subworkflows for better performance."
        )
        performance_warnings.append({
            "type": "high_node_count",
            "severity": "warning",
            "node_count": len(nodes),
            "threshold": PERFORMANCE_WARNING_NODE_COUNT,
            "message": warning_msg
        })
        logger.warning(warning_msg)
    
    # [Performance Optimization] ì—£ì§€ ë§µ ìƒì„± (Pre-indexed)
    # í–¥í›„ ì›Œí¬í”Œë¡œìš° ì €ì¥ ì‹œì ì— ë©”íƒ€ë°ì´í„°ë¡œ ì¶”ì¶œí•˜ì—¬ ì¬ì‚¬ìš© ê°€ëŠ¥
    incoming_edges: Dict[str, List[Dict[str, Any]]] = {}
    outgoing_edges: Dict[str, List[Dict[str, Any]]] = {}
    
    for edge in edges:
        source = edge.get("source")
        target = edge.get("target") 
        if source:
            outgoing_edges.setdefault(source, []).append(edge)
        if target:
            incoming_edges.setdefault(target, []).append(edge)
    
    # [Critical Fix #1] DAG ê²€ì¦ - ì‚¬ì´í´ ê°ì§€
    is_dag, cycle_path = validate_dag(nodes, outgoing_edges)
    if not is_dag:
        raise CycleDetectedError(cycle_path or ["unknown"])
    
    # [Critical Fix #3] Atomic Group íƒ€ì„ì•„ì›ƒ ê²€ì¦
    atomic_groups = extract_atomic_groups(config)
    for group in atomic_groups:
        try:
            validate_atomic_group_timeout(
                group_id=group["group_id"],
                nodes=group["nodes"],
                lambda_timeout=LAMBDA_TIMEOUT_SECONDS
            )
        except AtomicGroupTimeoutError as e:
            # ê²½ê³ ë§Œ ê¸°ë¡í•˜ê³  ê³„ì† ì§„í–‰ (ì‚¬ìš©ìê°€ ìœ„í—˜ ê°ìˆ˜ ê°€ëŠ¥)
            logger.warning(
                f"[ATOMIC_GROUP_WARNING] {e.group_id}: {e.estimated_duration:.1f}s "
                f"exceeds safe limit {e.lambda_timeout * 0.7:.1f}s. Consider optimizing."
            )
            # ì—„ê²© ëª¨ë“œì—ì„œëŠ” ì˜ˆì™¸ ë°œìƒ
            if os.environ.get("STRICT_ATOMIC_GROUP_VALIDATION", "false").lower() == "true":
                raise
    
    # [Critical Fix #2] í•©ë¥˜ì  ì§‘í•© - ì´ ë…¸ë“œë“¤ì€ ë°˜ë“œì‹œ ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ì ì´ ë¨
    # find_convergence_nodeë¡œ ì°¾ì€ ëª¨ë“  í•©ë¥˜ì ì„ ë¯¸ë¦¬ ìˆ˜ì§‘
    forced_segment_starts: Set[str] = set()
    
    # [Performance Optimization] Thread-safe ID ìƒì„±ê¸°
    class ThreadSafeIdGenerator:
        def __init__(self): 
            self._val = -1
            self._lock = threading.Lock()
        
        def next(self) -> int:
            with self._lock:
                self._val += 1
                return self._val
        
        @property
        def current(self) -> int:
            return self._val
    
    seg_id_gen = ThreadSafeIdGenerator()
    stats = {"llm": 0, "hitp": 0, "parallel_groups": 0, "branches": 0}
    
    # --- Helper: í•©ë¥˜ ì§€ì (Convergence Node) ì°¾ê¸° ---
    def find_convergence_node(start_nodes: List[str]) -> Optional[str]:
        """
        ë¸Œëœì¹˜ë“¤ì´ ê³µí†µìœ¼ë¡œ ë„ë‹¬í•˜ëŠ” ì²« ë²ˆì§¸ Merge Pointë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        in-degree > 1ì¸ ë…¸ë“œë¥¼ í›„ë³´ë¡œ ë´…ë‹ˆë‹¤.
        
        [Critical Fix #2] ì°¾ì€ í•©ë¥˜ì ì€ forced_segment_startsì— ë“±ë¡ë˜ì–´
        ë°˜ë“œì‹œ ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ì˜ ì‹œì‘ì ì´ ë©ë‹ˆë‹¤.
        """
        queue = list(start_nodes)
        seen = set(queue)
        
        while queue:
            node_id = queue.pop(0)
            # Merge Point í›„ë³´ í™•ì¸
            if len(incoming_edges.get(node_id, [])) > 1:
                if node_id not in start_nodes:
                    # [Critical Fix #2] í•©ë¥˜ì ì€ ë°˜ë“œì‹œ ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ì 
                    forced_segment_starts.add(node_id)
                    logger.debug(f"Convergence node registered as forced segment start: {node_id}")
                    return node_id
            
            for out_edge in outgoing_edges.get(node_id, []):
                target = out_edge.get("target")
                if target and target not in seen:
                    seen.add(target)
                    queue.append(target)
        return None
    
    # --- [Critical Fix] ìœ„ìƒ ì •ë ¬ í—¬í¼ ---
    def _topological_sort_nodes(nodes_map: Dict[str, Any], edges_list: List[Dict]) -> List[Dict[str, Any]]:
        """
        ì„¸ê·¸ë¨¼íŠ¸ ë‚´ ë…¸ë“œë“¤ì„ ìœ„ìƒ ì •ë ¬í•˜ì—¬ ì‹¤í–‰ ìˆœì„œëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        DynamicWorkflowBuilderëŠ” nodes[0]ì„ entry pointë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ,
        ì²« ë²ˆì§¸ ë…¸ë“œê°€ ì‹¤ì œ ì‹œì‘ ë…¸ë“œê°€ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
        
        Args:
            nodes_map: {node_id: node_config} ë§¤í•‘
            edges_list: ì„¸ê·¸ë¨¼íŠ¸ ë‚´ë¶€ ì—£ì§€ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ìœ„ìƒ ì •ë ¬ëœ ë…¸ë“œ ì„¤ì • ë¦¬ìŠ¤íŠ¸
        """
        if len(nodes_map) <= 1:
            return list(nodes_map.values())
        
        # ì„¸ê·¸ë¨¼íŠ¸ ë‚´ ë…¸ë“œ ID ì§‘í•©
        node_ids = set(nodes_map.keys())
        
        # ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ ë° ì§„ì… ì°¨ìˆ˜(in-degree) ê³„ì‚°
        in_degree = {nid: 0 for nid in node_ids}
        adj = {nid: [] for nid in node_ids}
        
        for edge in edges_list:
            src = edge.get("source")
            tgt = edge.get("target")
            if src in node_ids and tgt in node_ids:
                adj[src].append(tgt)
                in_degree[tgt] += 1
        
        # Kahn's Algorithm: ì§„ì… ì°¨ìˆ˜ê°€ 0ì¸ ë…¸ë“œë¶€í„° ì‹œì‘
        # heapqë¡œ ìµœì†Ÿê°’ ì¶”ì¶œ O(log n): ë§¤ ë°˜ë³µ queue.sort() O(n log n) ì œê±° â†’ ì „ì²´ O(n log n)
        # ê²°ì •ë¡ ì  ìˆœì„œ ë³´ì¥: heapqëŠ” í•­ìƒ ì•ŒíŒŒë²³ ìµœì†Ÿê°’ ë…¸ë“œ IDë¥¼ ë°˜í™˜
        queue = sorted([nid for nid in node_ids if in_degree[nid] == 0])
        heapq.heapify(queue)
        sorted_ids = []

        while queue:
            node_id = heapq.heappop(queue)
            sorted_ids.append(node_id)

            for neighbor in adj[node_id]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    heapq.heappush(queue, neighbor)
        
        # ì •ë ¬ë˜ì§€ ì•Šì€ ë…¸ë“œê°€ ìˆìœ¼ë©´ (ì‚¬ì´í´ ë˜ëŠ” ì—°ê²° ì•ˆë¨) ì›ë˜ ìˆœì„œë¡œ ì¶”ê°€
        if len(sorted_ids) < len(node_ids):
            remaining = [nid for nid in nodes_map.keys() if nid not in sorted_ids]
            logger.warning(f"Some nodes not topologically sorted, appending in original order: {remaining}")
            sorted_ids.extend(remaining)
        
        result = [nodes_map[nid] for nid in sorted_ids]
        logger.debug(f"Topological sort result: {[n.get('id') for n in result]}")
        return result
    
    # --- Segment ìƒì„± í—¬í¼ ---
    def create_segment(nodes_map, edges_list, s_type="normal", override_id=None, config=None):
        # ğŸ›¡ï¸ [v2.6 P0 Fix] 'code' íƒ€ì… ê°•ì œ ì •ì • - ValueError ë°©ì§€
        # ìƒìœ„ ë ˆì´ì–´(í”„ë¡ íŠ¸ì—”ë“œ, DB ë“±)ì—ì„œ ì˜ëª»ëœ íƒ€ì…ì´ ë“¤ì–´ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œ êµì •
        for node_id, node in nodes_map.items():
            if isinstance(node, dict) and node.get("type") == "code":
                logger.warning(
                    f"ğŸ›¡ï¸ [Kernel Defense] Fixing 'code' type to 'operator' for node {node_id} "
                    f"in partition_service.create_segment"
                )
                node["type"] = "operator"
        
        # [P0 Refactoring] Inter-segment edges ìˆ˜ì§‘
        outgoing_edges = []
        if config:
            all_edges = config.get("edges", [])
            for edge in all_edges:
                source = edge.get("source")
                target = edge.get("target")
                
                # Intra-segment edge (ì–‘ìª½ ë…¸ë“œê°€ ëª¨ë‘ ì´ ì„¸ê·¸ë¨¼íŠ¸ì— ìˆìŒ)
                if source in nodes_map and target in nodes_map:
                    if edge not in edges_list:  # ì¤‘ë³µ ë°©ì§€
                        edges_list.append(edge)
                
                # Inter-segment edge (sourceë§Œ ì´ ì„¸ê·¸ë¨¼íŠ¸ì— ìˆê³  targetì€ ë‹¤ë¥¸ ì„¸ê·¸ë¨¼íŠ¸)
                elif source in nodes_map and target not in nodes_map:
                    edge_data = edge.get("data", {})
                    outgoing_edges.append({
                        "source_node": source,
                        "target_node": target,
                        "edge_type": edge.get("type", "normal"),
                        # âŒ REMOVED: condition, router_func, mapping (ë¼ìš°íŒ… ì£¼ê¶Œ ì¼ì›í™”)
                        # ì´ìœ : ëª¨ë“  ë¼ìš°íŒ… ê²°ì •ì€ ë…¸ë“œê°€ ìˆ˜í–‰ (route_condition, __next_node)
                        "is_loop_exit": edge_data.get("isLoopExit", False),
                        "is_back_edge": edge_data.get("isBackEdge", False),
                        "metadata": {
                            "label": edge.get("label"),
                            "style": edge.get("style"),
                            "animated": edge.get("animated"),
                            "edgeType": edge_data.get("edgeType"),
                            "loopType": edge_data.get("loopType")
                        }
                    })
        
        # [Critical Fix] ë…¸ë“œ ìˆœì„œë¥¼ ìœ„ìƒ ì •ë ¬í•˜ì—¬ ì²« ë²ˆì§¸ ë…¸ë“œê°€ ì‹¤ì œ ì‹œì‘ ë…¸ë“œê°€ ë˜ë„ë¡ ë³´ì¥
        # DynamicWorkflowBuilderëŠ” nodes[0]ì„ entry pointë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ ìˆœì„œê°€ ì¤‘ìš”í•¨
        sorted_nodes = _topological_sort_nodes(nodes_map, edges_list)
        
        final_type = s_type
        if s_type == "normal":
            if any(n.get("hitp") in [True, "true"] for n in nodes_map.values()):
                final_type = "hitp"
        
        if final_type == "llm": 
            stats["llm"] += 1
        elif final_type == "hitp": 
            stats["hitp"] += 1
        
        logger.debug(f"[Segment Created] ID={seg_id_gen.current}, Type={final_type}, "
                    f"Nodes={len(sorted_nodes)}, IntraEdges={len(edges_list)}, "
                    f"OutgoingEdges={len(outgoing_edges)}")
            
        return {
            "id": override_id if override_id is not None else seg_id_gen.next(),
            "nodes": sorted_nodes,  # [Critical Fix] ìœ„ìƒ ì •ë ¬ëœ ë…¸ë“œ ì‚¬ìš©
            "edges": list(edges_list),
            "outgoing_edges": outgoing_edges,  # [P0 Refactoring] Inter-segment edges
            "type": final_type,
            "node_ids": [n["id"] for n in sorted_nodes]  # ì •ë ¬ëœ ìˆœì„œ ë°˜ì˜
        }
    
    # --- ì¬ê·€ì  íŒŒí‹°ì…”ë‹ ë¡œì§ ---
    visited_nodes: Set[str] = set()
    
    def run_partitioning(
        start_node_ids: List[str], 
        stop_at_nodes: Set[str] = None, 
        config=None,
        depth: int = 0  # [Critical Fix #1] ì¬ê·€ ê¹Šì´ ì¶”ì 
    ) -> List[Dict[str, Any]]:
        """
        ì¬ê·€ì  íŒŒí‹°ì…”ë‹ ë¡œì§.
        
        [Critical Fix #1] depth íŒŒë¼ë¯¸í„°ë¡œ ì¬ê·€ ê¹Šì´ ì œí•œ
        [Critical Fix #2] forced_segment_startsë¡œ í•©ë¥˜ì  ê°•ì œ ë¶„í• 
        """
        # [Critical Fix #1] ì¬ê·€ ê¹Šì´ ì œí•œ ê²€ì‚¬
        if depth > MAX_PARTITION_DEPTH:
            raise PartitionDepthExceededError(depth, MAX_PARTITION_DEPTH)
        
        local_segments = []
        local_current_nodes = {}
        local_current_edges = []
        queue = list(start_node_ids)
        
        # [Critical Fix #1] ë¬´í•œ ë£¨í”„ ë°©ì§€ìš© ë°˜ë³µ ì¹´ìš´í„°
        max_iterations = len(nodes) * 2  # ì•ˆì „ ë§ˆì§„
        iteration_count = 0
        
        def flush_local(seg_type="normal"):
            nonlocal local_current_nodes, local_current_edges
            if local_current_nodes or local_current_edges:
                seg = create_segment(local_current_nodes, local_current_edges, seg_type, config=config)
                local_segments.append(seg)
                local_current_nodes = {}
                local_current_edges = []
        
        while queue:
            # [Critical Fix #1] ë¬´í•œ ë£¨í”„ ë°©ì§€
            iteration_count += 1
            if iteration_count > max_iterations:
                logger.error(
                    f"Partition iteration limit exceeded ({max_iterations}). "
                    f"Possible infinite loop. Queue: {queue[:5]}..."
                )
                raise PartitionDepthExceededError(iteration_count, max_iterations)
            
            node_id = queue.pop(0)
            
            # Stop Condition
            if node_id in visited_nodes: 
                continue
            if stop_at_nodes and node_id in stop_at_nodes: 
                continue
            
            # [Safety] ë…¸ë“œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if node_id not in nodes:
                logger.warning(f"Node '{node_id}' referenced but not found in nodes map. Skipping.")
                continue
            
            node = nodes[node_id]
            
            # íŠ¸ë¦¬ê±° ì¡°ê±´ ê³„ì‚°
            in_edges = incoming_edges.get(node_id, [])
            non_hitp_in = [e for e in in_edges if e.get("type") not in HITP_EDGE_TYPES]
            
            is_hitp_start = any(e.get("type") in HITP_EDGE_TYPES for e in in_edges)
            is_llm = node.get("type") in LLM_NODE_TYPES
            is_merge = len(non_hitp_in) > 1
            is_branch = len(outgoing_edges.get(node_id, [])) > 1
            
            # ğŸ›¡ï¸ [v3.8] ì¸ë¼ì¸ parallel_group ë…¸ë“œ ê°ì§€
            # ë…¸ë“œ ìì²´ê°€ type="parallel_group"ì´ê³  branchesë¥¼ í¬í•¨í•˜ëŠ” ê²½ìš°
            is_inline_parallel = (
                node.get("type") == "parallel_group" and 
                isinstance(node.get("branches"), list) and
                len(node.get("branches", [])) > 0
            )
            
            # [Critical Fix #2] í•©ë¥˜ì ì€ ë°˜ë“œì‹œ ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘
            is_forced_start = node_id in forced_segment_starts
            
            # ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  íŠ¸ë¦¬ê±° (is_forced_start ì¶”ê°€)
            if (is_hitp_start or is_llm or is_merge or is_branch or is_forced_start or is_inline_parallel) and local_current_nodes:
                if node_id not in local_current_nodes:
                    flush_local("normal")
            
            # ğŸ›¡ï¸ [v3.8] ì¸ë¼ì¸ parallel_group ë…¸ë“œ ì²˜ë¦¬ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
            if is_inline_parallel:
                flush_local("normal")  # í˜„ì¬ê¹Œì§€ ì €ì¥
                visited_nodes.add(node_id)
                
                # ì¸ë¼ì¸ branchesë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                inline_branches = node.get("branches", [])
                branches_data = []
                
                for i, branch in enumerate(inline_branches):
                    if branch is None:
                        logger.warning(f"ğŸ›¡ï¸ [Self-Healing] Skipping None branch in inline parallel_group {node_id}")
                        continue
                    
                    branch_id = branch.get("id", f"B{i}")
                    branch_nodes = branch.get("nodes", [])
                    branch_edges = branch.get("edges", [])
                    
                    # ë¸Œëœì¹˜ ë‚´ë¶€ë¥¼ ì„œë¸Œ íŒŒí‹°ì…˜ìœ¼ë¡œ ì²˜ë¦¬
                    branch_partition = []
                    if branch_nodes:
                        # ë¸Œëœì¹˜ ë‚´ë¶€ ë…¸ë“œë“¤ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
                        branch_nodes_map = {}
                        for bn in branch_nodes:
                            if bn is not None and isinstance(bn, dict) and "id" in bn:
                                branch_nodes_map[bn["id"]] = bn
                        
                        if branch_nodes_map:
                            branch_seg = create_segment(
                                branch_nodes_map, 
                                branch_edges, 
                                "normal",
                                config={"nodes": branch_nodes, "edges": branch_edges}
                            )
                            branch_partition.append(branch_seg)
                    
                    branch_data = {
                        "branch_id": branch_id,
                        "partition_map": branch_partition,
                        "has_end": False,
                        "target_node": branch_nodes[0].get("id") if branch_nodes else None
                    }
                    branches_data.append(branch_data)
                    stats["branches"] += 1
                
                # Parallel Group ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
                if branches_data:
                    stats["parallel_groups"] += 1
                    p_seg_id = seg_id_gen.next()
                    parallel_seg = {
                        "id": p_seg_id,
                        "type": "parallel_group",
                        "branches": branches_data,
                        "node_ids": [node_id],
                        "branch_count": len(branches_data),
                        "resource_policy": node.get("resource_policy", {}),  # ì›ë³¸ resource_policy ë³´ì¡´
                        "label": node.get("label", "")
                    }
                    local_segments.append(parallel_seg)
                    
                    # Aggregator ìƒì„±
                    agg_seg_id = seg_id_gen.next()
                    aggregator_seg = {
                        "id": agg_seg_id,
                        "type": "aggregator",
                        "nodes": [],
                        "edges": [],
                        "node_ids": [],
                        "source_parallel_group": p_seg_id
                    }
                    local_segments.append(aggregator_seg)
                    
                    # next ì„¤ì •
                    parallel_seg["next_mode"] = "default"
                    parallel_seg["default_next"] = agg_seg_id
                
                # ë‹¤ìŒ ë…¸ë“œ íƒìƒ‰
                for out_edge in outgoing_edges.get(node_id, []):
                    tgt = out_edge.get("target")
                    if tgt and tgt not in visited_nodes and tgt not in queue:
                        if not (stop_at_nodes and tgt in stop_at_nodes):
                            queue.append(tgt)
                continue
            
            # ë³‘ë ¬ ê·¸ë£¹ ì²˜ë¦¬ (ê·¸ë˜í”„ ë¶„ê¸°ì  ê¸°ë°˜)
            if is_branch:
                flush_local("normal")  # í˜„ì¬ê¹Œì§€ ì €ì¥
                
                # ë¶„ê¸°ì  ë…¸ë“œ ì²˜ë¦¬
                seg = create_segment({node_id: node}, [], "normal", config=config) 
                local_segments.append(seg)
                visited_nodes.add(node_id)
                
                out_edges = outgoing_edges.get(node_id, [])
                branch_targets = [e.get("target") for e in out_edges if e.get("target")]
                
                # í•©ë¥˜ì  ì°¾ê¸° - [Critical Fix #2] í•©ë¥˜ì ì´ forced_segment_startsì— ë“±ë¡ë¨
                convergence_node = find_convergence_node(branch_targets)
                stop_set = {convergence_node} if convergence_node else set()
                
                # ê° ë¸Œëœì¹˜ ì‹¤í–‰
                branches_data = []
                for i, target in enumerate(branch_targets):
                    if target:
                        # [Critical Fix #1] ì¬ê·€ ê¹Šì´ ì „ë‹¬
                        branch_segs = run_partitioning(
                            [target], 
                            stop_at_nodes=stop_set, 
                            config=config,
                            depth=depth + 1
                        )
                        
                        # [Critical Fix #3] ë¸Œëœì¹˜ ì¢…ë£Œ ê²€ì¦
                        if branch_segs:
                            last_seg = branch_segs[-1]
                            # ë¸Œëœì¹˜ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                            branch_data = {
                                "branch_id": f"B{i}",
                                "partition_map": branch_segs,
                                "has_end": last_seg.get("next_mode") == "end",
                                "target_node": target
                            }
                            branches_data.append(branch_data)
                            stats["branches"] += 1
                        else:
                            # ë¹ˆ ë¸Œëœì¹˜ ê²½ê³ 
                            logger.warning(f"Branch {i} starting at {target} produced no segments")
                
                # Parallel Group ìƒì„±
                if branches_data:
                    stats["parallel_groups"] += 1
                    p_seg_id = seg_id_gen.next()
                    parallel_seg = {
                        "id": p_seg_id,
                        "type": "parallel_group",
                        "branches": branches_data,
                        "node_ids": [],
                        "branch_count": len(branches_data)  # [ì¶”ê°€] ë¸Œëœì¹˜ ìˆ˜ ë©”íƒ€ë°ì´í„°
                    }
                    local_segments.append(parallel_seg)
                    
                    # Aggregator ìƒì„±
                    agg_seg_id = seg_id_gen.next()
                    aggregator_seg = {
                        "id": agg_seg_id,
                        "type": "aggregator",
                        "nodes": [],
                        "edges": [],
                        "node_ids": [],
                        "convergence_node": convergence_node,  # í•©ë¥˜ ë…¸ë“œ ì €ì¥
                        "source_parallel_group": p_seg_id  # [ì¶”ê°€] ì›ë³¸ parallel_group ì°¸ì¡°
                    }
                    local_segments.append(aggregator_seg)
                    
                    # Parallel Groupì˜ next ì„¤ì •
                    parallel_seg["next_mode"] = "default"
                    parallel_seg["default_next"] = agg_seg_id
                
                # í•©ë¥˜ì ì´ ìˆë‹¤ë©´ íì— ì¶”ê°€
                if convergence_node and convergence_node not in visited_nodes:
                    queue.append(convergence_node)
                continue
            
            # ì¼ë°˜ ë…¸ë“œ ì²˜ë¦¬
            local_current_nodes[node_id] = node
            visited_nodes.add(node_id)
            
            # íŠ¹ìˆ˜ íƒ€ì… ì²˜ë¦¬ - HITPê°€ LLMë³´ë‹¤ ìš°ì„ ìˆœìœ„ ë†’ìŒ (HITPëŠ” ì¸ê°„ ê°œì… í•„ìš”)
            if is_hitp_start:
                flush_local("hitp")
            elif is_llm:
                flush_local("llm")
            
            # ë‹¤ìŒ ë…¸ë“œ íƒìƒ‰
            for out_edge in outgoing_edges.get(node_id, []):
                tgt = out_edge.get("target")
                if tgt and tgt not in visited_nodes and tgt not in queue:
                    if not (stop_at_nodes and tgt in stop_at_nodes):
                        queue.append(tgt)
        
        flush_local()  # ë‚¨ì€ ê²ƒ ì²˜ë¦¬
        return local_segments
    
    # ì‹œì‘ ë…¸ë“œ ì°¾ê¸° ë° ì‹¤í–‰
    start_nodes = [nid for nid in nodes if not incoming_edges.get(nid)]
    if not start_nodes and nodes: 
        start_nodes = [list(nodes.keys())[0]]
    
    segments = run_partitioning(start_nodes, config=config)
    
    # --- Pass 2: ì¬ê·€ì  Node-to-Segment ë§¤í•‘ ---
    node_to_seg_map = {}
    
    def map_nodes_recursive(seg_list):
        for seg in seg_list:
            for nid in seg.get("node_ids", []):
                node_to_seg_map[nid] = seg["id"]
            
            if seg["type"] == "parallel_group":
                for branch in seg["branches"]:
                    map_nodes_recursive(branch["partition_map"])
    
    map_nodes_recursive(segments)
    
    # --- Next Mode ì„¤ì • (ì¬ê·€ì ) ---
    def process_links_recursive(seg_list: List[Dict[str, Any]], parent_aggregator_id: Optional[int] = None):
        """
        ì„¸ê·¸ë¨¼íŠ¸ ê°„ ì—°ê²°(next_mode) ì„¤ì •.
        
        [Critical Fix #3] ë¸Œëœì¹˜ ë‚´ë¶€ ì„¸ê·¸ë¨¼íŠ¸ê°€ ì˜¬ë°”ë¥´ê²Œ ì¢…ë£Œë˜ëŠ”ì§€ ê²€ì¦.
        parent_aggregator_idê°€ ì£¼ì–´ì§€ë©´, ë¸Œëœì¹˜ ë‚´ ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ì´ aggregatorë¡œ ì—°ê²°ë˜ì–´ì•¼ í•¨.
        """
        # Aggregator ì„¸ê·¸ë¨¼íŠ¸ë“¤ì˜ ID ì§‘í•©ì„ ë¯¸ë¦¬ íŒŒì•…
        aggregator_ids = {s["id"] for s in seg_list if s.get("type") == "aggregator"}
        
        for idx, seg in enumerate(seg_list):
            if seg["type"] == "parallel_group":
                # parallel_group ë‹¤ìŒì˜ aggregator ID ì°¾ê¸°
                next_agg_id = seg.get("default_next")
                
                for branch in seg["branches"]:
                    branch_segs = branch.get("partition_map", [])
                    
                    # [Critical Fix #3] ë¸Œëœì¹˜ ë‚´ë¶€ ì¬ê·€ ì²˜ë¦¬ - aggregator ID ì „ë‹¬
                    process_links_recursive(branch_segs, parent_aggregator_id=next_agg_id)
                    
                    # [Critical Fix #3] ë¸Œëœì¹˜ ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ê²€ì¦
                    if branch_segs:
                        last_branch_seg = branch_segs[-1]
                        
                        # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ê°€ ëª…ì‹œì  ENDê°€ ì•„ë‹ˆê³  nextê°€ ì—†ìœ¼ë©´
                        # aggregatorë¡œ ì•”ë¬µì  ì—°ê²° ì„¤ì •
                        if last_branch_seg.get("next_mode") == "end" and next_agg_id:
                            # ë¹„ëŒ€ì¹­ ë¸Œëœì¹˜: í•œ ìª½ì€ ëë‚˜ê³  ë‹¤ë¥¸ ìª½ì€ í•©ë¥˜
                            # aggregatorì—ì„œ ì´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                            last_branch_seg["implicit_aggregator_target"] = next_agg_id
                            branch["terminates_early"] = True
                            logger.debug(
                                f"Branch {branch['branch_id']} terminates early. "
                                f"Implicit aggregator target: {next_agg_id}"
                            )
                continue
            
            # Aggregatorì˜ ê²½ìš° convergence_nodeë¥¼ ì‚¬ìš©í•´ ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì—°ê²°
            if seg.get("type") == "aggregator":
                convergence_node = seg.get("convergence_node")
                source_p_seg_id = seg.get("source_parallel_group")
                
                # Case 1: Branch Convergence (Explicit Logic)
                if convergence_node and convergence_node in node_to_seg_map:
                    next_seg_id = node_to_seg_map[convergence_node]
                    seg["next_mode"] = "default"
                    seg["default_next"] = next_seg_id
                    continue
                
                # Case 2: Inline Parallel Group (Source Node Logic)
                # Aggregator created from inline parallel group should follow the parallel group node's edges
                elif source_p_seg_id is not None:
                    # Find source parallel segment
                    # Note: seg_list might be partial (recursive), but source_p_seg should be in the same list or parent?
                    # Actually for inline parallel, they are siblings in the same list.
                    source_seg = next((s for s in seg_list if s["id"] == source_p_seg_id), None)
                    
                    if source_seg and source_seg.get("node_ids"):
                        p_node_id = source_seg["node_ids"][0] # Parallel group node ID
                        
                        # Find target segment from outgoing edges of the parallel group node
                        # Similar to normal node exit logic
                        p_exit_edges = []
                        for out_edge in outgoing_edges.get(p_node_id, []):
                            tgt = out_edge.get("target")
                            if tgt and tgt in node_to_seg_map:
                                tgt_seg = node_to_seg_map[tgt]
                                if tgt_seg != source_p_seg_id and tgt_seg != seg["id"]:
                                     p_exit_edges.append({"edge": out_edge, "target_segment": tgt_seg})
                        
                        if p_exit_edges:
                            if len(p_exit_edges) == 1:
                                seg["next_mode"] = "default"
                                seg["default_next"] = p_exit_edges[0]["target_segment"]
                            else:
                                # [v3.27 Fix] Edge.condition ì œê±°ë¡œ ì¸í•œ ìˆ˜ì •
                                # parallel_groupì˜ ë‹¤ì¤‘ exit edgeë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
                                logger.warning(
                                    f"[Partition] Parallel group segment {seg['id']} has {len(p_exit_edges)} "
                                    f"exit edges. Using default routing to first exit."
                                )
                                seg["next_mode"] = "default"
                                seg["default_next"] = p_exit_edges[0]["target_segment"]
                            continue

                # Fallback / Error Handling
                if convergence_node:
                    # [Critical Fix #2] í•©ë¥˜ì ì´ ë§µì— ì—†ìœ¼ë©´ ê°•ì œë¡œ ì°¾ê¸° ì‹œë„
                    if convergence_node in forced_segment_starts:
                        logger.error(
                            f"Aggregator {seg['id']} has convergence node '{convergence_node}' "
                            f"which is a forced segment start but not mapped. "
                            f"This indicates a partitioning logic error."
                        )
                    else:
                        logger.warning(
                            f"Aggregator {seg['id']} has convergence node '{convergence_node}' "
                            f"but it is not mapped to any segment. Treating as workflow end."
                        )
                
                seg["next_mode"] = "end"
                seg["default_next"] = None
                continue
            
            exit_edges = []
            for nid in seg.get("node_ids", []):
                for out_edge in outgoing_edges.get(nid, []):
                    tgt = out_edge.get("target")
                    if tgt and tgt in node_to_seg_map:
                        tgt_seg = node_to_seg_map[tgt]
                        
                        # íƒ€ê²Ÿì´ í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ì™€ ë‹¤ë¥´ê³ 
                        if tgt_seg != seg["id"]:
                            # ë§Œì•½ íƒ€ê²Ÿì´ Aggregatorë¼ë©´, ë¸Œëœì¹˜ ë‚´ë¶€ì—ì„œëŠ” ì´ë¥¼ ì—°ê²°í•˜ì§€ ì•ŠìŒ
                            # (ASL Map Stateê°€ ëë‚˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë„˜ì–´ê°€ë„ë¡ í•¨)
                            if tgt_seg in aggregator_ids:
                                continue 
                                
                            exit_edges.append({"edge": out_edge, "target_segment": tgt_seg})
            
            if not exit_edges:
                # [Critical Fix #3] ë¶€ëª¨ aggregatorê°€ ìˆìœ¼ë©´ ì•”ë¬µì  ì—°ê²°
                if parent_aggregator_id is not None:
                    seg["next_mode"] = "implicit_aggregator"
                    seg["default_next"] = None
                    seg["parent_aggregator"] = parent_aggregator_id
                else:
                    seg["next_mode"] = "end"
                    seg["default_next"] = None
            elif len(exit_edges) == 1:
                seg["next_mode"] = "default"
                seg["default_next"] = exit_edges[0]["target_segment"]
            else:
                # [v3.27 Fix] Edgeì—ì„œ condition í•„ë“œ ì œê±°ë¨ (ë¼ìš°íŒ… ì£¼ê¶Œ ì¼ì›í™”)
                # ë‹¤ì¤‘ exit edgeëŠ” route_condition ë…¸ë“œê°€ ì²˜ë¦¬í•´ì•¼ í•¨
                # ì„¸ê·¸ë¨¼íŠ¸ ë ˆë²¨ì—ì„œëŠ” ì²« ë²ˆì§¸ exit edgeë¡œ default routing
                logger.warning(
                    f"[Partition] Segment {seg['id']} has {len(exit_edges)} exit edges "
                    f"but Edge.condition field is removed. Using default routing to first exit. "
                    f"Consider using route_condition node for conditional branching."
                )
                seg["next_mode"] = "default"
                seg["default_next"] = exit_edges[0]["target_segment"]
    
    process_links_recursive(segments)
    
    # --- ì¬ê·€ì  ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ ê³„ì‚° ---
    def count_segments_recursive(seg_list):
        total = 0
        for seg in seg_list:
            total += 1
            if seg.get("type") == "parallel_group":
                for branch in seg.get("branches", []):
                    total += count_segments_recursive(branch.get("partition_map", []))
        return total
    
    total_segments_recursive = count_segments_recursive(segments)
    
    # ğŸ›¡ï¸ [Critical Fix] Step Functions Loop Control requires Top-Level Count
    # execution_segments_count must be defined BEFORE use in loop limit calculation
    # It must match len(partition_map), otherwise loop will try to access non-existent indices.
    execution_segments_count = len(segments)
    
    # ğŸ›¡ï¸ [P2 Fix] execution_segments_countê°€ 0ì´ë©´ ìµœì†Œ 1ë¡œ ë³´ì¥ (ë¹ˆ ì›Œí¬í”Œë¡œìš° ë°©ì–´)
    if execution_segments_count < 1:
        logger.warning(f"execution_segments_count calculated as {execution_segments_count}, forcing to 1")
        execution_segments_count = 1
    
    # ğŸ›¡ï¸ [Dynamic Loop Limit] Analyze loop structures for segment-based counting
    # nodes is Dict[str, Dict], but analyze_loop_structures expects List[Dict]
    loop_analysis = analyze_loop_structures(list(nodes.values()), node_to_seg_map)
    
    # [v3.17] Complexity-budget loop limit calculation
    # for_each: Lambda-internal budget = sub_node_count Ã— max_iterations
    # sequential loop: SFN CONTINUE ì „ì´ = segment_count Ã— (max_iter - 1)
    # ë‘ ê°€ì¤‘ì¹˜ ëª¨ë‘ rawì— í•©ì‚° â†’ ë³µì¡ë„ ì˜ˆì‚° ê¸°ë°˜ limit ê³„ì‚°
    weighted_loop_segments = loop_analysis["total_loop_weighted_segments"]
    raw_estimated_executions = execution_segments_count + weighted_loop_segments
    
    # loop_limit = max(raw Ã— 1.5 + 20, 50)
    # - 1.5x: API ì¬ì‹œë„Â·ë§ˆì´ë„ˆ ë¶„í•  ì—¬ìœ 
    # - +20: ê·œëª¨ ë¬´ê´€ ìµœì†Œ ì™„ì¶©
    # - floor=50: ì‹¤ì§ˆì  ë¬´í•œë£¨í”„ ì°¨ë‹¨
    estimated_executions = max(
        int(raw_estimated_executions * LOOP_LIMIT_SAFETY_MULTIPLIER) + LOOP_LIMIT_FLAT_BONUS,
        LOOP_LIMIT_FLOOR
    )
    
    logger.info(
        f"[Dynamic Loop Limit] Complexity-budget analysis (v3.17): "
        f"base_segments={execution_segments_count}, "
        f"loop_count={loop_analysis['loop_count']}, "
        f"total_complexity_weight={weighted_loop_segments}, "
        f"raw_estimate={raw_estimated_executions}, "
        f"formula=max(int({raw_estimated_executions}*{LOOP_LIMIT_SAFETY_MULTIPLIER})+{LOOP_LIMIT_FLAT_BONUS}, {LOOP_LIMIT_FLOOR}), "
        f"estimated_executions={estimated_executions}"
    )
    
    # [Performance Optimization] Pre-indexed ë©”íƒ€ë°ì´í„° ë°˜í™˜
    return {
        "partition_map": segments,
        "total_segments": execution_segments_count,  # [Fix] Use top-level count for execution loop
        "llm_segments": stats["llm"],
        "hitp_segments": stats["hitp"],
        # [v2.0] ì¶”ê°€ í†µê³„
        "parallel_groups": stats["parallel_groups"],
        "total_branches": stats["branches"],
        "forced_segment_starts": list(forced_segment_starts),
        # [Performance] Pre-indexed ë°ì´í„° (ì¬ì‚¬ìš© ê°€ëŠ¥)
        "node_to_segment_map": node_to_seg_map,
        # ğŸ›¡ï¸ [Dynamic Loop Limit] Loop analysis results
        "loop_analysis": loop_analysis,
        "estimated_executions": estimated_executions,
        # ğŸš¨ [Performance Warnings] ëŒ€ê·œëª¨ ì›Œí¬í”Œë¡œìš° ê²½ê³ 
        "performance_warnings": performance_warnings,
        "metadata": {
            "max_partition_depth": MAX_PARTITION_DEPTH,
            "max_nodes_limit": MAX_NODES_LIMIT,
            "performance_warning_threshold": PERFORMANCE_WARNING_NODE_COUNT,
            "nodes_processed": len(visited_nodes),
            "total_nodes": len(nodes),
            "total_segments_recursive": total_segments_recursive,  # [Fix] Store recursive count in metadata
            "loop_nodes_count": loop_analysis["loop_count"],
            "weighted_execution_estimate": estimated_executions,
            "has_performance_warnings": len(performance_warnings) > 0,
            # [v3.17] Complexity-Budget Loop Limit metadata
            "raw_estimated_executions": raw_estimated_executions,
            "loop_limit_safety_multiplier": LOOP_LIMIT_SAFETY_MULTIPLIER,
            "loop_limit_flat_bonus": LOOP_LIMIT_FLAT_BONUS,
            "loop_limit_floor": LOOP_LIMIT_FLOOR,
        }
    }


def lambda_handler(event: Dict[str, Any], context: Any = None) -> Dict[str, Any]:
    """
    PartitionWorkflow Lambda: ì›Œí¬í”Œë¡œìš°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    
    [v2.0 Production Hardening]
    - DAG ê²€ì¦ ì‹¤íŒ¨ ì‹œ ëª…í™•í•œ ì—ëŸ¬ ë°˜í™˜
    - ì¬ê·€ ê¹Šì´ ì´ˆê³¼ ì‹œ ì—ëŸ¬ ë°˜í™˜
    - ë…¸ë“œ ìˆ˜ ì œí•œ ì´ˆê³¼ ì‹œ ì—ëŸ¬ ë°˜í™˜
    
    Input event:
        - workflow_config: ë¶„í• í•  ì›Œí¬í”Œë¡œìš° ì„¤ì •
        - ownerId: ì†Œìœ ì ID (ë³´ì•ˆ/ë¡œê¹…ìš©)
        
    Output:
        - partition_result: partition_workflow_advanced() ê²°ê³¼
        - status: "success" | "error"
    """
    try:
        workflow_config = event.get("workflow_config")
        if not workflow_config:
            raise ValueError("workflow_config is required")
        
        owner_id = event.get("ownerId") or event.get("owner_id") or event.get("user_id")
        
        # ì›Œí¬í”Œë¡œìš° ë¶„í•  ì‹¤í–‰
        partition_result = partition_workflow_advanced(workflow_config)
        
        logger.info(
            "Partitioned workflow for owner=%s: %d total segments "
            "(%d LLM, %d HITP, %d parallel groups, %d branches)", 
            owner_id,
            partition_result["total_segments"],
            partition_result["llm_segments"], 
            partition_result["hitp_segments"],
            partition_result.get("parallel_groups", 0),
            partition_result.get("total_branches", 0)
        )
        
        # ğŸ›¡ï¸ [Critical Fix] ë°˜í™˜ êµ¬ì¡° í‰íƒ„í™” - Step Functions ASLì´ $.Payload.total_segmentsë¥¼ ì§ì ‘ ì°¸ì¡°í•  ìˆ˜ ìˆë„ë¡
        # ê¸°ì¡´: {"status": "success", "partition_result": {...}} â†’ ASLì—ì„œ $.Payload.partition_result.total_segmentsë¡œ ì ‘ê·¼ í•„ìš”
        # ìˆ˜ì •: {"status": "success", "total_segments": N, ...} â†’ ASLì—ì„œ $.Payload.total_segmentsë¡œ ì§ì ‘ ì ‘ê·¼ ê°€ëŠ¥
        return {
            "status": "success",
            **partition_result  # ğŸ›¡ï¸ ê²°ê³¼ë¥¼ í‰íƒ„í™”í•˜ì—¬ ASL ë§¤í•‘ ì˜¤ë¥˜ í•´ê²°
        }
    
    except CycleDetectedError as e:
        logger.error(f"Cycle detected in workflow: {e.cycle_path}")
        return {
            "status": "error",
            "error_type": "CycleDetectedError",
            "error_message": str(e),
            "cycle_path": e.cycle_path,
            "total_segments": 1,  # ğŸ›¡ï¸ [P0] ASL null ì°¸ì¡° ë°©ì§€
            "partition_map": []
        }
    
    except AtomicGroupTimeoutError as e:
        logger.error(
            f"Atomic Group '{e.group_id}' timeout risk: "
            f"{e.estimated_duration:.1f}s > {e.lambda_timeout * 0.7:.1f}s (70% of {e.lambda_timeout}s)"
        )
        return {
            "status": "error",
            "error_type": "AtomicGroupTimeoutError",
            "error_message": str(e),
            "group_id": e.group_id,
            "estimated_duration": e.estimated_duration,
            "lambda_timeout": e.lambda_timeout,
            "total_segments": 1,  # ğŸ›¡ï¸ [P0] ASL null ì°¸ì¡° ë°©ì§€
            "partition_map": []
        }
    
    except PartitionDepthExceededError as e:
        logger.error(f"Partition depth exceeded: {e.depth}/{e.max_depth}")
        return {
            "status": "error",
            "error_type": "PartitionDepthExceededError",
            "error_message": str(e),
            "depth": e.depth,
            "max_depth": e.max_depth,
            "total_segments": 1,  # ğŸ›¡ï¸ [P0] ASL null ì°¸ì¡° ë°©ì§€
            "partition_map": []
        }
    
    except ValueError as e:
        logger.error(f"Validation error: {e}")
        return {
            "status": "error",
            "error_type": "ValidationError",
            "error_message": str(e),
            "total_segments": 1,  # ğŸ›¡ï¸ [P0] ASL null ì°¸ì¡° ë°©ì§€
            "partition_map": []
        }
        
    except Exception as e:
        logger.exception("Failed to partition workflow")
        return {
            "status": "error",
            "error_type": type(e).__name__,
            "error_message": str(e),
            "total_segments": 1,  # ğŸ›¡ï¸ [P0] ASL null ì°¸ì¡° ë°©ì§€
            "partition_map": []
        }
