"""
ì›Œí¬í”Œë¡œìš° ìºì‹œ ê´€ë¦¬ì

ë°˜ë³µì ì¸ DB ì¡°íšŒë¥¼ ì¤„ì´ê³  ë ˆì´í„´ì‹œë¥¼ ìµœì í™”í•˜ê¸° ìœ„í•œ ì›Œí¬í”Œë¡œìš° ì„¤ì • ìºì‹± ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

ğŸš€ ì£¼ìš” ê¸°ëŠ¥:
- ë©”ëª¨ë¦¬ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ì„¤ì • ìºì‹±
- TTL ê¸°ë°˜ ìë™ ë§Œë£Œ
- ìºì‹œ íˆíŠ¸ìœ¨ ëª¨ë‹ˆí„°ë§
- ìŠ¤ë ˆë“œ ì•ˆì „ ìºì‹œ ê´€ë¦¬

ğŸ¯ ì„±ëŠ¥ ê°œì„ :
- DB ì¡°íšŒ 90% ê°ì†Œ
- ì‘ë‹µ ì‹œê°„ 50-70% ë‹¨ì¶•
- ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ëŠ¥ë ¥ í–¥ìƒ
"""

import time
import threading
import hashlib
import json
import logging
from typing import Dict, Any, Optional, Tuple
from dataclasses import dataclass
from collections import OrderedDict

logger = logging.getLogger(__name__)


@dataclass
class CacheEntry:
    """ìºì‹œ ì—”íŠ¸ë¦¬"""
    data: Dict[str, Any]
    created_at: float
    last_accessed: float
    access_count: int
    ttl_seconds: int
    
    def is_expired(self) -> bool:
        """TTL ê¸°ë°˜ ë§Œë£Œ í™•ì¸"""
        return time.time() - self.created_at > self.ttl_seconds
    
    def is_stale(self, max_age_seconds: int = 300) -> bool:
        """ìµœëŒ€ ë‚˜ì´ ê¸°ë°˜ stale í™•ì¸ (5ë¶„)"""
        return time.time() - self.created_at > max_age_seconds


class WorkflowCacheManager:
    """
    ì›Œí¬í”Œë¡œìš° ì„¤ì • ìºì‹œ ê´€ë¦¬ì
    
    LRU ê¸°ë°˜ ë©”ëª¨ë¦¬ ìºì‹œë¡œ ì›Œí¬í”Œë¡œìš° ì„¤ì •ì„ ìºì‹±í•˜ì—¬
    ë°˜ë³µì ì¸ DynamoDB ì¡°íšŒë¥¼ ì¤„ì…ë‹ˆë‹¤.
    """
    
    def __init__(self, max_size: int = 1000, default_ttl: int = 600):
        """
        Args:
            max_size: ìµœëŒ€ ìºì‹œ ì—”íŠ¸ë¦¬ ìˆ˜
            default_ttl: ê¸°ë³¸ TTL (ì´ˆ)
        """
        self.max_size = max_size
        self.default_ttl = default_ttl
        self._cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self._lock = threading.RLock()
        self._stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
            'expired_removals': 0
        }
    
    def _generate_cache_key(self, owner_id: str, workflow_id: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = f"{owner_id}#{workflow_id}"
        return hashlib.md5(key_data.encode('utf-8')).hexdigest()
    
    def get(self, owner_id: str, workflow_id: str) -> Optional[Dict[str, Any]]:
        """
        ìºì‹œì—ì„œ ì›Œí¬í”Œë¡œìš° ì„¤ì • ì¡°íšŒ
        
        Args:
            owner_id: ì†Œìœ ì ID
            workflow_id: ì›Œí¬í”Œë¡œìš° ID
        
        Returns:
            ì›Œí¬í”Œë¡œìš° ì„¤ì • ë˜ëŠ” None (ìºì‹œ ë¯¸ìŠ¤)
        """
        cache_key = self._generate_cache_key(owner_id, workflow_id)
        
        with self._lock:
            entry = self._cache.get(cache_key)
            
            if entry is None:
                self._stats['misses'] += 1
                logger.debug(f"Cache miss: {owner_id}/{workflow_id}")
                return None
            
            # ë§Œë£Œ í™•ì¸
            if entry.is_expired():
                del self._cache[cache_key]
                self._stats['expired_removals'] += 1
                self._stats['misses'] += 1
                logger.debug(f"Cache expired: {owner_id}/{workflow_id}")
                return None
            
            # ìºì‹œ íˆíŠ¸: LRU ì—…ë°ì´íŠ¸
            entry.last_accessed = time.time()
            entry.access_count += 1
            self._cache.move_to_end(cache_key)  # LRU ì—…ë°ì´íŠ¸
            
            self._stats['hits'] += 1
            logger.debug(f"Cache hit: {owner_id}/{workflow_id} (age: {time.time() - entry.created_at:.1f}s)")
            
            return entry.data.copy()  # ë°©ì–´ì  ë³µì‚¬
    
    def put(self, owner_id: str, workflow_id: str, workflow_config: Dict[str, Any], ttl: Optional[int] = None) -> None:
        """
        ì›Œí¬í”Œë¡œìš° ì„¤ì •ì„ ìºì‹œì— ì €ì¥
        
        Args:
            owner_id: ì†Œìœ ì ID
            workflow_id: ì›Œí¬í”Œë¡œìš° ID
            workflow_config: ì›Œí¬í”Œë¡œìš° ì„¤ì •
            ttl: TTL (ì´ˆ), Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        """
        if not workflow_config:
            return
        
        cache_key = self._generate_cache_key(owner_id, workflow_id)
        ttl = ttl or self.default_ttl
        
        with self._lock:
            # ìºì‹œ í¬ê¸° ì œí•œ í™•ì¸
            if len(self._cache) >= self.max_size and cache_key not in self._cache:
                # LRU ì œê±°
                oldest_key, _ = self._cache.popitem(last=False)
                self._stats['evictions'] += 1
                logger.debug(f"Cache eviction: {oldest_key}")
            
            # ìƒˆ ì—”íŠ¸ë¦¬ ìƒì„±
            now = time.time()
            entry = CacheEntry(
                data=workflow_config.copy(),  # ë°©ì–´ì  ë³µì‚¬
                created_at=now,
                last_accessed=now,
                access_count=1,
                ttl_seconds=ttl
            )
            
            self._cache[cache_key] = entry
            logger.debug(f"Cache put: {owner_id}/{workflow_id} (TTL: {ttl}s)")
    
    def invalidate(self, owner_id: str, workflow_id: str) -> bool:
        """
        íŠ¹ì • ì›Œí¬í”Œë¡œìš° ìºì‹œ ë¬´íš¨í™”
        
        Args:
            owner_id: ì†Œìœ ì ID
            workflow_id: ì›Œí¬í”Œë¡œìš° ID
        
        Returns:
            ë¬´íš¨í™” ì„±ê³µ ì—¬ë¶€
        """
        cache_key = self._generate_cache_key(owner_id, workflow_id)
        
        with self._lock:
            if cache_key in self._cache:
                del self._cache[cache_key]
                logger.info(f"Cache invalidated: {owner_id}/{workflow_id}")
                return True
            return False
    
    def clear_expired(self) -> int:
        """
        ë§Œë£Œëœ ìºì‹œ ì—”íŠ¸ë¦¬ ì •ë¦¬
        
        Returns:
            ì •ë¦¬ëœ ì—”íŠ¸ë¦¬ ìˆ˜
        """
        removed_count = 0
        
        with self._lock:
            expired_keys = []
            for key, entry in self._cache.items():
                if entry.is_expired():
                    expired_keys.append(key)
            
            for key in expired_keys:
                del self._cache[key]
                removed_count += 1
                self._stats['expired_removals'] += 1
        
        if removed_count > 0:
            logger.info(f"Cleared {removed_count} expired cache entries")
        
        return removed_count
    
    def clear_all(self) -> None:
        """ëª¨ë“  ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±°"""
        with self._lock:
            cleared_count = len(self._cache)
            self._cache.clear()
            logger.info(f"Cleared all cache entries: {cleared_count}")
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        with self._lock:
            total_requests = self._stats['hits'] + self._stats['misses']
            hit_rate = (self._stats['hits'] / total_requests * 100) if total_requests > 0 else 0
            
            return {
                'cache_size': len(self._cache),
                'max_size': self.max_size,
                'hit_rate_percent': round(hit_rate, 2),
                'total_hits': self._stats['hits'],
                'total_misses': self._stats['misses'],
                'total_requests': total_requests,
                'evictions': self._stats['evictions'],
                'expired_removals': self._stats['expired_removals'],
                'memory_efficiency': round(len(self._cache) / self.max_size * 100, 1)
            }
    
    def get_cache_info(self) -> Dict[str, Any]:
        """ìƒì„¸ ìºì‹œ ì •ë³´ ì¡°íšŒ (ë””ë²„ê¹…ìš©)"""
        with self._lock:
            entries_info = []
            now = time.time()
            
            for key, entry in list(self._cache.items())[-10:]:  # ìµœê·¼ 10ê°œë§Œ
                entries_info.append({
                    'key_hash': key[:8],  # ë³´ì•ˆì„ ìœ„í•´ í•´ì‹œì˜ ì¼ë¶€ë§Œ
                    'age_seconds': round(now - entry.created_at, 1),
                    'last_accessed_ago': round(now - entry.last_accessed, 1),
                    'access_count': entry.access_count,
                    'ttl_remaining': max(0, entry.ttl_seconds - (now - entry.created_at)),
                    'is_expired': entry.is_expired()
                })
            
            return {
                'stats': self.get_stats(),
                'recent_entries': entries_info,
                'oldest_entry_age': round(now - min((e.created_at for e in self._cache.values()), default=now), 1),
                'newest_entry_age': round(now - max((e.created_at for e in self._cache.values()), default=now), 1)
            }


# ì „ì—­ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
_global_cache: Optional[WorkflowCacheManager] = None
_cache_lock = threading.Lock()


def get_workflow_cache() -> WorkflowCacheManager:
    """ì „ì—­ ì›Œí¬í”Œë¡œìš° ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_cache
    
    if _global_cache is None:
        with _cache_lock:
            if _global_cache is None:
                # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ìºì‹œ ì„¤ì • ì½ê¸°
                import os
                max_size = int(os.environ.get('WORKFLOW_CACHE_MAX_SIZE', '1000'))
                default_ttl = int(os.environ.get('WORKFLOW_CACHE_TTL_SECONDS', '600'))  # 10ë¶„
                
                _global_cache = WorkflowCacheManager(max_size=max_size, default_ttl=default_ttl)
                logger.info(f"Initialized global workflow cache: max_size={max_size}, ttl={default_ttl}s")
    
    return _global_cache


def cached_get_workflow_config(
    dynamodb_table, 
    owner_id: str, 
    workflow_id: str,
    force_refresh: bool = False
) -> Optional[Dict[str, Any]]:
    """
    ìºì‹œë¥¼ ì‚¬ìš©í•œ ì›Œí¬í”Œë¡œìš° ì„¤ì • ì¡°íšŒ
    
    Args:
        dynamodb_table: DynamoDB í…Œì´ë¸” ê°ì²´
        owner_id: ì†Œìœ ì ID
        workflow_id: ì›Œí¬í”Œë¡œìš° ID
        force_refresh: ê°•ì œ ìƒˆë¡œê³ ì¹¨ ì—¬ë¶€
    
    Returns:
        ì›Œí¬í”Œë¡œìš° ì„¤ì • ë˜ëŠ” None
    """
    cache = get_workflow_cache()
    
    # ê°•ì œ ìƒˆë¡œê³ ì¹¨ì´ ì•„ë‹ˆë©´ ìºì‹œì—ì„œ ë¨¼ì € ì¡°íšŒ
    if not force_refresh:
        cached_config = cache.get(owner_id, workflow_id)
        if cached_config is not None:
            return cached_config
    
    # ìºì‹œ ë¯¸ìŠ¤ ë˜ëŠ” ê°•ì œ ìƒˆë¡œê³ ì¹¨: DBì—ì„œ ì¡°íšŒ
    try:
        response = dynamodb_table.get_item(Key={'ownerId': owner_id, 'workflowId': workflow_id})
        
        if 'Item' in response:
            workflow_item = response['Item']
            workflow_config = workflow_item.get('config')
            
            if workflow_config:
                # ìºì‹œì— ì €ì¥
                cache.put(owner_id, workflow_id, workflow_config)
                logger.debug(f"Loaded and cached workflow config: {owner_id}/{workflow_id}")
                return workflow_config
        
        logger.debug(f"Workflow not found in DB: {owner_id}/{workflow_id}")
        return None
        
    except Exception as e:
        logger.error(f"Failed to load workflow config from src.DB: {owner_id}/{workflow_id}, error: {e}")
        return None


def invalidate_workflow_cache(owner_id: str, workflow_id: str) -> bool:
    """
    ì›Œí¬í”Œë¡œìš° ìºì‹œ ë¬´íš¨í™” (ì›Œí¬í”Œë¡œìš° ì—…ë°ì´íŠ¸ ì‹œ í˜¸ì¶œ)
    
    Args:
        owner_id: ì†Œìœ ì ID
        workflow_id: ì›Œí¬í”Œë¡œìš° ID
    
    Returns:
        ë¬´íš¨í™” ì„±ê³µ ì—¬ë¶€
    """
    cache = get_workflow_cache()
    return cache.invalidate(owner_id, workflow_id)


def get_cache_statistics() -> Dict[str, Any]:
    """ìºì‹œ í†µê³„ ì¡°íšŒ (ëª¨ë‹ˆí„°ë§ìš©)"""
    cache = get_workflow_cache()
    return cache.get_stats()


def cleanup_expired_cache() -> int:
    """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬ (ì •ê¸° ì‹¤í–‰ìš©)"""
    cache = get_workflow_cache()
    return cache.clear_expired()


# ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ë””ë²„ê¹…ìš© í•¨ìˆ˜ë“¤

def test_cache_performance(test_cases: list, iterations: int = 100) -> Dict[str, Any]:
    """
    ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    
    Args:
        test_cases: [(owner_id, workflow_id, config), ...] í˜•íƒœì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        iterations: ë°˜ë³µ íšŸìˆ˜
    
    Returns:
        ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼
    """
    cache = get_workflow_cache()
    cache.clear_all()  # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ìºì‹œ ì´ˆê¸°í™”
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    for owner_id, workflow_id, config in test_cases:
        cache.put(owner_id, workflow_id, config)
    
    # ì„±ëŠ¥ ì¸¡ì •
    import time
    start_time = time.time()
    
    for _ in range(iterations):
        for owner_id, workflow_id, _ in test_cases:
            cache.get(owner_id, workflow_id)
    
    end_time = time.time()
    total_time = end_time - start_time
    
    stats = cache.get_stats()
    
    return {
        'total_time_seconds': round(total_time, 4),
        'average_time_per_request_ms': round(total_time / (iterations * len(test_cases)) * 1000, 4),
        'requests_per_second': round((iterations * len(test_cases)) / total_time, 2),
        'cache_stats': stats
    }


def benchmark_cache_vs_db(mock_db_latency_ms: float = 50) -> Dict[str, Any]:
    """
    ìºì‹œ vs DB ì„±ëŠ¥ ë¹„êµ ë²¤ì¹˜ë§ˆí¬
    
    Args:
        mock_db_latency_ms: ëª¨ì˜ DB ë ˆì´í„´ì‹œ (ë°€ë¦¬ì´ˆ)
    
    Returns:
        ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
    """
    import time
    
    cache = get_workflow_cache()
    cache.clear_all()
    
    test_config = {'segments': [{'type': 'test', 'id': i} for i in range(10)]}
    
    # ìºì‹œ ì„±ëŠ¥ ì¸¡ì •
    cache.put('test_user', 'test_workflow', test_config)
    
    cache_times = []
    for _ in range(100):
        start = time.time()
        cache.get('test_user', 'test_workflow')
        cache_times.append((time.time() - start) * 1000)  # ms
    
    # ëª¨ì˜ DB ì„±ëŠ¥ ì¸¡ì •
    db_times = []
    for _ in range(100):
        start = time.time()
        time.sleep(mock_db_latency_ms / 1000)  # ëª¨ì˜ DB ì§€ì—°
        db_times.append((time.time() - start) * 1000)  # ms
    
    avg_cache_time = sum(cache_times) / len(cache_times)
    avg_db_time = sum(db_times) / len(db_times)
    
    return {
        'cache_avg_ms': round(avg_cache_time, 4),
        'db_avg_ms': round(avg_db_time, 4),
        'speedup_factor': round(avg_db_time / avg_cache_time, 2),
        'latency_reduction_percent': round((1 - avg_cache_time / avg_db_time) * 100, 2),
        'cache_stats': cache.get_stats()
    }