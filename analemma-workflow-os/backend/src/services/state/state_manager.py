"""
StateManager - Legacy State Management Utilities

âš ï¸ DEPRECATED: Phase Eì—ì„œ StateVersioningServiceë¡œ í†µí•© ì¤‘

í˜„ì¬ ìƒíƒœ:
- âœ… PII ë§ˆìŠ¤í‚¹ â†’ SecurityUtilsë¡œ ë¶„ë¦¬ ì™„ë£Œ
- ğŸ”„ S3 ì—…ë¡œë“œ/ë‹¤ìš´ë¡œë“œ â†’ StateVersioningServiceë¡œ í†µí•© ì¤‘
- âœ… Backward Compatibility ìœ ì§€ (ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ì‘ë™)

ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ:
    # ê¸°ì¡´ ì½”ë“œ (ê³„ì† ì‘ë™)
    from src.services.state.state_manager import StateManager
    manager = StateManager()
    s3_path = manager.upload_state_to_s3(bucket, prefix, state)
    
    # ìƒˆ ì½”ë“œ (ê¶Œì¥)
    from src.services.state.state_versioning_service import StateVersioningService
    versioning = StateVersioningService(...)
    s3_path = versioning.save_state(state, workflow_id, execution_id)
"""

import json
import logging
import os
import boto3
from typing import Dict, Any, Optional, Tuple

logger = logging.getLogger(__name__)

# âœ… Phase E: PII ë§ˆìŠ¤í‚¹ì€ SecurityUtilsë¡œ ë¶„ë¦¬
try:
    from src.common.security_utils import mask_pii_in_state as _mask_pii_in_state
    _HAS_SECURITY_UTILS = True
except ImportError:
    logger.warning("[StateManager] SecurityUtils not available, using legacy masking")
    _HAS_SECURITY_UTILS = False
    
    # Fallback: ê¸°ì¡´ ë§ˆìŠ¤í‚¹ ë¡œì§
    import re
    
    PII_REGEX_PATTERNS = {
        'email': (re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'), '[EMAIL_MASKED]'),
        'phone_kr': (re.compile(r'0\d{1,2}-\d{3,4}-\d{4}'), '[PHONE_MASKED]'),
    }
    
    def _mask_pii_in_state(state: Any) -> Any:
        """Legacy PII ë§ˆìŠ¤í‚¹ (fallback)"""
        if isinstance(state, str):
            for _, (pattern, replacement) in PII_REGEX_PATTERNS.items():
                state = pattern.sub(replacement, state)
            return state
        elif isinstance(state, dict):
            return {k: _mask_pii_in_state(v) for k, v in state.items()}
        elif isinstance(state, list):
            return [_mask_pii_in_state(item) for item in state]
        else:
            return state


class StateManager:
    """
    âœ… Phase E: Wrapper Class (Backward Compatibility)
    
    ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ë˜ëŠ” ë˜í¼ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    ì‹¤ì œ êµ¬í˜„ì€ StateVersioningServiceì™€ SecurityUtilsì— ìœ„ì„ë©ë‹ˆë‹¤.
    
    âš ï¸ DEPRECATED: ìƒˆ ì½”ë“œì—ì„œëŠ” ì§ì ‘ StateVersioningService ì‚¬ìš© ê¶Œì¥
    """
    
    def __init__(self, s3_client=None):
        self.s3_client = s3_client or boto3.client("s3")
        self._versioning_service = None  # Lazy initialization
    
    @property
    def versioning_service(self):
        """âœ… Phase E: Lazy StateVersioningService ì´ˆê¸°í™”"""
        if self._versioning_service is None:
            try:
                from src.services.state.state_versioning_service import StateVersioningService
                
                # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
                manifests_table = os.environ.get('MANIFESTS_TABLE', 'WorkflowManifests-v3-dev')
                state_bucket = os.environ.get('SKELETON_S3_BUCKET') or os.environ.get('WORKFLOW_STATE_BUCKET')
                
                self._versioning_service = StateVersioningService(
                    dynamodb_table=manifests_table,
                    s3_bucket=state_bucket
                )
                logger.info("[StateManager] âœ… StateVersioningService initialized (Lazy)")
            except Exception as e:
                logger.error(f"[StateManager] âŒ Failed to initialize StateVersioningService: {e}")
                raise
        
        return self._versioning_service

    def download_state_from_s3(self, s3_path: str) -> Dict[str, Any]:
        """
        âœ… Phase E: Wrapper â†’ StateVersioningService.load_state()
        
        Download state JSON from S3.
        
        âš ï¸ DEPRECATED: ìƒˆ ì½”ë“œì—ì„œëŠ” StateVersioningService.load_state() ì§ì ‘ ì‚¬ìš©
        """
        logger.debug("[StateManager] download_state_from_s3() wrapper called")
        return self.versioning_service.load_state(s3_path)

    def upload_state_to_s3(self, bucket: str, prefix: str, state: Dict[str, Any], deterministic_filename: Optional[str] = None) -> str:
        """
        âœ… Phase E: Wrapper â†’ StateVersioningService.save_state()
        
        Upload state JSON to S3.
        
        âš ï¸ DEPRECATED: ìƒˆ ì½”ë“œì—ì„œëŠ” StateVersioningService.save_state() ì§ì ‘ ì‚¬ìš©
        """
        logger.debug("[StateManager] upload_state_to_s3() wrapper called")
        
        # bucketê³¼ prefixì—ì„œ workflow_id, execution_id ì¶”ì¶œ
        # prefix í˜•ì‹: "workflows/{workflow_id}/executions/{execution_id}/segments/{segment_id}"
        try:
            parts = prefix.split('/')
            workflow_id = parts[1] if len(parts) > 1 else 'unknown'
            execution_id = parts[3] if len(parts) > 3 else 'unknown'
            segment_id = int(parts[5]) if len(parts) > 5 and parts[4] == 'segments' else None
        except:
            workflow_id = 'legacy'
            execution_id = 'unknown'
            segment_id = None
        
        return self.versioning_service.save_state(
            state=state,
            workflow_id=workflow_id,
            execution_id=execution_id,
            segment_id=segment_id,
            deterministic_filename=deterministic_filename
        )

    def _upload_raw_bytes_to_s3(self, bucket: str, prefix: str, serialized_bytes: bytes, deterministic_filename: Optional[str] = None) -> str:
        """
        [Perf Optimization] Upload pre-serialized bytes directly to S3.
        Eliminates double serialization overhead.
        """
        try:
            import time
            import uuid
            
            file_name = deterministic_filename if deterministic_filename else f"{int(time.time())}_{uuid.uuid4().hex[:8]}.json"
            key = f"{prefix}/{file_name}"
            s3_path = f"s3://{bucket}/{key}"
            
            logger.info("â¬†ï¸ [Optimized] Uploading pre-serialized bytes to: %s (%d bytes)", s3_path, len(serialized_bytes))
            self.s3_client.put_object(
                Bucket=bucket,
                Key=key,
                Body=serialized_bytes,
                ContentType="application/json"
            )
            return s3_path
        except Exception as e:
            logger.error("âŒ Failed to upload raw bytes to %s: %s", bucket, e)
            raise RuntimeError(f"Failed to upload raw bytes to S3: {e}")

    def handle_state_storage(self, state: Dict[str, Any], auth_user_id: str, workflow_id: str, segment_id: int, bucket: Optional[str], threshold: Optional[int] = None, loop_counter: Optional[int] = None) -> Tuple[Dict[str, Any], Optional[str]]:
        """
        âœ… Phase E: PII ë§ˆìŠ¤í‚¹ì€ SecurityUtils ì‚¬ìš©
        
        Decide whether to store state inline or in S3 based on size threshold.
        PII data is masked before storage to ensure privacy compliance.
        
        âš ï¸ ë³€ê²½ ì‚¬í•­:
        - PII ë§ˆìŠ¤í‚¹: SecurityUtils.mask_pii_in_state() ì‚¬ìš©
        - ê¸°ì¡´ ë¡œì§ ìœ ì§€ (Backward Compatibility)
        """
        try:
            # âœ… Phase E: SecurityUtilsë¡œ PII ë§ˆìŠ¤í‚¹
            masked_state = _mask_pii_in_state(state)
            logger.debug("ğŸ”’ PII masking applied to state before storage")
            
            # [Perf Optimization] Single Serialization - ì§ë ¬í™” í•œ ë²ˆë§Œ ìˆ˜í–‰
            serialized_bytes = json.dumps(masked_state, ensure_ascii=False).encode("utf-8")
            state_size = len(serialized_bytes)
            
            # [Critical Fix] Step Functions hard limit with safety buffer
            # 256KB = 262,144 bytes, but AWS wrapper adds ~10-15KB overhead
            # Using 180KB (180,000 bytes) for safe margin
            SF_HARD_LIMIT = 180000  # ~175KB safe threshold
            
            # [Fix] Handle None threshold - default to 180KB (safe Step Functions limit)
            if threshold is None:
                threshold = 180000
                logger.warning("âš ï¸ threshold parameter was None, using default 180KB (safe SF limit)")
            
            if state_size > threshold:
                if not bucket:
                    logger.error("ğŸš¨ CRITICAL: State size (%d bytes, %.1fKB) exceeds threshold (%d) but no S3 bucket provided!", 
                                state_size, state_size/1024, threshold)
                    
                    # [Critical Fix] Instead of returning the full state (which causes SF failure),
                    # return a truncated state with error information
                    if state_size > SF_HARD_LIMIT:
                        logger.error("ğŸš¨ State exceeds Step Functions safe limit (180KB)! Creating safe fallback state.")
                        
                        # Create a minimal safe state that won't exceed limits
                        safe_state = {
                            "__state_truncated": True,
                            "__original_size_bytes": state_size,
                            "__original_size_kb": round(state_size / 1024, 2),
                            "__truncation_reason": "State exceeded 180KB Step Functions safe limit but no S3 bucket available",
                            "__error": "PAYLOAD_TOO_LARGE_NO_S3_BUCKET",
                            # Preserve essential metadata if present
                            "workflowId": masked_state.get("workflowId") if isinstance(masked_state, dict) else None,
                            "ownerId": masked_state.get("ownerId") if isinstance(masked_state, dict) else None,
                            "segment_id": segment_id,
                        }
                        
                        # Try to preserve test result if this is a test workflow
                        if isinstance(masked_state, dict):
                            for key in ['TEST_RESULT', 'VALIDATION_STATUS', '__kernel_actions']:
                                if key in masked_state:
                                    safe_state[key] = masked_state[key]
                        
                        logger.warning("âš ï¸ Returning truncated safe state (%d bytes) instead of full state (%d bytes)", 
                                      len(json.dumps(safe_state)), state_size)
                        return safe_state, None
                    else:
                        # State is below SF limit but above our threshold - return with warning
                        logger.warning("âš ï¸ State size (%d) exceeds threshold but below SF safe limit. Returning inline (risky).", state_size)
                        return masked_state, None

                if not auth_user_id:
                    raise PermissionError("Missing authenticated user id for S3 upload")
                
                # [v3.10] Loop-Safe Path Construction
                if loop_counter is not None and isinstance(loop_counter, int) and loop_counter >= 0:
                    # e.g. .../segments/10/5/output.json (Loop #5)
                    prefix = f"workflow-states/{auth_user_id}/{workflow_id}/segments/{segment_id}/{loop_counter}"
                else:
                    prefix = f"workflow-states/{auth_user_id}/{workflow_id}/segments/{segment_id}"
                
                # [Perf Optimization] ì´ë¯¸ ì§ë ¬í™”ëœ ë°”ì´íŠ¸ë¥¼ ì§ì ‘ S3ì— ì—…ë¡œë“œ (ì¤‘ë³µ ì§ë ¬í™” ì œê±°)
                s3_path = self._upload_raw_bytes_to_s3(bucket, prefix, serialized_bytes, deterministic_filename="output.json")
                logger.info("ğŸ“¦ State uploaded to S3: %s (%d bytes, %.1fKB)", s3_path, state_size, state_size/1024)
                
                # [Critical Fix] Return S3 metadata instead of None to prevent AttributeError
                # downstream when calling .get() on the result
                s3_metadata = {
                    "__s3_offloaded": True,
                    "__s3_path": s3_path,
                    "__original_size_kb": round(state_size / 1024, 2)
                }
                return s3_metadata, s3_path
            else:
                logger.info("ğŸ“¦ Returning state inline (%d bytes <= %d threshold)", state_size, threshold)
                return masked_state, None
        except Exception as e:
            logger.exception("Failed to handle state storage")
            raise RuntimeError(f"Failed to handle state storage: {e}")
