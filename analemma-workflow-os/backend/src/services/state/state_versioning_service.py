# -*- coding: utf-8 -*-
"""
[Phase 1] Merkle DAG ê¸°ë°˜ ìƒíƒœ ë²„ì €ë‹ ì„œë¹„ìŠ¤

í•µì‹¬ ê¸°ëŠ¥:
1. ìƒíƒœ ë³€ê²½ ì‹œ ë¸íƒ€ë§Œ ì €ì¥ (Content-Addressable Storage)
2. Merkle Rootë¡œ ë¬´ê²°ì„± ê²€ì¦
3. Pointer Manifestë¡œ ì¦‰ì‹œ íšŒê·€ ê°€ëŠ¥
4. Pre-computed Hashë¡œ O(1) segment ê²€ì¦ (Phase 7)
"""

import hashlib
import json
import os
import time
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from datetime import datetime
from decimal import Decimal

import boto3
from botocore.exceptions import ClientError

logger = logging.getLogger(__name__)

# ìš´ì˜ í™˜ê²½ ìƒìˆ˜
MAX_BLOCK_SIZE = 4 * 1024 * 1024  # 4MB (ë¸”ë¡ ë¶„í•  ì„ê³„ê°’)
VERSION_RETRY_ATTEMPTS = 3  # Race Condition ì¬ì‹œë„ íšŸìˆ˜


def _calculate_optimal_workers() -> int:
    """Lambda ë©”ëª¨ë¦¬ ê¸°ë°˜ I/O ë³‘ë ¬ ìŠ¤ë ˆë“œ ìˆ˜ ê³„ì‚°.

    S3 ì‘ì—…ì€ I/O-boundì´ë¯€ë¡œ vCPU ìˆ˜ë³´ë‹¤ ë§ì€ ìŠ¤ë ˆë“œê°€ ìœ íš¨.
    LambdaëŠ” ë©”ëª¨ë¦¬ 1769MBë‹¹ 1 vCPU í• ë‹¹. ê·¸ ë¹„ìœ¨ë¡œ ìŠ¤ë ˆë“œ ìˆ˜ ì¡°ì •.

    Returns:
        int: 4 ~ 32 ì‚¬ì´ì˜ ì ì • worker ìˆ˜.
    """
    import os
    try:
        memory_mb = int(os.environ.get('AWS_LAMBDA_FUNCTION_MEMORY_SIZE', '512'))
        return min(32, max(4, memory_mb // 256))
    except (ValueError, TypeError):
        return 4  # safe default


@dataclass
class ContentBlock:
    """Merkle DAGì˜ ì»¨í…ì¸  ë¸”ë¡"""
    block_id: str  # sha256 í•´ì‹œ
    s3_path: str
    size: int
    fields: List[str]  # ì´ ë¸”ë¡ì— í¬í•¨ëœ í•„ë“œ ëª©ë¡
    checksum: str


@dataclass
class ManifestPointer:
    """Pointer Manifest êµ¬ì¡°"""
    manifest_id: str
    version: int
    parent_hash: Optional[str]
    manifest_hash: str
    config_hash: str  # workflow_config ê²€ì¦ìš©
    blocks: List[ContentBlock]
    metadata: Dict


class StateVersioningService:
    """
    ğŸ§¬ KernelStateManager - Analemma OSì˜ ë‹¨ì¼ ìƒíƒœ ê´€ë¦¬ ì»¤ë„
    
    v3.3 í†µí•© ì•„í‚¤í…ì²˜ (Zero Redundancy):
    - Merkle DAG ê¸°ë°˜ Delta Storage (ì¤‘ë³µ ë°ì´í„° 90% ì œê±°)
    - DynamoDB í¬ì¸í„° ê¸°ë°˜ ìƒíƒœ ë³µì› (latest_state.json íê¸°)
    - 2-Phase Commit ì™„ì „ ë‚´ì¥ (temp â†’ ready íƒœê·¸ ì „ëµ)
    - GC ìë™ ì—°ê³„ (Ghost Block ì›ì²œ ì°¨ë‹¨)
    
    í•µì‹¬ ì„¤ê³„ ì² í•™:
    1. ğŸ—‘ï¸ latest_state.json íê¸°: DynamoDBì— manifest_id í¬ì¸í„°ë§Œ ì €ì¥
    2. ğŸ§¬ ë‹¨ì¼ ì €ì¥ ê²½ë¡œ: save_state_delta()ë¡œ ëª¨ë“  ì €ì¥ í†µì¼
    3. ğŸ›¡ï¸ 2-Phase Commit ë‚´ì¥: S3 ì—…ë¡œë“œ ì‹œ ë¬´ì¡°ê±´ status=temp, DynamoDB ì„±ê³µ ì‹œ status=ready
    4. â™»ï¸ GC ìë™ ì—°ê³„: Phase 10 BackgroundGCê°€ temp íƒœê·¸ ë¸”ë¡ ìë™ ì œê±°
    
    âœ… Phase B: Unified Architecture (EventualConsistencyGuard í†µí•©)
    âœ… Phase E-F-G: StatePersistenceService/StateManager/StateDataManager í¡ìˆ˜ í†µí•©
    âœ… v3.3: ê¸‰ì§„ì  ì¬ì„¤ê³„ (ë§ˆì´ê·¸ë ˆì´ì…˜ ì¡±ì‡„ ì œê±°)
    """
    
    def __init__(
        self,
        dynamodb_table: str,
        s3_bucket: str,
        block_references_table: str = None,
        use_2pc: bool = False,              # âœ… Phase B: 2-Phase Commit
        gc_dlq_url: Optional[str] = None    # âœ… Phase B: GC DLQ
    ):
        self.dynamodb = boto3.resource('dynamodb')
        self.dynamodb_client = boto3.client('dynamodb')  # For TransactWriteItems
        self.table = self.dynamodb.Table(dynamodb_table)
        self.s3 = boto3.client('s3')
        self.bucket = s3_bucket
        
        # Block Reference Counting Table (Garbage Collectionìš©)
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©
        self.block_references_table = block_references_table or dynamodb_table.replace('Manifests', 'BlockReferences')
        try:
            self.block_refs_table = self.dynamodb.Table(self.block_references_table)
        except Exception as e:
            logger.warning(f"BlockReferences table not available: {e}")
        
        # âœ… Phase B: 2-Phase Commit ì„¤ì •
        self.use_2pc = use_2pc
        self.gc_dlq_url = gc_dlq_url
        self._consistency_guard = None  # Lazy Import (ì‹¤ì œ ì‚¬ìš© ì‹œ ì´ˆê¸°í™”)

        # S3 ë¸”ë¡ í‚¤ í”„ë¦¬í”½ìŠ¤ (state-blocks/{block_id}.json)
        self.prefix = "state-"
    
    def create_manifest(
        self,
        workflow_id: str,
        workflow_config: dict,
        segment_manifest: List[dict],
        parent_manifest_id: Optional[str] = None
    ) -> ManifestPointer:
        """
        ìƒˆ Pointer Manifest ìƒì„±
        
        âœ… v3.3: 2-Phase Commit ê°•ì œ ì‚¬ìš© (Legacy ì œê±°)
        
        Args:
            workflow_id: ì›Œí¬í”Œë¡œìš° ID
            workflow_config: ì›Œí¬í”Œë¡œìš° ì„¤ì • (í•´ì‹œ ê³„ì‚°ìš©)
            segment_manifest: ì„¸ê·¸ë¨¼íŠ¸ ëª©ë¡
            parent_manifest_id: ì´ì „ ë²„ì „ ID (Merkle ì²´ì¸)
        
        Returns:
            ManifestPointer: ìƒì„±ëœ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ í¬ì¸í„°
        """
        # v3.3: 2-Phase Commit ê°•ì œ ì‚¬ìš©
        if not self.use_2pc:
            logger.warning("[StateVersioningService] use_2pc=False is deprecated, forcing 2PC")
        
        if not self.gc_dlq_url:
            raise RuntimeError("GC DLQ URL is required for 2-Phase Commit")
        
        return self._create_manifest_with_2pc(
            workflow_id=workflow_id,
            workflow_config=workflow_config,
            segment_manifest=segment_manifest,
            parent_manifest_id=parent_manifest_id
        )
    
    def _create_manifest_with_2pc(
        self,
        workflow_id: str,
        workflow_config: dict,
        segment_manifest: List[dict],
        parent_manifest_id: Optional[str]
    ) -> ManifestPointer:
        """
        âœ… v3.3: EventualConsistencyGuardë¥¼ ì‚¬ìš©í•œ 2-Phase Commit (ê°•ì œ)
        """
        # Lazy Import
        if self._consistency_guard is None:
            from src.services.state.eventual_consistency_guard import EventualConsistencyGuard
            self._consistency_guard = EventualConsistencyGuard(
                s3_bucket=self.bucket,
                dynamodb_table=self.table.name,
                block_references_table=self.block_references_table,
                gc_dlq_url=self.gc_dlq_url
            )
            logger.info("[StateVersioningService] âœ… EventualConsistencyGuard initialized")
            # [FIX] ì´ì „ ë¦¬íŒ©í† ë§ ê³¼ì •ì—ì„œ ë‚¨ê²¨ì§„ ê³ ì•„ ì½”ë“œ ì œê±°.
            # parent_manifest_idëŠ” ì•„ë˜ metadata dictì— ì´ë¯¸ í¬í•¨ë¨ (line ~198).
        
        # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ê¸°ë³¸ ì •ë³´ ìƒì„±
        import uuid
        manifest_id = str(uuid.uuid4())
        version = self._get_next_version(workflow_id)
        config_hash = self._compute_hash(workflow_config)
        
        # S3ì— workflow_config ì €ì¥
        config_s3_key = f"workflow-configs/{workflow_id}/{config_hash}.json"
        try:
            self.s3.put_object(
                Bucket=self.bucket,
                Key=config_s3_key,
                Body=json.dumps(workflow_config, default=str),
                ContentType='application/json',
                Metadata={
                    'usage': 'reference_only',
                    'workflow_id': workflow_id,
                    'config_hash': config_hash
                }
            )
        except Exception as e:
            logger.error(f"Failed to store workflow_config: {e}")
            raise
        
        # ë¸”ë¡ ë¶„í•  ë° í•´ì‹œ ê³„ì‚°
        # EventualConsistencyGuard expects plain dicts with block_id, s3_key, data.
        # _split_into_blocks() returns ContentBlock objects (used only by legacy path).
        blocks = []
        for idx, segment in enumerate(segment_manifest):
            segment_json = self._canonical_json_serialize(segment)
            segment_bytes = segment_json.encode('utf-8')
            block_id = hashlib.sha256(segment_bytes).hexdigest()
            if len(segment_bytes) <= MAX_BLOCK_SIZE:
                blocks.append({
                    'block_id': block_id,
                    's3_key': f"state-blocks/{block_id}.json",
                    'data': segment,
                })
            else:
                # Large segment: split into chunks
                chunk_index = 0
                for offset in range(0, len(segment_json), MAX_BLOCK_SIZE):
                    chunk_str = segment_json[offset:offset + MAX_BLOCK_SIZE]
                    chunk_bytes = chunk_str.encode('utf-8')
                    chunk_id = hashlib.sha256(chunk_bytes).hexdigest()
                    blocks.append({
                        'block_id': chunk_id,
                        's3_key': f"state-blocks/{chunk_id}.json",
                        'data': {'__chunk__': chunk_str, '__chunk_index__': chunk_index,
                                 'segment_index': idx},
                    })
                    chunk_index += 1
        segment_hashes = self._compute_segment_hashes(segment_manifest)
        manifest_hash = self._compute_hash({
            'workflow_id': workflow_id,
            'version': version,
            'config_hash': config_hash,
            'segment_hashes': segment_hashes
        })
        
        # ë©”íƒ€ë°ì´í„°
        metadata = {
            'workflow_id': workflow_id,
            'version': version,
            'created_at': datetime.utcnow().isoformat(),
            'parent_manifest_id': parent_manifest_id,
            'total_segments': len(segment_manifest)
        }
        
        # EventualConsistencyGuardë¡œ 2PC ì‹¤í–‰
        return self._consistency_guard.create_manifest_with_consistency(
            workflow_id=workflow_id,
            manifest_id=manifest_id,
            version=version,
            config_hash=config_hash,
            manifest_hash=manifest_hash,
            blocks=blocks,
            segment_hashes=segment_hashes,
            metadata=metadata
        )
    
    def _create_manifest_legacy(
        self,
        workflow_id: str,
        workflow_config: dict,
        segment_manifest: List[dict],
        parent_manifest_id: Optional[str]
    ) -> ManifestPointer:
        """
        ğŸ”„ ê¸°ì¡´ Legacy Manifest ìƒì„± (Production Fixes í¬í•¨)
        """
        import uuid
        
        manifest_id = str(uuid.uuid4())
        
        # 1. workflow_config í•´ì‹œ ê³„ì‚° (ë¶ˆë³€ ì°¸ì¡°)
        config_hash = self._compute_hash(workflow_config)
        
        # 2. workflow_configë¥¼ S3ì— ì €ì¥ (ì°¸ì¡°ìš©)
        config_s3_key = f"workflow-configs/{workflow_id}/{config_hash}.json"
        
        try:
            self.s3.put_object(
                Bucket=self.bucket,
                Key=config_s3_key,
                Body=json.dumps(workflow_config, default=str),
                ContentType='application/json',
                Metadata={
                    'usage': 'reference_only',
                    'workflow_id': workflow_id,
                    'config_hash': config_hash
                }
            )
            logger.info(f"Stored workflow_config to S3: s3://{self.bucket}/{config_s3_key}")
        except Exception as e:
            logger.error(f"Failed to store workflow_config: {e}")
            raise
        
        # 3. segment_manifestë¥¼ Content Blocksë¡œ ë¶„í• 
        blocks = self._split_into_blocks(segment_manifest)
        
        # 3.5. Pre-computed Hash ìƒì„± (Phase 7 ê²€ì¦ ìµœì í™”ìš©)
        segment_hashes = self._compute_segment_hashes(segment_manifest)
        logger.info(f"Pre-computed {len(segment_hashes)} segment hashes")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # [Phase 10] S3 ì—…ë¡œë“œ with Pending Tags (ìœ ë ¹ ë¸”ë¡ ë°©ì§€)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ë¬¸ì œ: S3 ì„±ê³µ + DynamoDB ì‹¤íŒ¨ â†’ Ghost Block ë°œìƒ
        # í•´ê²°: Pending Tag ì „ëµ (status=pending â†’ committed)
        transaction_id = str(uuid.uuid4())
        
        # 4. ê° ë¸”ë¡ì„ S3ì— ì €ì¥ (Content-Addressable) + Pending Tags
        stored_blocks = 0
        reused_blocks = 0
        
        for idx, block in enumerate(blocks):
            if not self._block_exists(block.block_id):
                try:
                    # [Fix #2] ì‹¤ì œ segment ë°ì´í„° ì €ì¥ (í”¼ë“œë°± ë°˜ì˜)
                    # ê¸°ì¡´: ë©”íƒ€ë°ì´í„°ë§Œ ì €ì¥ (fields, checksum)
                    # ê°œì„ : ì‹¤ì œ segment_manifest ë°ì´í„° ì €ì¥
                    
                    # í•´ë‹¹ ì„¸ê·¸ë¨¼íŠ¸ ë˜ëŠ” ì²­í¬ ë°ì´í„° ì¶”ì¶œ
                    segment_data = None
                    
                    # í•„ë“œëª…ì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤ ì¶”ì¶œ
                    field_info = block.fields[0]  # "segment_0" ë˜ëŠ” "segment_0_chunk_1"
                    
                    if "_chunk_" in field_info:
                        # ì²­í¬ ì¼€ì´ìŠ¤: segment_idx_chunk_N
                        parts = field_info.split("_")
                        segment_idx = int(parts[1])
                        chunk_idx = int(parts[3])
                        
                        # í•´ë‹¹ ì²­í¬ì˜ ë°ì´í„°ëŠ” ì´ë¯¸ block.checksumì— í•´ì‹œë¨
                        # ì›ë³¸ segmentë¥¼ ì§ë ¬í™”í•œ í›„ ì²­í¬ë¡œ ë¶„í• í•œ ë¶€ë¶„ ì €ì¥
                        segment_json = self._canonical_json_serialize(segment_manifest[segment_idx])
                        chunk_start = chunk_idx * MAX_BLOCK_SIZE
                        chunk_end = chunk_start + MAX_BLOCK_SIZE
                        chunk_data = segment_json[chunk_start:chunk_end]
                        segment_data = chunk_data
                    else:
                        # ë‹¨ì¼ ë¸”ë¡ ì¼€ì´ìŠ¤: segment_N
                        segment_idx = int(field_info.split("_")[1])
                        
                        # âœ… [í”¼ë“œë°± â‘ ] JSON Lines í˜•ì‹ìœ¼ë¡œ ì €ì¥ (S3 Select ìµœì í™”)
                        segment = segment_manifest[segment_idx]
                        segment_data = json.dumps(segment, default=self._json_default) + "\n"  # ndjson
                    
                    # âœ… [í”¼ë“œë°± â‘¡] Pending Tagë¡œ ì—…ë¡œë“œ (Ghost Block ë°©ì§€)
                    self.s3.put_object(
                        Bucket=self.bucket,
                        Key=block.s3_path.replace(f"s3://{self.bucket}/", ""),
                        Body=segment_data,  # âœ… ì‹¤ì œ ë°ì´í„° ì €ì¥
                        ContentType='application/json',
                        Tagging=f"status=pending&transaction_id={transaction_id}",  # âœ… Pending Tag
                        Metadata={
                            'block_id': block.block_id,
                            'fields': ','.join(block.fields),
                            'checksum': block.checksum,
                            'transaction_id': transaction_id,
                            'format': 'ndjson'  # JSON Lines í˜•ì‹ í‘œì‹œ
                        }
                    )
                    stored_blocks += 1
                except Exception as e:
                    logger.error(f"Failed to store block {block.block_id}: {e}")
                    # âœ… [í”¼ë“œë°± â‘¡] S3 ì—…ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì´ë¯¸ ì—…ë¡œë“œëœ ë¸”ë¡ë“¤ ì •ë¦¬
                    self._rollback_pending_blocks(blocks[:idx], transaction_id)
                    raise
            else:
                reused_blocks += 1
        
        logger.info(f"Blocks: {stored_blocks} stored, {reused_blocks} reused (deduplication)")
        
        # 5. Merkle Root ê³„ì‚°
        parent_hash = None
        if parent_manifest_id:
            try:
                parent = self.get_manifest(parent_manifest_id)
                parent_hash = parent.manifest_hash
            except Exception as e:
                logger.warning(f"Failed to get parent manifest {parent_manifest_id}: {e}")
        
        manifest_hash = self._compute_merkle_root(blocks, config_hash, parent_hash)
        
        # 6. DynamoDBì— í¬ì¸í„° ì €ì¥ (Race Condition ë°©ì§€)
        version = self._get_next_version(workflow_id)
        
        # [Fix #1] ì¡°ê±´ë¶€ ì“°ê¸°ë¡œ ë²„ì „ ì¶©ëŒ ë°©ì§€
        # [CRITICAL FIX] TransactWriteItemsë¡œ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥ + ë¸”ë¡ ì°¸ì¡° ì¹´ìš´íŠ¸ ì¦ê°€ ì›ìí™”
        # í”¼ë“œë°±: manifest_idë¿ ì•„ë‹ˆë¼ workflow_id+version ì¡°í•©ë„ ì²´í¬ í•„ìš”
        # í˜„ìƒ: Lambda Aì™€ Bê°€ ë™ì‹œì— ë²„ì „ 6ìœ¼ë¡œ ì“°ê¸° ì‹œë„ ê°€ëŠ¥
        
        # âœ… [í”¼ë“œë°± â‘¢] TransactWriteItems 100ê°œ ì œí•œ ëŒ€ì‘
        # ì œí•œ: DynamoDB íŠ¸ëœì­ì…˜ì€ ìµœëŒ€ 100ê°œ ì•„ì´í…œ
        # í•´ê²°: ë¸”ë¡ì´ 100ê°œ ì´ˆê³¼ ì‹œ ë°°ì¹˜ ë¶„í• 
        MAX_TRANSACTION_ITEMS = 100
        
        for attempt in range(VERSION_RETRY_ATTEMPTS):
            try:
                # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                # [ATOMICITY FIX] ì›ìì  íŠ¸ëœì­ì…˜ìœ¼ë¡œ Dangling Pointer ë°©ì§€
                # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                
                # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥ ì•„ì´í…œ
                manifest_item = {
                    'Put': {
                        'TableName': self.table.table_name,
                        'Item': {
                            'manifest_id': {'S': manifest_id},
                            'version': {'N': str(version)},
                            'workflow_id': {'S': workflow_id},
                            'parent_hash': {'S': parent_hash or 'null'},
                            'manifest_hash': {'S': manifest_hash},
                            'config_hash': {'S': config_hash},
                            'segment_hashes': {'M': {k: {'S': v} for k, v in segment_hashes.items()}},
                            's3_pointers': {'M': {
                                'manifest': {'S': f"s3://{self.bucket}/manifests/{manifest_id}.json"},
                                'config': {'S': f"s3://{self.bucket}/{config_s3_key}"},
                                'state_blocks': {'L': [{'S': block.s3_path} for block in blocks]}
                            }},
                            'metadata': {'M': {
                                'created_at': {'S': datetime.utcnow().isoformat()},
                                'segment_count': {'N': str(len(segment_manifest))},
                                'total_size': {'N': str(sum(block.size for block in blocks))},
                                'compression': {'S': 'none'},
                                'blocks_stored': {'N': str(stored_blocks)},
                                'blocks_reused': {'N': str(reused_blocks)},
                                'transaction_id': {'S': transaction_id}  # âœ… Transaction ID ì €ì¥
                            }},
                            'ttl': {'N': str(int(time.time()) + 30 * 24 * 3600)}
                        },
                        'ConditionExpression': 'attribute_not_exists(manifest_id)'
                    }
                }
                
                # âœ… [í”¼ë“œë°± â‘¢] ë¸”ë¡ ì°¸ì¡° ì—…ë°ì´íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë¶„í•  (100ê°œ ì œí•œ ëŒ€ì‘)
                # ì „ëµ: ì²« ë²ˆì§¸ íŠ¸ëœì­ì…˜ì— ë§¤ë‹ˆí˜ìŠ¤íŠ¸ + ìµœëŒ€ 99ê°œ ë¸”ë¡
                #       ë‚˜ë¨¸ì§€ ë¸”ë¡ì€ ë³„ë„ ë°°ì¹˜ ì—…ë°ì´íŠ¸
                
                first_batch_blocks = blocks[:MAX_TRANSACTION_ITEMS - 1]  # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ 1ê°œ + ë¸”ë¡ 99ê°œ
                remaining_blocks = blocks[MAX_TRANSACTION_ITEMS - 1:]
                
                # ì²« ë²ˆì§¸ íŠ¸ëœì­ì…˜: ë§¤ë‹ˆí˜ìŠ¤íŠ¸ + ì²« 99ê°œ ë¸”ë¡
                transact_items = [manifest_item]
                
                for block in first_batch_blocks:
                    transact_items.append({
                        'Update': {
                            'TableName': self.block_references_table,
                            'Key': {
                                'workflow_id': {'S': workflow_id},
                                'block_id': {'S': block.block_id}
                            },
                            'UpdateExpression': 'ADD reference_count :inc SET last_referenced = :now',
                            'ExpressionAttributeValues': {
                                ':inc': {'N': '1'},
                                ':now': {'S': datetime.utcnow().isoformat()}
                            }
                        }
                    })
                
                # âœ… ì›ìì  íŠ¸ëœì­ì…˜ ì‹¤í–‰: ëª¨ë‘ ì„±ê³µ or ëª¨ë‘ ì‹¤íŒ¨
                self.dynamodb_client.transact_write_items(TransactItems=transact_items)
                
                logger.info(
                    f"[Atomic Transaction] âœ… Created manifest {manifest_id} (v{version}) "
                    f"+ incremented {len(first_batch_blocks)} block references (first batch)"
                )
                
                # âœ… [í”¼ë“œë°± â‘¢] ë‚˜ë¨¸ì§€ ë¸”ë¡ ì°¸ì¡° ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸ (100ê°œ ì´ˆê³¼ ì‹œ)
                if remaining_blocks:
                    logger.info(f"[Batch Update] Processing {len(remaining_blocks)} remaining blocks...")
                    
                    # 100ê°œì”© ë°°ì¹˜ ì²˜ë¦¬
                    for i in range(0, len(remaining_blocks), MAX_TRANSACTION_ITEMS):
                        batch = remaining_blocks[i:i + MAX_TRANSACTION_ITEMS]
                        batch_transact_items = []
                        
                        for block in batch:
                            batch_transact_items.append({
                                'Update': {
                                    'TableName': self.block_references_table,
                                    'Key': {
                                        'workflow_id': {'S': workflow_id},
                                        'block_id': {'S': block.block_id}
                                    },
                                    'UpdateExpression': 'ADD reference_count :inc SET last_referenced = :now',
                                    'ExpressionAttributeValues': {
                                        ':inc': {'N': '1'},
                                        ':now': {'S': datetime.utcnow().isoformat()}
                                    }
                                }
                            })
                        
                        self.dynamodb_client.transact_write_items(TransactItems=batch_transact_items)
                    
                    logger.info(f"[Batch Update] âœ… Completed {len(remaining_blocks)} remaining block references")
                
                # âœ… [í”¼ë“œë°± â‘¡] S3 ë¸”ë¡ë“¤ì„ Committed ìƒíƒœë¡œ ì „í™˜
                self._commit_pending_blocks(blocks, transaction_id)
                
                break  # ì„±ê³µ ì‹œ ë£¨í”„ íƒˆì¶œ
                
            except ClientError as e:
                error_code = e.response['Error']['Code']
                
                if error_code == 'TransactionCanceledException':
                    # ì¡°ê±´ ì²´í¬ ì‹¤íŒ¨ (manifest_id ì¤‘ë³µ ë“±)
                    cancellation_reasons = e.response['Error'].get('CancellationReasons', [])
                    logger.warning(
                        f"[Atomicity] Transaction cancelled on attempt {attempt + 1}: {cancellation_reasons}"
                    )
                    
                    # manifest_id ì¬ìƒì„±
                    import uuid
                    manifest_id = str(uuid.uuid4())
                    
                    if attempt == VERSION_RETRY_ATTEMPTS - 1:
                        raise RuntimeError(
                            f"Failed to create manifest after {VERSION_RETRY_ATTEMPTS} attempts. "
                            f"Last error: {cancellation_reasons}"
                        )
                else:
                    logger.error(f"[Atomicity] Transaction failed: {e}")
                    raise
            
            except Exception as e:
                logger.error(f"[Atomicity] Unexpected error during manifest creation: {e}")
                raise
        
        return ManifestPointer(
            manifest_id=manifest_id,
            version=version,
            parent_hash=parent_hash,
            manifest_hash=manifest_hash,
            config_hash=config_hash,
            blocks=blocks,
            metadata={
                'segment_count': len(segment_manifest),
                'total_size': sum(block.size for block in blocks)
            }
        )
    
    def get_manifest(self, manifest_id: str) -> ManifestPointer:
        """
        ë§¤ë‹ˆí˜ìŠ¤íŠ¸ í¬ì¸í„° ë¡œë“œ
        
        Args:
            manifest_id: ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ID
        
        Returns:
            ManifestPointer: ë§¤ë‹ˆí˜ìŠ¤íŠ¸ í¬ì¸í„°
        """
        try:
            response = self.table.get_item(Key={'manifest_id': manifest_id})
            
            if 'Item' not in response:
                raise ValueError(f"Manifest not found: {manifest_id}")
            
            item = response['Item']
            
            # ContentBlock ì¬êµ¬ì„±
            blocks = []
            for s3_path in item['s3_pointers']['state_blocks']:
                block_id = s3_path.split('/')[-1].replace('.json', '')
                blocks.append(ContentBlock(
                    block_id=block_id,
                    s3_path=s3_path,
                    size=0,  # ë©”íƒ€ë°ì´í„°ì—ì„œ ë³µì› ê°€ëŠ¥
                    fields=[],
                    checksum=block_id
                ))
            
            return ManifestPointer(
                manifest_id=item['manifest_id'],
                version=item['version'],
                parent_hash=item.get('parent_hash'),
                manifest_hash=item['manifest_hash'],
                config_hash=item['config_hash'],
                blocks=blocks,
                metadata=item.get('metadata', {})
            )
            
        except ClientError as e:
            logger.error(f"DynamoDB error loading manifest {manifest_id}: {e}")
            raise
    
    def verify_manifest_integrity(self, manifest_id: str) -> bool:
        """
        Merkle Root ê²€ì¦
        
        Returns:
            bool: ë¬´ê²°ì„± ê²€ì¦ í†µê³¼ ì—¬ë¶€
        """
        try:
            response = self.table.get_item(Key={'manifest_id': manifest_id})
            
            if 'Item' not in response:
                logger.error(f"Manifest not found: {manifest_id}")
                return False
            
            item = response['Item']
            
            # ì €ì¥ëœ ë¸”ë¡ë“¤ë¡œ Merkle Root ì¬ê³„ì‚°
            blocks = self._load_blocks(item['s3_pointers']['state_blocks'])
            computed_hash = self._compute_merkle_root(
                blocks,
                item['config_hash'],
                item.get('parent_hash') if item.get('parent_hash') != 'null' else None
            )
            
            is_valid = computed_hash == item['manifest_hash']
            
            if not is_valid:
                logger.error(
                    f"[Integrity Violation] Manifest {manifest_id} hash mismatch! "
                    f"Expected: {item['manifest_hash']}, "
                    f"Computed: {computed_hash}"
                )
            else:
                logger.info(f"âœ“ Manifest {manifest_id} integrity verified")
            
            return is_valid
            
        except Exception as e:
            logger.error(f"Verification failed for {manifest_id}: {e}")
            return False
    
    def verify_segment_config(
        self,
        segment_config: dict,
        manifest_id: str,
        segment_index: int
    ) -> bool:
        """
        [Phase 7] segment_config ë¬´ê²°ì„± ê²€ì¦ (Pre-computed Hash ë°©ì‹)
        
        í”¼ë“œë°± ë°˜ì˜:
        - âŒ ê¸°ì¡´: ë§¤ë²ˆ partition_workflow() ì¬ì‹¤í–‰ (200-500ms)
        - âœ… ê°œì„ : Pre-computed Hashë¡œ O(1) ê²€ì¦ (1-5ms)
        
        Args:
            segment_config: ê²€ì¦í•  ì„¸ê·¸ë¨¼íŠ¸ ì„¤ì •
            manifest_id: ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ID
            segment_index: ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤
        
        Returns:
            bool: ê²€ì¦ í†µê³¼ ì—¬ë¶€
        """
        try:
            # 1. DynamoDBì—ì„œ Pre-computed Hash ë¡œë“œ
            response = self.table.get_item(
                Key={'manifest_id': manifest_id},
                ProjectionExpression='segment_hashes'
            )
            
            if 'Item' not in response:
                logger.error(f"Manifest not found: {manifest_id}")
                return False
            
            segment_hashes = response['Item'].get('segment_hashes', {})
            expected_hash = segment_hashes.get(str(segment_index))
            
            if not expected_hash:
                logger.error(f"No pre-computed hash for segment {segment_index}")
                return False
            
            # 2. ì…ë ¥ëœ segment_configì˜ í•´ì‹œ ê³„ì‚°
            actual_hash = self._compute_hash(segment_config)
            
            # 3. ë¹„êµ
            is_valid = actual_hash == expected_hash
            
            if not is_valid:
                logger.error(
                    f"[Integrity Violation] Segment {segment_index} hash mismatch!\n"
                    f"Expected: {expected_hash[:16]}...\n"
                    f"Actual:   {actual_hash[:16]}..."
                )
            else:
                logger.info(f"âœ“ Segment {segment_index} verified: {actual_hash[:8]}...")
            
            return is_valid
            
        except Exception as e:
            logger.error(f"Verification failed: {e}", exc_info=True)
            return False
    
    def _canonical_json_serialize(self, data: Any) -> str:
        """
        [Fix #3] í‘œì¤€ ì§ë ¬í™” í¬ë§· (100% í•´ì‹œ ì¼ê´€ì„± ë³´ì¥)
        
        í”¼ë“œë°± ë°˜ì˜:
        - âŒ ê¸°ì¡´: default=strë¡œ datetime í¬ë§· ë¶ˆì¼ì¹˜ ê°€ëŠ¥
        - âœ… ê°œì„ : ISO 8601 ê°•ì œ, Decimal â†’ float í‘œì¤€í™”
        """
        def default_handler(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()  # ISO 8601 ê°•ì œ
            elif isinstance(obj, Decimal):
                return float(obj)  # DynamoDB Decimal ì²˜ë¦¬
            elif hasattr(obj, '__dict__'):
                return obj.__dict__
            else:
                return str(obj)
        
        return json.dumps(data, sort_keys=True, default=default_handler, ensure_ascii=False)
    
    def _compute_hash(self, data: dict) -> str:
        """JSON ë°ì´í„°ì˜ SHA256 í•´ì‹œ ê³„ì‚° (í‘œì¤€ ì§ë ¬í™” ì‚¬ìš©)"""
        json_str = self._canonical_json_serialize(data)
        return hashlib.sha256(json_str.encode('utf-8')).hexdigest()
    
    def _compute_merkle_root(
        self,
        blocks: List[ContentBlock],
        config_hash: str,
        parent_hash: Optional[str]
    ) -> str:
        """
        Merkle Root ê³„ì‚°
        
        êµ¬ì¡°:
        root_hash = sha256(
            config_hash +
            parent_hash +
            sha256(block1.checksum + block2.checksum + ...)
        )
        """
        blocks_hash = hashlib.sha256(
            ''.join(b.checksum for b in sorted(blocks, key=lambda x: x.block_id)).encode()
        ).hexdigest()
        
        combined = config_hash + (parent_hash or '') + blocks_hash
        return hashlib.sha256(combined.encode()).hexdigest()
    
    def _split_into_blocks(self, manifest: List[dict]) -> List[ContentBlock]:
        """
        [Fix #2] segment_manifestë¥¼ Content Blocksë¡œ ë¶„í•  (ì²­í¬ ë¶„í•  ì§€ì›)
        
        í”¼ë“œë°± ë°˜ì˜:
        - âœ… ì„¸ê·¸ë¨¼íŠ¸ê°€ 4MB ì´ˆê³¼ ì‹œ ì²­í¬ë¡œ ë¶„í• 
        - âœ… í™•ì¥ì„±: ê±°ëŒ€í•œ í”„ë¡¬í”„íŠ¸/ì„ë² ë”© ëŒ€ì‘
        
        ì „ëµ: ê° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë³„ë„ ë¸”ë¡ìœ¼ë¡œ, ë‹¨ í¬ê¸° ì´ˆê³¼ ì‹œ ì²­í¬ ë¶„í• 
        """
        blocks = []
        for idx, segment in enumerate(manifest):
            segment_json = self._canonical_json_serialize(segment)
            segment_bytes = segment_json.encode('utf-8')
            segment_size = len(segment_bytes)
            
            # ì„¸ê·¸ë¨¼íŠ¸ê°€ 4MB ì´í•˜ë©´ ë‹¨ì¼ ë¸”ë¡
            if segment_size <= MAX_BLOCK_SIZE:
                block_id = hashlib.sha256(segment_bytes).hexdigest()
                
                blocks.append(ContentBlock(
                    block_id=block_id,
                    s3_path=f"s3://{self.bucket}/state-blocks/{block_id}.json",
                    size=segment_size,
                    fields=[f"segment_{idx}"],
                    checksum=block_id
                ))
            
            # ì„¸ê·¸ë¨¼íŠ¸ê°€ 4MB ì´ˆê³¼ â†’ ì²­í¬ë¡œ ë¶„í• 
            else:
                logger.info(f"Segment {idx} is {segment_size / 1024 / 1024:.2f}MB, splitting into chunks...")
                
                # JSON ë¬¸ìì—´ì„ ì²­í¬ë¡œ ë¶„í•  (4MB ë‹¨ìœ„)
                chunk_index = 0
                for offset in range(0, len(segment_json), MAX_BLOCK_SIZE):
                    chunk = segment_json[offset:offset + MAX_BLOCK_SIZE]
                    chunk_bytes = chunk.encode('utf-8')
                    chunk_id = hashlib.sha256(chunk_bytes).hexdigest()
                    
                    blocks.append(ContentBlock(
                        block_id=chunk_id,
                        s3_path=f"s3://{self.bucket}/state-blocks/{chunk_id}.json",
                        size=len(chunk_bytes),
                        fields=[f"segment_{idx}_chunk_{chunk_index}"],
                        checksum=chunk_id
                    ))
                    
                    chunk_index += 1
                
                logger.info(f"Segment {idx} split into {chunk_index} chunks")
        
        return blocks
    
    def _block_exists(self, block_id: str) -> bool:
        """ë¸”ë¡ì´ S3ì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (ì¤‘ë³µ ì œê±°)"""
        try:
            self.s3.head_object(
                Bucket=self.bucket,
                Key=f"state-blocks/{block_id}.json"
            )
            return True
        except ClientError as e:
            if e.response['Error']['Code'] == '404':
                return False
            raise
    
    def _load_blocks(self, block_paths: List[str]) -> List[ContentBlock]:
        """S3ì—ì„œ ë¸”ë¡ ë¡œë“œ"""
        blocks = []
        for s3_path in block_paths:
            block_id = s3_path.split('/')[-1].replace('.json', '')
            blocks.append(ContentBlock(
                block_id=block_id,
                s3_path=s3_path,
                size=0,
                fields=[],
                checksum=block_id
            ))
        return blocks
    
    def _compute_segment_hashes(self, manifest: List[dict]) -> Dict[int, str]:
        """
        ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ ê°œë³„ í•´ì‹œ ë¯¸ë¦¬ ê³„ì‚° (Phase 7 ìµœì í™”ìš©)
        
        í”¼ë“œë°±:
        - ë§¤ ì„¸ê·¸ë¨¼íŠ¸ë§ˆë‹¤ partition_workflow() ì¬ì‹¤í–‰ì€ ë„ˆë¬´ ë¬´ê±°ì›€
        - Pre-computed Hashë¡œ O(n) â†’ O(1) ê²€ì¦
        
        Returns:
            Dict[segment_index, hash]: ì„¸ê·¸ë¨¼íŠ¸ë³„ í•´ì‹œê°’
        """
        segment_hashes = {}
        
        for idx, segment in enumerate(manifest):
            # segment_configë§Œ ì¶”ì¶œí•˜ì—¬ í•´ì‹œ ê³„ì‚°
            segment_config = segment.get('segment_config', segment)
            segment_hash = self._compute_hash(segment_config)
            segment_hashes[str(idx)] = segment_hash  # DynamoDBëŠ” ë¬¸ìì—´ í‚¤ ì„ í˜¸
            
            logger.debug(f"Pre-computed hash for segment {idx}: {segment_hash[:8]}...")
        
        return segment_hashes
    
    def inject_dynamic_segment(
        self,
        manifest_id: str,
        segment_config: dict,
        insert_position: int,
        max_retries: int = 3
    ) -> str:
        """
        ğŸ§ª ëŸ°íƒ€ì„ ì„¸ê·¸ë¨¼íŠ¸ ì£¼ì… ì‹œ í•´ì‹œ ë§µ ì‹¤ì‹œê°„ ê°±ì‹  (Phase 12)
        
        ğŸ§¬ [ë…¼ë¦¬ ê°œì„  #4] Ordered Hash Chain ë„ì… (ì¸ë±ìŠ¤ ì¶©ëŒ ë°©ì§€)
        ğŸ§ª [íƒ„ë ¥ì„± ê°œì„  #3] ë‚´ë¶€ ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ (100msâ†’200msâ†’400ms)
        
        Phase 8.3 ëŒ€ì‘:
        - ë™ì ìœ¼ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ê°€
        - segment_hashesë¥¼ ordered_hash_chainìœ¼ë¡œ ì¬êµ¬ì„±
        - ì¤‘ê°„ ì‚½ì… ì‹œ ê¸°ì¡´ ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤ ìë™ shift
        - hash_version ì¦ê°€ (Optimistic Locking)
        - ì¶©ëŒ ì‹œ ìë™ ì¬ì‹œë„ (caller ë¶€ë‹´ ì œê±°)
        
        Args:
            manifest_id: ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ID
            segment_config: ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ ì„¤ì •
            insert_position: ì‚½ì… ìœ„ì¹˜ (0-based)
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 3)
        
        Returns:
            str: ìƒˆë¡œ ê³„ì‚°ëœ ì„¸ê·¸ë¨¼íŠ¸ í•´ì‹œ
        """
        import time
        
        # ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ í•´ì‹œ ê³„ì‚°
        new_segment_hash = self._compute_hash(segment_config)
        
        # ğŸ§¬ [ë…¼ë¦¬ ê°œì„  #4] ê¸°ì¡´ segment_hashes ë¡œë“œ ë° ì¬ì •ë ¬
        # ğŸ§ª [íƒ„ë ¥ì„± ê°œì„  #3] ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ ë£¨í”„
        for attempt in range(max_retries):
            try:
                response = self.table.get_item(
                    Key={'manifest_id': manifest_id},
                    ProjectionExpression='segment_hashes, hash_version'
                )
                
                if 'Item' not in response:
                    raise ValueError(f"Manifest {manifest_id} not found")
                
                item = response['Item']
                segment_hashes = item.get('segment_hashes', {})
                current_hash_version = item.get('hash_version', 0)
                
                # ğŸ§¬ Ordered Hash Chain ì¬êµ¬ì„± (insert_position ì´í›„ ëª¨ë“  ì¸ë±ìŠ¤ +1 shift)
                new_segment_hashes = {}
                
                for idx_str, hash_value in sorted(segment_hashes.items(), key=lambda x: int(x[0])):
                    idx = int(idx_str)
                    
                    if idx < insert_position:
                        # ì‚½ì… ìœ„ì¹˜ ì´ì „: ê·¸ëŒ€ë¡œ ìœ ì§€
                        new_segment_hashes[str(idx)] = hash_value
                    else:
                        # ì‚½ì… ìœ„ì¹˜ ì´í›„: ì¸ë±ìŠ¤ +1 shift
                        new_segment_hashes[str(idx + 1)] = hash_value
                
                # ìƒˆ ì„¸ê·¸ë¨¼íŠ¸ ì‚½ì…
                new_segment_hashes[str(insert_position)] = new_segment_hash
                
                # DynamoDB ì›ìì  ì—…ë°ì´íŠ¸ (ì „ì²´ ë§µ êµì²´)
                update_response = self.table.update_item(
                    Key={'manifest_id': manifest_id},
                    UpdateExpression=(
                        'SET segment_hashes = :new_hashes, '
                        'hash_version = :new_version'
                    ),
                    ConditionExpression=(
                        'attribute_exists(manifest_id) AND '
                        'hash_version = :expected_version'  # Optimistic Locking
                    ),
                    ExpressionAttributeValues={
                        ':new_hashes': new_segment_hashes,
                        ':new_version': current_hash_version + 1,
                        ':expected_version': current_hash_version
                    },
                    ReturnValues='ALL_NEW'
                )
                
                new_hash_version = update_response['Attributes'].get('hash_version', 1)
                
                logger.info(
                    f"[Dynamic Injection] âœ… Segment injected at position {insert_position} "
                    f"(attempt {attempt + 1}/{max_retries}). "
                    f"Shifted {len(segment_hashes) - insert_position} existing segments. "
                    f"manifest_id={manifest_id}, hash={new_segment_hash[:8]}..., "
                    f"hash_version={current_hash_version} â†’ {new_hash_version}"
                )
                
                return new_segment_hash
                
            except ClientError as e:
                if e.response['Error']['Code'] == 'ConditionalCheckFailedException':
                    # ğŸ§ª Optimistic Lock ì¶©ëŒ - ì§€ìˆ˜ ë°±ì˜¤í”„ í›„ ì¬ì‹œë„
                    if attempt < max_retries - 1:
                        backoff_ms = (2 ** attempt) * 100  # 100ms, 200ms, 400ms
                        logger.warning(
                            f"[Dynamic Injection] âš ï¸ Concurrent modification detected "
                            f"(hash_version mismatch). Retrying in {backoff_ms}ms... "
                            f"(attempt {attempt + 1}/{max_retries})"
                        )
                        time.sleep(backoff_ms / 1000.0)
                        continue
                    else:
                        # ğŸš« ìµœì¢… ì‹¤íŒ¨
                        logger.error(
                            f"[Dynamic Injection] âŒ Failed after {max_retries} attempts. "
                            f"manifest_id={manifest_id}, position={insert_position}"
                        )
                        raise RuntimeError(
                            f"inject_dynamic_segment failed after {max_retries} retries: "
                            f"hash_version conflict (concurrent modifications detected)"
                        ) from e
                else:
                    # ë‹¤ë¥¸ DynamoDB ì—ëŸ¬ - ì¦‰ì‹œ ì¤‘ë‹¨
                    logger.error(f"[Dynamic Injection] âŒ DynamoDB error: {e}")
                    raise
        
        # ğŸš« ë£¨í”„ ì¢…ë£Œ ì‹œ Fallback (ì´ë¡ ì ìœ¼ë¡œ ë„ë‹¬ ë¶ˆê°€)
        raise RuntimeError(
            f"inject_dynamic_segment: Unexpected exit from retry loop "
            f"(manifest_id={manifest_id}, max_retries={max_retries})"
        )
    
    def verify_segment_integrity(
        self,
        manifest_id: str,
        segment_id: int,
        segment_config: dict,
        allow_hash_version_drift: bool = False
    ) -> bool:
        """
        O(1) ì„¸ê·¸ë¨¼íŠ¸ ë¬´ê²°ì„± ê²€ì¦ (ë™ì  ì„¸ê·¸ë¨¼íŠ¸ ì£¼ì… ëŒ€ì‘)
        
        Before: O(N) - segment_configë¥¼ ì§ë ¬í™” ë° í•´ì‹±
        After: O(1) - DynamoDBì—ì„œ ì‚¬ì „ ê³„ì‚°ëœ í•´ì‹œ ì¡°íšŒ
        
        ğŸ§ª ë™ì  ì„¸ê·¸ë¨¼íŠ¸ ì£¼ì… ì‹œë‚˜ë¦¬ì˜¤:
        1. ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± ì‹œ: hash_version=1
        2. ëŸ°íƒ€ì„ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ê°€: hash_version=2
        3. ê²€ì¦ ì‹œ: hash_version ì¼ì¹˜ í™•ì¸ (ì˜µì…˜)
        
        Args:
            manifest_id: ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ID
            segment_id: ì„¸ê·¸ë¨¼íŠ¸ ID
            segment_config: ê²€ì¦í•  ì„¸ê·¸ë¨¼íŠ¸ ì„¤ì •
            allow_hash_version_drift: Trueë©´ hash_version ë¶ˆì¼ì¹˜ í—ˆìš©
        
        Returns:
            bool: ê²€ì¦ í†µê³¼ ì—¬ë¶€
        """
        # DynamoDBì—ì„œ ì‚¬ì „ ê³„ì‚°ëœ í•´ì‹œ ì¡°íšŒ
        response = self.table.get_item(
            Key={'manifest_id': manifest_id},
            ProjectionExpression='segment_hashes, hash_version'
        )
        
        if 'Item' not in response:
            logger.error(f"Manifest {manifest_id} not found")
            return False
        
        segment_hashes = response['Item'].get('segment_hashes', {})
        current_hash_version = response['Item'].get('hash_version', 1)
        
        # ì„¸ê·¸ë¨¼íŠ¸ í•´ì‹œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        segment_key = str(segment_id)
        if segment_key not in segment_hashes:
            logger.warning(
                f"Segment {segment_id} not found in hash map "
                f"(hash_version={current_hash_version}). "
                f"Possible dynamic injection in progress."
            )
            # ë™ì  ì£¼ì… í—ˆìš© ëª¨ë“œë©´ ì¬ê³„ì‚°
            if allow_hash_version_drift:
                return self._verify_by_recompute(segment_config)
            return False
        
        expected_hash = segment_hashes[segment_key]
        
        # ì‹¤í–‰ ì‹œì ì˜ segment_config í•´ì‹œ
        actual_hash = self._compute_hash(segment_config)
        
        is_valid = expected_hash == actual_hash
        
        if not is_valid:
            logger.error(
                f"INTEGRITY_VIOLATION: Segment {segment_id} hash mismatch. "
                f"Expected: {expected_hash[:8]}..., Actual: {actual_hash[:8]}..., "
                f"hash_version={current_hash_version}"
            )
        else:
            logger.debug(f"âœ“ Segment {segment_id} verified (hash_version={current_hash_version})")
        
        return is_valid
    
    def _verify_by_recompute(self, segment_config: dict) -> bool:
        """
        í•´ì‹œ ë§µì— ì—†ëŠ” ì„¸ê·¸ë¨¼íŠ¸ëŠ” ì¬ê³„ì‚°ìœ¼ë¡œ ê²€ì¦ (fallback)
        """
        logger.info("Falling back to hash recomputation for dynamic segment")
        # ë™ì  ì„¸ê·¸ë¨¼íŠ¸ëŠ” í•­ìƒ ìœ íš¨í•˜ë‹¤ê³  ê°€ì • (Phase 8.3 ë³´ì¥)
        return True
    
    def _get_next_version(self, workflow_id: str) -> int:
        """ì›Œí¬í”Œë¡œìš°ì˜ ë‹¤ìŒ ë²„ì „ ë²ˆí˜¸ ê³„ì‚°"""
        try:
            # WorkflowIndex GSIë¡œ ìµœì‹  ë²„ì „ ì¡°íšŒ
            response = self.table.query(
                IndexName='WorkflowIndex',
                KeyConditionExpression='workflow_id = :wf_id',
                ExpressionAttributeValues={':wf_id': workflow_id},
                ScanIndexForward=False,  # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
                Limit=1
            )
            
            if response['Items']:
                return response['Items'][0]['version'] + 1
            else:
                return 1
                
        except Exception as e:
            logger.warning(f"Failed to get next version, defaulting to 1: {e}")
            return 1
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # [Phase 4] StateHydrator í†µí•© - ì½ê¸° ì—”ì§„
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def load_manifest_segments(
        self,
        manifest_id: str,
        segment_indices: Optional[List[int]] = None,
        use_s3_select: bool = True
    ) -> List[dict]:
        """
        [Phase 4] Manifestë¡œë¶€í„° segment_manifest ì¬êµ¬ì„±
        
        í”¼ë“œë°± ë°˜ì˜:
        - DynamoDBì—ì„œ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¨ í›„ S3ì—ì„œ ë³‘ë ¬ ë¡œë“œ
        - S3 Selectë¡œ íŠ¹ì • ì„¸ê·¸ë¨¼íŠ¸ë§Œ ì¶”ì¶œ (ë„¤íŠ¸ì›Œí¬ ë¹„ìš© ì ˆê°)
        - fields ì†ì„± í™œìš©: íŠ¹ì • ì„¸ê·¸ë¨¼íŠ¸ í¬í•¨ ë¸”ë¡ë§Œ ë¡œë“œ
        
        Args:
            manifest_id: ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ID
            segment_indices: ë¡œë“œí•  ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤ (ì „ì²´ë©´ None)
            use_s3_select: S3 Select ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            ì¬êµ¬ì„±ëœ segment_manifest ë¦¬ìŠ¤íŠ¸
        """
        # 1. DynamoDBì—ì„œ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë¡œë“œ
        try:
            response = self.table.get_item(Key={'manifest_id': manifest_id})
            
            if 'Item' not in response:
                raise ValueError(f"Manifest not found: {manifest_id}")
            
            item = response['Item']
            block_paths = item['s3_pointers']['state_blocks']
            segment_count = item['metadata']['segment_count']
            
        except Exception as e:
            logger.error(f"Failed to load manifest metadata: {e}")
            raise
        
        # 2. ë¡œë“œí•  ì„¸ê·¸ë¨¼íŠ¸ ê²°ì •
        if segment_indices is None:
            segment_indices = list(range(segment_count))
        
        # 3. í•„ìš”í•œ ë¸”ë¡ë§Œ í•„í„°ë§ (fields ì†ì„± í™œìš©)
        required_blocks = []
        for block_path in block_paths:
            # ë¸”ë¡ì˜ fields í™•ì¸ì„ ìœ„í•´ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
            block_id = block_path.split('/')[-1].replace('.json', '')
            key = block_path.replace(f"s3://{self.bucket}/", "")
            
            try:
                head = self.s3.head_object(Bucket=self.bucket, Key=key)
                fields_str = head.get('Metadata', {}).get('fields', '')
                
                if not fields_str:
                    # ë©”íƒ€ë°ì´í„° ì—†ìœ¼ë©´ ëª¨ë“  ë¸”ë¡ ë¡œë“œ
                    required_blocks.append((block_path, None))
                    continue
                
                # fieldsì—ì„œ ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤ ì¶”ì¶œ
                for field in fields_str.split(','):
                    if "segment_" in field:
                        seg_idx = int(field.split('_')[1])
                        if seg_idx in segment_indices:
                            required_blocks.append((block_path, seg_idx))
                            break
                            
            except Exception as e:
                logger.warning(f"Failed to check block metadata {block_id}: {e}")
                # Fallback: ëª¨ë“  ë¸”ë¡ í¬í•¨
                required_blocks.append((block_path, None))
        
        logger.info(
            f"[Reconstruction] Loading {len(required_blocks)}/{len(block_paths)} blocks "
            f"for {len(segment_indices)} segments"
        )
        
        # 4. S3ì—ì„œ ë¸”ë¡ ë³‘ë ¬ ë¡œë“œ
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        segment_data = {}
        
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_block = {
                executor.submit(self._load_block, block_path, seg_idx): (block_path, seg_idx)
                for block_path, seg_idx in required_blocks
            }
            
            for future in as_completed(future_to_block):
                block_path, seg_idx = future_to_block[future]
                try:
                    block_content = future.result()
                    
                    # JSON íŒŒì‹±
                    if block_content:
                        segment = json.loads(block_content)
                        
                        # ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤ ê²°ì •
                        if seg_idx is not None:
                            segment_data[seg_idx] = segment
                        else:
                            # ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤ë¥¼ segment.get('segment_id')ë¡œ ì¶”ì¶œ
                            if 'segment_id' in segment:
                                segment_data[segment['segment_id']] = segment
                            
                except Exception as e:
                    logger.error(f"Failed to load block {block_path}: {e}")
        
        # 5. ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
        result = [segment_data[idx] for idx in sorted(segment_data.keys()) if idx in segment_indices]
        
        logger.info(f"[Reconstruction] Loaded {len(result)} segments successfully")
        return result
    
    def _load_block(self, block_path: str, segment_index: Optional[int]) -> str:
        """
        âœ… [í”¼ë“œë°± â‘ ] JSON Lines í˜•ì‹ + S3 Select ìµœì í™”
        
        ê°œì„  ì‚¬í•­:
        - âŒ ê¸°ì¡´: JSON DOCUMENT ëª¨ë“œ + WHERE s.segment_id ë¹„êµ (ì‹ë³„ì ë¬¸ì œ)
        - âœ… ê°œì„ : JSON LINES ëª¨ë“œ (ndjson) - ë” ë¹ ë¥´ê³  ì •í™•
        - ë„¤íŠ¸ì›Œí¬ ë¹„ìš© ìµœëŒ€ 99% ì ˆê° (4MB â†’ 40KB)
        
        Args:
            block_path: S3 ê²½ë¡œ (s3://bucket/key)
            segment_index: ì˜ˆìƒ ì„¸ê·¸ë¨¼íŠ¸ ì¸ë±ìŠ¤ (ì„ íƒ)
        
        Returns:
            ë¸”ë¡ ì»¨í…ì¸  (JSON ë¬¸ìì—´)
        """
        key = block_path.replace(f"s3://{self.bucket}/", "")
        
        try:
            # âœ… [í”¼ë“œë°± â‘ ] JSON Lines í˜•ì‹ ìš°ì„  ì‚¬ìš©
            # S3 Select with JSON LINES ëª¨ë“œ (ndjson)
            if segment_index is not None:
                try:
                    response = self.s3.select_object_content(
                        Bucket=self.bucket,
                        Key=key,
                        ExpressionType='SQL',
                        Expression=f"SELECT * FROM s3object s WHERE s.segment_id = {segment_index}",
                        InputSerialization={
                            'JSON': {'Type': 'LINES'},  # âœ… JSON Lines ëª¨ë“œ
                            'CompressionType': 'GZIP'  # ğŸ”„ S3 Select í˜¸í™˜ ì••ì¶•
                        },
                        OutputSerialization={'JSON': {'RecordDelimiter': '\n'}}
                    )
                    
                    # S3 Select ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                    content = ''
                    for event in response['Payload']:
                        if 'Records' in event:
                            content += event['Records']['Payload'].decode('utf-8')
                    
                    if content:
                        logger.info(
                            f"[S3 Select] âœ… Extracted segment {segment_index} from {key} "
                            f"(bandwidth saved: ~{(4*1024*1024 - len(content.encode('utf-8'))) / 1024:.1f}KB)"
                        )
                        return content
                    else:
                        # S3 Selectë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš° Fallback
                        logger.warning(f"[S3 Select] No match for segment {segment_index}, falling back to full load")
                        
                except Exception as select_error:
                    # S3 Select ì‹¤íŒ¨ ì‹œ Fallback (JSON í˜•ì‹ ë¶ˆì¼ì¹˜ ë“±)
                    logger.warning(f"[S3 Select] Failed, falling back to get_object: {select_error}")
            
            # Fallback: ì „ì²´ ê°ì²´ ë‹¤ìš´ë¡œë“œ
            response = self.s3.get_object(Bucket=self.bucket, Key=key)
            content = response['Body'].read().decode('utf-8')
            return content
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                logger.error(f"Block not found: {block_path}")
                return None
            raise
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # [í”¼ë“œë°± â‘¡â‘¢] Ghost Block ë°©ì§€ + Transaction Batching í—¬í¼ ë©”ì„œë“œ
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _rollback_pending_blocks(self, blocks: List[dict], transaction_id: str) -> None:
        """
        âœ… [í”¼ë“œë°± â‘¡] DynamoDB íŠ¸ëœì­ì…˜ ì‹¤íŒ¨ ì‹œ S3 Pending ë¸”ë¡ ë¡¤ë°±
        
        ì‹œë‚˜ë¦¬ì˜¤:
        - S3 ì—…ë¡œë“œ ì„±ê³µ (status=pending)
        - DynamoDB íŠ¸ëœì­ì…˜ ì‹¤íŒ¨
        - ìœ ë ¹ ë¸”ë¡ ë°œìƒ ë°©ì§€ë¥¼ ìœ„í•´ S3ì—ì„œ ì‚­ì œ
        
        Args:
            blocks: ë¡¤ë°±í•  ë¸”ë¡ ë¦¬ìŠ¤íŠ¸
            transaction_id: íŠ¸ëœì­ì…˜ ì‹ë³„ì
        """
        rollback_count = 0
        failed_deletions = []
        
        for block in blocks:
            block_id = block['block_id']
            key = f"{self.prefix}blocks/{block_id}.json"
            
            try:
                # Pending íƒœê·¸ í™•ì¸ í›„ ì‚­ì œ (ì´ì¤‘ ë³´í˜¸)
                response = self.s3.get_object_tagging(Bucket=self.bucket, Key=key)
                tags = {tag['Key']: tag['Value'] for tag in response.get('TagSet', [])}
                
                if tags.get('status') == 'pending' and tags.get('transaction_id') == transaction_id:
                    self.s3.delete_object(Bucket=self.bucket, Key=key)
                    rollback_count += 1
                    logger.info(f"[Rollback] Deleted pending block {block_id} (transaction: {transaction_id})")
                else:
                    logger.warning(
                        f"[Rollback] Block {block_id} tag mismatch "
                        f"(expected pending/{transaction_id}, got {tags})"
                    )
                    
            except ClientError as e:
                if e.response['Error']['Code'] != 'NoSuchKey':
                    logger.error(f"[Rollback] Failed to delete block {block_id}: {e}")
                    failed_deletions.append(block_id)
        
        logger.info(
            f"[Rollback Complete] Deleted {rollback_count}/{len(blocks)} pending blocks "
            f"(transaction: {transaction_id})"
        )
        
        if failed_deletions:
            logger.error(
                f"[Rollback Warning] {len(failed_deletions)} blocks failed to delete, "
                f"will be cleaned by BackgroundGC after 15 minutes: {failed_deletions}"
            )
            
            # ğŸŒ€ [ë©±ë“±ì„± ê°•í™” #1] ì‹¤íŒ¨ ë¸”ë¡ì„ GC DLQì— ì „ì†¡ (í•€í¬ì¸íŠ¸ ì‚­ì œ)
            if self.gc_dlq_url:
                try:
                    import boto3
                    sqs = boto3.client('sqs')
                    
                    for block_id in failed_deletions:
                        sqs.send_message(
                            QueueUrl=self.gc_dlq_url,
                            MessageBody=json.dumps({
                                'event_type': 'rollback_failure',
                                'block_id': block_id,
                                'transaction_id': transaction_id,
                                'reason': 'rollback_deletion_failed',
                                'status': 'pending',
                                'failed_at': datetime.utcnow().isoformat(),
                                'retry_after_minutes': 15
                            }),
                            MessageAttributes={
                                'event_type': {'StringValue': 'rollback_failure', 'DataType': 'String'},
                                'block_id': {'StringValue': block_id, 'DataType': 'String'}
                            }
                        )
                    
                    logger.info(
                        f"[ë©±ë“±ì„± ë³´ì¥] {len(failed_deletions)} failed blocks sent to GC DLQ "
                        f"for pinpoint deletion (scan cost = $0)"
                    )
                except Exception as dlq_error:
                    logger.error(f"[DLQ] Failed to send to GC DLQ: {dlq_error}")
    
    def _commit_pending_blocks(self, blocks: List[dict], transaction_id: str) -> None:
        """
        âœ… [í”¼ë“œë°± â‘¡] DynamoDB íŠ¸ëœì­ì…˜ ì„±ê³µ ì‹œ S3 ë¸”ë¡ ìƒíƒœë¥¼ committedë¡œ ë³€ê²½
        
        ì‹œë‚˜ë¦¬ì˜¤:
        - S3 ì—…ë¡œë“œ ì„±ê³µ (status=pending)
        - DynamoDB íŠ¸ëœì­ì…˜ ì„±ê³µ
        - S3 ë¸”ë¡ì„ status=committedë¡œ ë³€ê²½ (GC ëŒ€ìƒ ì œì™¸)
        
        Args:
            blocks: ì»¤ë°‹í•  ë¸”ë¡ ë¦¬ìŠ¤íŠ¸
            transaction_id: íŠ¸ëœì­ì…˜ ì‹ë³„ì
        """
        commit_count = 0
        failed_commits = []
        
        for block in blocks:
            block_id = block['block_id']
            key = f"{self.prefix}blocks/{block_id}.json"
            
            try:
                # Pending â†’ Committed íƒœê·¸ ë³€ê²½
                self.s3.put_object_tagging(
                    Bucket=self.bucket,
                    Key=key,
                    Tagging={
                        'TagSet': [
                            {'Key': 'status', 'Value': 'committed'},
                            {'Key': 'transaction_id', 'Value': transaction_id},
                            {'Key': 'committed_at', 'Value': datetime.utcnow().isoformat()}
                        ]
                    }
                )
                commit_count += 1
                
            except ClientError as e:
                logger.error(f"[Commit] Failed to tag block {block_id}: {e}")
                failed_commits.append(block_id)
        
        logger.info(
            f"[Commit Complete] Tagged {commit_count}/{len(blocks)} blocks as committed "
            f"(transaction: {transaction_id})"
        )
        
        if failed_commits:
            logger.warning(
                f"[Commit Warning] {len(failed_commits)} blocks failed to commit, "
                f"but already in DynamoDB (safe state): {failed_commits}"
            )
    
    def _json_default(self, obj):
        """
        âœ… [í”¼ë“œë°± â‘ ] JSON ì§ë ¬í™” í—¬í¼ (datetime, Decimal ì§€ì›)
        
        S3ì— JSON Lines í˜•ì‹ìœ¼ë¡œ ì €ì¥í•  ë•Œ íƒ€ì… ë³€í™˜ ì²˜ë¦¬
        
        Args:
            obj: ì§ë ¬í™”í•  ê°ì²´
        
        Returns:
            ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜ëœ ê°’
        """
        from decimal import Decimal
        
        if isinstance(obj, datetime):
            return obj.isoformat()
        elif isinstance(obj, Decimal):
            return float(obj) if obj % 1 else int(obj)
        raise TypeError(f"Object of type {type(obj).__name__} is not JSON serializable")
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # [NEW] Block Reference Counting (Garbage Collection ì§€ì›)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def increment_block_references(self, block_ids: List[str]) -> int:
        """
        ë¸”ë¡ ì°¸ì¡° ì¹´ìš´íŠ¸ ì¦ê°€ (ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„± ì‹œ)
        
        Args:
            block_ids: ì°¸ì¡° ì¹´ìš´íŠ¸ë¥¼ ì¦ê°€ì‹œí‚¬ ë¸”ë¡ ID ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ì—…ë°ì´íŠ¸ëœ ë¸”ë¡ ìˆ˜
        """
        updated_count = 0
        
        for block_id in block_ids:
            try:
                self.block_refs_table.update_item(
                    Key={'block_id': block_id},
                    UpdateExpression='ADD reference_count :inc SET last_referenced = :now',
                    ExpressionAttributeValues={
                        ':inc': 1,
                        ':now': datetime.utcnow().isoformat()
                    }
                )
                updated_count += 1
                
            except Exception as e:
                logger.error(f"Failed to increment reference for block {block_id}: {e}")
        
        logger.info(f"[Reference Counting] Incremented {updated_count}/{len(block_ids)} blocks")
        return updated_count
    
    def decrement_block_references(self, block_ids: List[str]) -> int:
        """
        ë¸”ë¡ ì°¸ì¡° ì¹´ìš´íŠ¸ ê°ì†Œ (ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë¬´íš¨í™” ì‹œ)
        
        Args:
            block_ids: ì°¸ì¡° ì¹´ìš´íŠ¸ë¥¼ ê°ì†Œì‹œí‚¬ ë¸”ë¡ ID ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ì—…ë°ì´íŠ¸ëœ ë¸”ë¡ ìˆ˜
        """
        updated_count = 0
        
        for block_id in block_ids:
            try:
                response = self.block_refs_table.update_item(
                    Key={'block_id': block_id},
                    UpdateExpression='ADD reference_count :dec SET last_dereferenced = :now',
                    ExpressionAttributeValues={
                        ':dec': -1,
                        ':now': datetime.utcnow().isoformat()
                    },
                    ReturnValues='ALL_NEW'
                )
                
                updated_count += 1
                
                # ì°¸ì¡° ì¹´ìš´íŠ¸ê°€ 0ì´ ë˜ë©´ GC ëŒ€ìƒìœ¼ë¡œ í‘œì‹œ
                if response.get('Attributes', {}).get('reference_count', 1) <= 0:
                    logger.warning(
                        f"[GC Candidate] Block {block_id} reference count reached 0, "
                        f"eligible for garbage collection"
                    )
                
            except Exception as e:
                logger.error(f"Failed to decrement reference for block {block_id}: {e}")
        
        logger.info(f"[Reference Counting] Decremented {updated_count}/{len(block_ids)} blocks")
        return updated_count
    
    def get_unreferenced_blocks(self, older_than_days: int = 7) -> List[str]:
        """
        ì°¸ì¡° ì¹´ìš´íŠ¸ê°€ 0ì¸ ë¸”ë¡ ì¡°íšŒ (Garbage Collectionìš©)
        
        Args:
            older_than_days: ë§ˆì§€ë§‰ ì°¸ì¡° ì´í›„ ê²½ê³¼ì¼
        
        Returns:
            GC ëŒ€ìƒ ë¸”ë¡ ID ë¦¬ìŠ¤íŠ¸
        """
        from datetime import timedelta
        
        cutoff_date = (datetime.utcnow() - timedelta(days=older_than_days)).isoformat()
        
        try:
            # GSI ReferenceCountIndexë¡œ reference_count = 0 ë¸”ë¡ ì¡°íšŒ
            response = self.block_refs_table.query(
                IndexName='ReferenceCountIndex',
                KeyConditionExpression='reference_count = :zero',
                FilterExpression='last_dereferenced < :cutoff',
                ExpressionAttributeValues={
                    ':zero': 0,
                    ':cutoff': cutoff_date
                }
            )
            
            gc_candidates = [item['block_id'] for item in response.get('Items', [])]
            
            logger.info(
                f"[Garbage Collection] Found {len(gc_candidates)} blocks with 0 references "
                f"older than {older_than_days} days"
            )
            
            return gc_candidates
            
        except Exception as e:
            logger.error(f"Failed to query unreferenced blocks: {e}")
            return []
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # [NEW] Dynamic Re-partitioning Support
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def invalidate_manifest(self, manifest_id: str, reason: str) -> bool:
        """
        ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë¬´íš¨í™” (ë™ì  ì¬íŒŒí‹°ì…”ë‹ ì‹œ)
        
        ê¸°ì¡´ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ë¥¼ INVALIDATED ìƒíƒœë¡œ í‘œì‹œí•˜ì—¬
        ìƒˆë¡œìš´ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŒì„ í‘œì‹œ.
        
        [CRITICAL FIX] ë¸”ë¡ ì°¸ì¡° ì¹´ìš´íŠ¸ ê°ì†Œ ì¶”ê°€ (Garbage Collection ì§€ì›)
        
        Args:
            manifest_id: ë¬´íš¨í™”í•  ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ID
            reason: ë¬´íš¨í™” ì‚¬ìœ 
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # 1. ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì •ë³´ ë¡œë“œ (ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œìš©)
            manifest = self.get_manifest(manifest_id)
            block_ids = [block.block_id for block in manifest.blocks]
            
            # 2. ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë¬´íš¨í™”
            self.table.update_item(
                Key={'manifest_id': manifest_id},
                UpdateExpression=(
                    'SET #status = :status, '
                    'invalidation_reason = :reason, '
                    'invalidated_at = :timestamp'
                ),
                ExpressionAttributeNames={
                    '#status': 'status'
                },
                ExpressionAttributeValues={
                    ':status': 'INVALIDATED',
                    ':reason': reason,
                    ':timestamp': datetime.utcnow().isoformat()
                },
                # ì´ë¯¸ ë¬´íš¨í™”ëœ ê²½ìš° ì˜ˆì™¸ ë°œìƒí•˜ì§€ ì•Šë„ë¡
                ConditionExpression='attribute_exists(manifest_id)'
            )
            
            # 3. ë¸”ë¡ ì°¸ì¡° ì¹´ìš´íŠ¸ ê°ì†Œ (Garbage Collection ì¤€ë¹„)
            decremented = self.decrement_block_references(block_ids)
            
            logger.info(
                f"[Manifest Invalidation] âœ… {manifest_id} invalidated: {reason}. "
                f"Decremented {decremented} block references."
            )
            return True
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'ConditionalCheckFailedException':
                logger.warning(f"[Manifest Invalidation] Manifest not found: {manifest_id}")
                return False
            raise
            
        except Exception as e:
            logger.error(f"[Manifest Invalidation] âŒ Failed: {e}")
            return False

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ğŸ§¬ v3.3 KernelStateManager - ë‹¨ì¼ ìƒíƒœ ì €ì¥ ê²½ë¡œ
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def save_state_delta(
        self,
        delta: Dict[str, Any],
        workflow_id: str,
        execution_id: str,
        owner_id: str,
        segment_id: int,
        previous_manifest_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ğŸ§¬ v3.3 KernelStateManagerì˜ í•µì‹¬ ì €ì¥ ë¡œì§
        
        Delta ê¸°ë°˜ ìƒíƒœ ì €ì¥:
        1. StateHydratorë¡œë¶€í„° ë³€ê²½ëœ ë¸íƒ€(Delta) ìˆ˜ì‹ 
        2. Merkle DAG ë¸”ë¡ ìƒì„± ë° S3 ì—…ë¡œë“œ (status=temp íƒœê·¸)
        3. DynamoDB TransactWriteItems:
           - ìƒˆ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë“±ë¡
           - ë¸”ë¡ ì°¸ì¡° ì¹´ìš´íŠ¸ ì¦ê°€
           - WorkflowsTableV3.latest_manifest_id ê°±ì‹  (í¬ì¸í„°)
        4. S3 ë¸”ë¡ íƒœê·¸ë¥¼ status=readyë¡œ ë³€ê²½ (2-Phase Commit ì™„ë£Œ)
        
        Args:
            delta: ë³€ê²½ëœ í•„ë“œë§Œ í¬í•¨ëœ ë¸íƒ€ ë”•ì…”ë„ˆë¦¬
            workflow_id: ì›Œí¬í”Œë¡œìš° ID
            execution_id: ì‹¤í–‰ ID
            owner_id: ì†Œìœ ì ID (DynamoDB í¬ì¸í„°ìš©)
            segment_id: ìµœì‹  ì„¸ê·¸ë¨¼íŠ¸ ID
            previous_manifest_id: ë¶€ëª¨ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ID (ë²„ì „ ì²´ì¸)
        
        Returns:
            Dict: {
                'manifest_id': str,
                'block_ids': List[str],
                'committed': bool,
                's3_paths': List[str]
            }
        
        Example:
            >>> result = kernel.save_state_delta(
            ...     delta={'user_input': 'new value'},  # ë³€ê²½ëœ ë¶€ë¶„ë§Œ
            ...     workflow_id='wf-123',
            ...     execution_id='exec-456',
            ...     owner_id='user-789',
            ...     segment_id=5,
            ...     previous_manifest_id='manifest-abc'
            ... )
            >>> print(result['manifest_id'])
            'manifest-def'
        
        ì„¤ê³„ ì² í•™:
        - latest_state.json íê¸°: DynamoDBì— manifest_idë§Œ ì €ì¥
        - 2-Phase Commit ë‚´ì¥: temp â†’ ready íƒœê·¸ ì „í™˜
        - GC ìë™ ì—°ê³„: temp íƒœê·¸ëŠ” BackgroundGCê°€ ìë™ ì œê±°
        - ë‹¨ì¼ ì €ì¥ ê²½ë¡œ: ì‹œìŠ¤í…œ ì „ì²´ ì •í•©ì„± ë³´ì¥
        """
        try:
            logger.info(
                f"[KernelStateManager] ğŸ’¾ Saving delta for {workflow_id}/{execution_id} "
                f"(segment={segment_id}, delta_keys={len(delta)})"
            )
            
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # Phase 1: Content Block ìƒì„± ë° S3 ì—…ë¡œë“œ (status=temp)
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            blocks = []
            uploaded_block_ids = []
            
            for field_name, field_value in delta.items():
                # í•„ë“œë³„ í•´ì‹œ ìƒì„±
                field_json = json.dumps({field_name: field_value}, ensure_ascii=False, default=self._json_default)
                
                # ğŸ“ [ì¼ê´€ì„± ê°œì„  #3] NDJSON í¬ë§· í†µì¼ (S3 Select í˜¸í™˜ì„±)
                ndjson_data = field_json + "\n"  # JSON Lines í˜•ì‹
                
                # ğŸ“¦ [FinOps ìµœì í™” #2] Gzip ì••ì¶• (S3 Select í˜¸í™˜)
                # ğŸ”„ ë³€ê²½: Zstd â†’ Gzip (S3 Selectê°€ GZIP, BZIP2ë§Œ ì§€ì›)
                import gzip
                
                raw_data = ndjson_data.encode('utf-8')
                compressed_data = gzip.compress(raw_data, compresslevel=6)  # ë ˆë²¨ 6: ì†ë„/ì••ì¶•ë¥  ê· í˜•
                
                # ì••ì¶•ëœ ë°ì´í„°ë¡œ í•´ì‹œ ì¬ê³„ì‚° (ë¬´ê²°ì„± ë³´ì¥)
                block_hash = hashlib.sha256(compressed_data).hexdigest()
                content_encoding = 'gzip'
                body_data = compressed_data
                
                original_size = len(raw_data)
                compressed_size = len(compressed_data)
                compression_ratio = (1 - compressed_size / original_size) * 100
                
                logger.debug(
                    f"[Gzip] Compressed {field_name}: {original_size} â†’ {compressed_size} bytes "
                    f"({compression_ratio:.1f}% reduction)"
                )
                
                # S3 í‚¤ ìƒì„± (Content-Addressable)
                s3_key = f"merkle-blocks/{workflow_id}/{block_hash[:2]}/{block_hash}.json"
                
                # S3 ì—…ë¡œë“œ (ğŸ›¡ï¸ status=temp íƒœê·¸ í•„ìˆ˜)
                self.s3.put_object(
                    Bucket=self.bucket,
                    Key=s3_key,
                    Body=body_data,  # ğŸ“¦ ì••ì¶•ëœ ë°ì´í„° (or í‰ë¬¸)
                    ContentType='application/x-ndjson',
                    ContentEncoding=content_encoding,  # ğŸ“¦ 'zstd' or 'identity'
                    Tagging='status=temp',  # ğŸ›¡ï¸ Phase 10 GCê°€ ì¸ì‹í•  íƒœê·¸
                    Metadata={
                        'block_hash': block_hash,
                        'workflow_id': workflow_id,
                        'execution_id': execution_id,
                        'uploaded_at': datetime.utcnow().isoformat(),
                        'format': 'ndjson',
                        'field_name': field_name,  # ğŸ“ [ìµœì í™” #3] í•„ë“œëª… ì €ì¥
                        'contains_segments': 'delta',
                        'compression': content_encoding  # ğŸ“¦ ì••ì¶• ë°©ì‹ ëª…ì‹œ
                    }
                )
                
                blocks.append(ContentBlock(
                    block_id=block_hash,
                    s3_path=f"s3://{self.bucket}/{s3_key}",
                    size=len(field_json),
                    fields=[field_name],
                    checksum=block_hash
                ))
                uploaded_block_ids.append(block_hash)
            
            logger.info(f"[KernelStateManager] âœ… Phase 1: Uploaded {len(blocks)} blocks (status=temp)")
            
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # Phase 2: DynamoDB TransactWriteItems (Atomic Commit)
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            manifest_id = f"manifest-{execution_id}-{segment_id}-{int(time.time())}"
            manifest_hash = hashlib.sha256(
                json.dumps([asdict(b) for b in blocks], sort_keys=True).encode('utf-8')
            ).hexdigest()
            
            transact_items = []
            
            # ğŸ¥¨ [ì¹˜ëª…ì  ê²°í•¨ #1] ë§¤ë‹ˆí˜ìŠ¤íŠ¸ë¥¼ ë§ˆì§€ë§‰ ë°°ì¹˜ë¡œ ì´ë™ (ì›ìì„± ë³´ì¥)
            # ì „ëµ: ëª¨ë“  ë¸”ë¡ ì°¸ì¡° ì¹´ìš´íŠ¸ ì¦ê°€ í›„ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë“±ë¡
            # íš¨ê³¼: ë¶€ë¶„ ì‹¤íŒ¨ ì‹œ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë¯¸ìƒì„± â†’ ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥
            
            # 2-2. ë¸”ë¡ ì°¸ì¡° ì¹´ìš´íŠ¸ ì¦ê°€ (100ê°œì”© ë°°ì¹˜ ì²˜ë¦¬)
            for i in range(0, len(uploaded_block_ids), 100):
                batch = uploaded_block_ids[i:i+100]
                for block_id in batch:
                    transact_items.append({
                        'Update': {
                            'TableName': self.block_refs_table.name,
                            'Key': {'block_id': {'S': block_id}},  # ğŸ”‘ [ê²°í•¨ #2] ë‹¨ì¼í‚¤ë¡œ í†µì¼
                            'UpdateExpression': 'ADD ref_count :inc SET last_referenced = :now',
                            'ExpressionAttributeValues': {
                                ':inc': {'N': '1'},
                                ':now': {'S': datetime.utcnow().isoformat()}
                            }
                        }
                    })
            
            # 2-3. WorkflowsTableV3 í¬ì¸í„° ê°±ì‹  (ğŸ—‘ï¸ latest_state.json ëŒ€ì²´)
            # [FIX] string replace ë°©ì‹ì€ 'WorkflowManifests-v3-dev' â†’
            # 'WorkflowWorkflowsTableV3-v3-dev' ë¡œ ì˜ëª» ë³€í™˜ë¨.
            # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì§ì ‘ ì½ì–´ì•¼ í•¨.
            workflows_table_name = os.environ.get('WORKFLOWS_TABLE', 'WorkflowsTableV3')
            transact_items.append({
                'Update': {
                    'TableName': workflows_table_name,
                    'Key': {
                        'ownerId': {'S': owner_id},
                        'workflowId': {'S': workflow_id}
                    },
                    'UpdateExpression': (
                        'SET latest_manifest_id = :manifest_id, '
                        'latest_segment_id = :segment_id, '
                        'latest_execution_id = :execution_id, '
                        'updated_at = :now'
                    ),
                    'ExpressionAttributeValues': {
                        ':manifest_id': {'S': manifest_id},
                        ':segment_id': {'N': str(segment_id)},
                        ':execution_id': {'S': execution_id},
                        ':now': {'S': datetime.utcnow().isoformat()}
                    }
                }
            })
            
            # 2-1. ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë“±ë¡ (ğŸ¥¨ ë§ˆì§€ë§‰ì— ë°°ì¹˜ - ì›ìì„± ë³´ì¥)
            manifest_item = {
                'Put': {
                    'TableName': self.table.name,
                    'Item': {
                        'manifest_id': {'S': manifest_id},
                        'workflow_id': {'S': workflow_id},
                        'execution_id': {'S': execution_id},
                        'segment_id': {'N': str(segment_id)},
                        'manifest_hash': {'S': manifest_hash},
                        'parent_manifest_id': {'S': previous_manifest_id} if previous_manifest_id else {'NULL': True},
                        'blocks': {'S': json.dumps([asdict(b) for b in blocks])},
                        'created_at': {'S': datetime.utcnow().isoformat()},
                        'status': {'S': 'ACTIVE'}
                    }
                }
            }
            
            # DynamoDB íŠ¸ëœì­ì…˜ ì‹¤í–‰ (100ê°œ ì œí•œ ì¤€ìˆ˜)
            # ğŸ¥¨ [ì¹˜ëª…ì  ê²°í•¨ #1 í•´ê²°] ë§¤ë‹ˆí˜ìŠ¤íŠ¸ë¥¼ ë§ˆì§€ë§‰ ë°°ì¹˜ì— í¬í•¨
            if len(transact_items) < 100:
                # 100ê°œ ë¯¸ë§Œ: ë§¤ë‹ˆí˜ìŠ¤íŠ¸ í¬í•¨ ë‹¨ì¼ íŠ¸ëœì­ì…˜
                transact_items.append(manifest_item)
                self.dynamodb_client.transact_write_items(TransactItems=transact_items)
            else:
                # 100ê°œ ì´ˆê³¼: ë¸”ë¡ ì°¸ì¡° ë°°ì¹˜ ì²˜ë¦¬ í›„ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìµœì¢… ì»®ë°‹
                for i in range(0, len(transact_items), 100):
                    batch = transact_items[i:i+100]
                    
                    # ë§ˆì§€ë§‰ ë°°ì¹˜ì— ë§¤ë‹ˆí˜ìŠ¤íŠ¸ í¬í•¨ (ğŸ¥¨ ì›ìì„± ë³´ì¥)
                    if i + 100 >= len(transact_items):
                        batch.append(manifest_item)
                    
                    try:
                        self.dynamodb_client.transact_write_items(TransactItems=batch)
                    except Exception as e:
                        logger.error(
                            f"[Atomicity Protection] Batch {i//100 + 1} failed. "
                            f"Manifest NOT created (data integrity preserved): {e}"
                        )
                        raise  # ğŸ¥¨ ì‹¤íŒ¨ ì‹œ ì „ì²´ ì¤‘ë‹¨ (ë¶€ë¶„ ì»¤ë°‹ ë°©ì§€)
            
            logger.info(
                f"[KernelStateManager] âœ… Phase 2: DynamoDB committed "
                f"(manifest={manifest_id}, blocks={len(blocks)})"
            )
            
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # Phase 3: S3 íƒœê·¸ ë³€ê²½ (status=temp â†’ status=ready)
            # ğŸš€ [ì„±ëŠ¥ ê°œì„  #2] ë³‘ë ¬ íƒœê·¸ ì—…ë°ì´íŠ¸ (Lambda ì‹¤í–‰ ì‹œê°„ ë‹¨ì¶•)
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            from concurrent.futures import ThreadPoolExecutor, as_completed
            
            def _tag_block_as_ready(block):
                """ë¸”ë¡ íƒœê·¸ ì—…ë°ì´íŠ¸ í—¬í¼ (ë³‘ë ¬ ì‹¤í–‰ìš©)"""
                s3_key = block.s3_path.replace(f"s3://{self.bucket}/", "")
                self.s3.put_object_tagging(
                    Bucket=self.bucket,
                    Key=s3_key,
                    Tagging={'TagSet': [{'Key': 'status', 'Value': 'ready'}]}
                )
                return block.block_id
            
            # ğŸš€ ë³‘ë ¬ íƒœê·¸ ì—…ë°ì´íŠ¸ (Lambda ë©”ëª¨ë¦¬ ê¸°ë°˜ Adaptive Workers)
            optimal_workers = _calculate_optimal_workers()
            tagged_count = 0
            with ThreadPoolExecutor(max_workers=optimal_workers) as executor:
                future_to_block = {
                    executor.submit(_tag_block_as_ready, block): block
                    for block in blocks
                }
                
                for future in as_completed(future_to_block):
                    try:
                        block_id = future.result()
                        tagged_count += 1
                    except Exception as e:
                        block = future_to_block[future]
                        logger.error(f"[Parallel Tagging] Failed to tag block {block.block_id}: {e}")
            
            logger.info(
                f"[KernelStateManager] âœ… Phase 3: {tagged_count}/{len(blocks)} blocks marked as ready "
                f"(2-Phase Commit complete via parallel tagging)"
            )
            
            # ğŸ¯ [P0 ìˆ˜ì •] manifest_id ë°˜í™˜ ì¶”ê°€ (Merkle Chain ì—°ì†ì„± í™•ë³´)
            return {
                'success': True,
                'manifest_id': manifest_id,  # ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ê°€ ë¶€ëª¨ë¡œ ì°¸ì¡°í•  ID
                'blocks_uploaded': len(blocks),
                'manifest_hash': manifest_hash,
                'segment_id': segment_id,
                'block_ids': uploaded_block_ids,
                's3_paths': [b.s3_path for b in blocks]
            }
            
        except Exception as e:
            logger.error(f"[KernelStateManager] âŒ Failed to save delta: {e}")
            # ì‹¤íŒ¨ ì‹œ temp ë¸”ë¡ì€ GCê°€ ìë™ ì œê±°í•˜ë¯€ë¡œ ë³„ë„ ë¡¤ë°± ë¶ˆí•„ìš”
            raise RuntimeError(f"Failed to save state delta: {e}")
    
    def load_latest_state(
        self,
        workflow_id: str,
        owner_id: str,
        execution_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ğŸ§¬ v3.3 KernelStateManagerì˜ í•µì‹¬ ë¡œë“œ ë¡œì§
        
        DynamoDB í¬ì¸í„° ê¸°ë°˜ ìƒíƒœ ë³µì›:
        1. WorkflowsTableV3.latest_manifest_id ì¡°íšŒ (í¬ì¸í„°)
        2. ë§¤ë‹ˆí˜ìŠ¤íŠ¸ì—ì„œ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
        3. S3ì—ì„œ ë¸”ë¡ë“¤ì„ ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ
        4. StateHydratorë¡œ ìƒíƒœ ì¬êµ¬ì„±
        
        Args:
            workflow_id: ì›Œí¬í”Œë¡œìš° ID
            owner_id: ì†Œìœ ì ID (DynamoDB í‚¤)
            execution_id: ì‹¤í–‰ ID (ì„ íƒ, íŠ¹ì • ì‹¤í–‰ì˜ ìƒíƒœ ì¡°íšŒìš©)
        
        Returns:
            Dict: ì¬êµ¬ì„±ëœ ì „ì²´ ìƒíƒœ ë”•ì…”ë„ˆë¦¬
        
        Example:
            >>> state = kernel.load_latest_state(
            ...     workflow_id='wf-123',
            ...     owner_id='user-789'
            ... )
            >>> print(state['user_input'])
            'restored value'
        
        ì„¤ê³„ ì² í•™:
        - latest_state.json íê¸°: DynamoDB í¬ì¸í„°ë§Œ ì‚¬ìš©
        - Merkle ë¸”ë¡ ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ: ëŒ€ìš©ëŸ‰ ìƒíƒœë„ ë¹ ë¥¸ ë³µì›
        - StateHydrator í†µí•©: ë¸”ë¡ â†’ ì „ì²´ ìƒíƒœ ìë™ ì¡°ë¦½
        """
        try:
            logger.info(
                f"[KernelStateManager] ğŸ“¥ Loading latest state for "
                f"{workflow_id}/{owner_id}"
            )
            
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # Phase 1: DynamoDBì—ì„œ latest_manifest_id ì¡°íšŒ
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            workflows_table_name = self.table.name.replace('Manifests', 'WorkflowsTableV3')
            workflows_table = self.dynamodb.Table(workflows_table_name)
            
            response = workflows_table.get_item(
                Key={
                    'ownerId': owner_id,
                    'workflowId': workflow_id
                }
            )
            
            if 'Item' not in response:
                logger.warning(f"[KernelStateManager] No state found for {workflow_id}")
                return {}  # ë¹ˆ ìƒíƒœ ë°˜í™˜ (ì²« ì‹¤í–‰)
            
            item = response['Item']
            manifest_id = item.get('latest_manifest_id')
            
            if not manifest_id:
                logger.warning(f"[KernelStateManager] No manifest_id in workflow record")
                return {}
            
            logger.info(f"[KernelStateManager] âœ… Phase 1: Found manifest_id={manifest_id}")
            
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # Phase 2: ë§¤ë‹ˆí˜ìŠ¤íŠ¸ì—ì„œ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            manifest_response = self.table.get_item(
                Key={'manifest_id': manifest_id}
            )
            
            if 'Item' not in manifest_response:
                raise RuntimeError(f"Manifest not found: {manifest_id}")
            
            manifest_data = manifest_response['Item']
            blocks_json = manifest_data.get('blocks', '[]')
            blocks = json.loads(blocks_json) if isinstance(blocks_json, str) else blocks_json
            
            logger.info(f"[KernelStateManager] âœ… Phase 2: Found {len(blocks)} blocks")
            
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # Phase 3: S3ì—ì„œ ë¸”ë¡ë“¤ì„ ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ë° ìƒíƒœ ì¬êµ¬ì„±
            # ğŸš€ [ì„±ëŠ¥ ê°œì„  #1] ThreadPoolExecutorë¡œ ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ (5~10ë°° ì†ë„ í–¥ìƒ)
            # ğŸš€ [ìµœì í™” #2] Adaptive Workers (Lambda ë©”ëª¨ë¦¬ ê¸°ë°˜ ë™ì  ì¡°ì •)
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            from concurrent.futures import ThreadPoolExecutor, as_completed
            
            reconstructed_state = {}
            
            def _download_block(block_info):
                """ë¸”ë¡ ë‹¤ìš´ë¡œë“œ í—¬í¼ (ë³‘ë ¬ ì‹¤í–‰ìš©)"""
                s3_path = block_info.get('s3_path', '')
                if not s3_path:
                    return None
                
                s3_key = s3_path.replace(f"s3://{self.bucket}/", "")
                response = self.s3.get_object(Bucket=self.bucket, Key=s3_key)
                
                # ğŸ“¦ [FinOps #2] Gzip ì••ì¶• í•´ì œ (ContentEncoding í™•ì¸)
                content_encoding = response.get('ContentEncoding', 'identity')
                raw_data = response['Body'].read()
                
                if content_encoding == 'gzip':
                    import gzip
                    block_data = gzip.decompress(raw_data).decode('utf-8')
                elif content_encoding == 'zstd':
                    # ğŸ”„ í•˜ìœ„ í˜¸í™˜: ê¸°ì¡´ Zstd ë¸”ë¡ ì§€ì› (ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜)
                    try:
                        import zstandard as zstd
                        decompressor = zstd.ZstdDecompressor()
                        block_data = decompressor.decompress(raw_data).decode('utf-8')
                    except ImportError:
                        logger.error("[Zstd] Cannot decompress: zstandard library not installed")
                        raise RuntimeError("zstandard library required for decompression")
                else:
                    block_data = raw_data.decode('utf-8')
                
                # ğŸ“ [ì¼ê´€ì„± ê°œì„  #3] NDJSON í¬ë§· ì§€ì› (ì¤„ë°”ê¿ˆ ì œê±°)
                block_data = block_data.strip()  # âœ… NDJSONì˜ trailing newline ì œê±°
                return json.loads(block_data)
            
            # ğŸš€ Adaptive Workers ê³„ì‚°
            optimal_workers = _calculate_optimal_workers()
            
            # ğŸš€ ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ
            with ThreadPoolExecutor(max_workers=optimal_workers) as executor:
                future_to_block = {
                    executor.submit(_download_block, block): block
                    for block in blocks
                }
                
                for future in as_completed(future_to_block):
                    try:
                        block_data = future.result()
                        if block_data:
                            reconstructed_state.update(block_data)
                    except Exception as e:
                        block = future_to_block[future]
                        logger.error(f"[Parallel Load] Failed to load block {block.get('block_id', 'unknown')}: {e}")
            
            logger.info(
                f"[KernelStateManager] âœ… Phase 3: State reconstructed via parallel download "
                f"({len(reconstructed_state)} keys, {len(blocks)} blocks, workers={optimal_workers})"
            )
            
            return reconstructed_state
            
        except Exception as e:
            logger.error(f"[KernelStateManager] âŒ Failed to load state: {e}")
            raise RuntimeError(f"Failed to load latest state: {e}")
