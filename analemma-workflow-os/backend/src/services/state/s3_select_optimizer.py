# -*- coding: utf-8 -*-
"""
[Phase 4] S3 Select ìµœì í™” ì„œë¹„ìŠ¤

í•µì‹¬ ê¸°ëŠ¥:
1. CloudWatch ê¸°ë°˜ ë™ì  ì„ê³„ê°’ íŠœë‹ (ë ˆì´í„´ì‹œ ì§€í„° ëŒ€ì‘)
2. í•„ë“œë³„ ì„ íƒì  ë¡œë”© (ë„¤íŠ¸ì›Œí¬ ë¹„ìš© 80% ì ˆê°)
3. GetObject vs S3 Select ìë™ ì„ íƒ
"""

import time
import logging
from typing import Dict, List, Optional, Any, Set
from datetime import datetime, timedelta
from dataclasses import dataclass

import boto3
from botocore.exceptions import ClientError

logger = logging.getLogger(__name__)

# ì´ˆê¸° ì„ê³„ê°’ (CloudWatch ë©”íŠ¸ë¦­ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ì¡°ì •)
DEFAULT_SIZE_THRESHOLD = 10 * 1024  # 10KB
DEFAULT_FIELD_RATIO_THRESHOLD = 0.2  # 20% ì´í•˜ í•„ë“œë§Œ í•„ìš”í•˜ë©´ S3 Select


@dataclass
class S3SelectMetrics:
    """S3 Select ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    avg_select_latency_ms: float
    avg_getobject_latency_ms: float
    select_success_rate: float
    recommended_threshold_kb: int
    last_updated: datetime


class S3SelectOptimizer:
    """
    S3 Select ë™ì  ìµœì í™” ì„œë¹„ìŠ¤
    
    í”¼ë“œë°± ë°˜ì˜:
    - S3 Select ì¿¼ë¦¬ íŒŒì‹± ì˜¤ë²„í—¤ë“œëŠ” ë°ì´í„° ì–‘ì— ë”°ë¼ ë³€ë™
    - ì‘ì€ ê°ì²´(<10KB)ëŠ” GetObjectê°€ ë” ë¹ ë¥¼ ìˆ˜ ìˆìŒ
    - CloudWatch ë©”íŠ¸ë¦­ ê¸°ë°˜ ë™ì  ì¡°ì • í•„ìš”
    """
    
    def __init__(
        self,
        s3_client=None,
        cloudwatch_client=None,
        enable_dynamic_tuning: bool = True
    ):
        self.s3 = s3_client or boto3.client('s3')
        self.cloudwatch = cloudwatch_client or boto3.client('cloudwatch')
        self.enable_dynamic_tuning = enable_dynamic_tuning
        
        # í˜„ì¬ ì„ê³„ê°’ (ë™ì  ì¡°ì •ë¨)
        self.size_threshold = DEFAULT_SIZE_THRESHOLD
        self.field_ratio_threshold = DEFAULT_FIELD_RATIO_THRESHOLD
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìºì‹œ (5ë¶„ TTL)
        self._metrics_cache: Optional[S3SelectMetrics] = None
        self._metrics_cache_expiry = 0
    
    def should_use_s3_select(
        self,
        object_size: int,
        total_fields: int,
        required_fields: int
    ) -> bool:
        """
        S3 Select ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        
        Decision Tree:
        1. object_size < threshold â†’ GetObject (ë¹ ë¦„)
        2. required_fields / total_fields > 80% â†’ GetObject (ëŒ€ë¶€ë¶„ í•„ìš”)
        3. required_fields / total_fields < 20% â†’ S3 Select (ì¼ë¶€ë§Œ í•„ìš”)
        4. ë‚˜ë¨¸ì§€ â†’ CloudWatch ë©”íŠ¸ë¦­ ê¸°ë°˜ íŒë‹¨
        
        Args:
            object_size: S3 ê°ì²´ í¬ê¸° (bytes)
            total_fields: ì „ì²´ í•„ë“œ ìˆ˜
            required_fields: í•„ìš”í•œ í•„ë“œ ìˆ˜
        
        Returns:
            Trueë©´ S3 Select ì‚¬ìš©
        """
        # 1. ì‘ì€ ê°ì²´ëŠ” ë¬´ì¡°ê±´ GetObject
        if object_size < self.size_threshold:
            logger.debug(
                f"[S3SelectOptimizer] GetObject: size={object_size}B < threshold={self.size_threshold}B"
            )
            return False
        
        # 2. í•„ë“œ ë¹„ìœ¨ ê³„ì‚°
        field_ratio = required_fields / max(total_fields, 1)
        
        # ëŒ€ë¶€ë¶„ í•„ë“œ í•„ìš” â†’ GetObject
        if field_ratio > 0.8:
            logger.debug(
                f"[S3SelectOptimizer] GetObject: field_ratio={field_ratio:.2f} > 0.8 "
                f"({required_fields}/{total_fields})"
            )
            return False
        
        # ì¼ë¶€ í•„ë“œë§Œ í•„ìš” â†’ S3 Select
        if field_ratio < self.field_ratio_threshold:
            logger.debug(
                f"[S3SelectOptimizer] S3 Select: field_ratio={field_ratio:.2f} < {self.field_ratio_threshold} "
                f"({required_fields}/{total_fields})"
            )
            return True
        
        # 3. ë™ì  íŠœë‹: CloudWatch ë©”íŠ¸ë¦­ ê¸°ë°˜ íŒë‹¨
        if self.enable_dynamic_tuning:
            metrics = self._get_or_refresh_metrics()
            
            if metrics:
                # S3 Selectê°€ 2ë°° ì´ìƒ ëŠë¦¬ë©´ GetObject ì„ í˜¸
                if metrics.avg_select_latency_ms > metrics.avg_getobject_latency_ms * 2:
                    logger.info(
                        f"[S3SelectOptimizer] GetObject: Select latency too high "
                        f"({metrics.avg_select_latency_ms:.1f}ms vs {metrics.avg_getobject_latency_ms:.1f}ms)"
                    )
                    return False
        
        # ê¸°ë³¸ê°’: S3 Select ì‚¬ìš©
        logger.debug(
            f"[S3SelectOptimizer] S3 Select: default choice for size={object_size}B, "
            f"field_ratio={field_ratio:.2f}"
        )
        return True
    
    def load_with_s3_select(
        self,
        bucket: str,
        key: str,
        fields_to_extract: Set[str]
    ) -> Dict[str, Any]:
        """
        S3 Selectë¡œ í•„ë“œë³„ ì„ íƒì  ë¡œë”©
        
        Args:
            bucket: S3 ë²„í‚·
            key: S3 í‚¤
            fields_to_extract: ì¶”ì¶œí•  í•„ë“œëª… ì§‘í•©
        
        Returns:
            ì¶”ì¶œëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        try:
            # SQL ì¿¼ë¦¬ ìƒì„±: SELECT s.field1, s.field2 FROM S3Object[*] s
            select_fields = ", ".join([f"s.{field}" for field in fields_to_extract])
            sql_query = f"SELECT {select_fields} FROM S3Object[*] s"
            
            logger.info(
                f"[S3 Select] Query: {sql_query[:100]}... "
                f"(extracting {len(fields_to_extract)} fields)"
            )
            
            response = self.s3.select_object_content(
                Bucket=bucket,
                Key=key,
                ExpressionType='SQL',
                Expression=sql_query,
                InputSerialization={
                    'JSON': {'Type': 'DOCUMENT'},
                    'CompressionType': 'GZIP'  # ğŸ”„ v3.3 KernelStateManager í˜¸í™˜
                },
                OutputSerialization={'JSON': {'RecordDelimiter': '\n'}}
            )
            
            # ìŠ¤íŠ¸ë¦¼ì—ì„œ ê²°ê³¼ ìˆ˜ì§‘
            result_data = {}
            for event in response['Payload']:
                if 'Records' in event:
                    records = event['Records']['Payload'].decode('utf-8')
                    for line in records.strip().split('\n'):
                        if line:
                            import json
                            result_data.update(json.loads(line))
            
            latency_ms = (time.time() - start_time) * 1000
            
            # CloudWatch ë©”íŠ¸ë¦­ ê¸°ë¡
            self._record_metric('S3Select', latency_ms, True)
            
            logger.info(
                f"[S3 Select] Success: {len(result_data)} fields loaded in {latency_ms:.1f}ms"
            )
            
            return result_data
            
        except Exception as e:
            latency_ms = (time.time() - start_time) * 1000
            
            # ì‹¤íŒ¨ ë©”íŠ¸ë¦­ ê¸°ë¡
            self._record_metric('S3Select', latency_ms, False)
            
            logger.error(f"[S3 Select] Failed after {latency_ms:.1f}ms: {e}")
            raise
    
    def load_with_getobject(
        self,
        bucket: str,
        key: str,
        fields_to_extract: Optional[Set[str]] = None
    ) -> Dict[str, Any]:
        """
        GetObjectë¡œ ì „ì²´ ë¡œë”© í›„ í•„í„°ë§
        
        Args:
            bucket: S3 ë²„í‚·
            key: S3 í‚¤
            fields_to_extract: ì¶”ì¶œí•  í•„ë“œëª… (í•„í„°ë§ìš©, Noneì´ë©´ ì „ì²´)
        
        Returns:
            ë¡œë“œëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        
        try:
            response = self.s3.get_object(Bucket=bucket, Key=key)
            content = response['Body'].read().decode('utf-8')
            
            import json
            data = json.loads(content)
            
            # í•„ë“œ í•„í„°ë§
            if fields_to_extract:
                data = {k: v for k, v in data.items() if k in fields_to_extract}
            
            latency_ms = (time.time() - start_time) * 1000
            
            # CloudWatch ë©”íŠ¸ë¦­ ê¸°ë¡
            self._record_metric('GetObject', latency_ms, True)
            
            logger.info(
                f"[GetObject] Success: {len(data)} fields loaded in {latency_ms:.1f}ms"
            )
            
            return data
            
        except Exception as e:
            latency_ms = (time.time() - start_time) * 1000
            
            # ì‹¤íŒ¨ ë©”íŠ¸ë¦­ ê¸°ë¡
            self._record_metric('GetObject', latency_ms, False)
            
            logger.error(f"[GetObject] Failed after {latency_ms:.1f}ms: {e}")
            raise
    
    def smart_load(
        self,
        bucket: str,
        key: str,
        fields_to_extract: Optional[Set[str]] = None
    ) -> Dict[str, Any]:
        """
        ìë™ ìµœì í™” ë¡œë”© (GetObject vs S3 Select ì„ íƒ)
        
        Args:
            bucket: S3 ë²„í‚·
            key: S3 í‚¤
            fields_to_extract: ì¶”ì¶œí•  í•„ë“œëª… (Noneì´ë©´ ì „ì²´)
        
        Returns:
            ë¡œë“œëœ ë°ì´í„°
        """
        # ê°ì²´ í¬ê¸° í™•ì¸
        try:
            head = self.s3.head_object(Bucket=bucket, Key=key)
            object_size = head['ContentLength']
        except Exception as e:
            logger.warning(f"Failed to get object size: {e}, using GetObject")
            return self.load_with_getobject(bucket, key, fields_to_extract)
        
        # ì „ì²´ ë¡œë“œë©´ ë¬´ì¡°ê±´ GetObject
        if not fields_to_extract:
            return self.load_with_getobject(bucket, key, None)
        
        # ì „ì²´ í•„ë“œ ìˆ˜ ì¶”ì • (ë©”íƒ€ë°ì´í„°ì—ì„œ)
        total_fields = head.get('Metadata', {}).get('total_fields', 100)
        if isinstance(total_fields, str):
            total_fields = int(total_fields)
        
        # S3 Select ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        use_select = self.should_use_s3_select(
            object_size=object_size,
            total_fields=total_fields,
            required_fields=len(fields_to_extract)
        )
        
        if use_select:
            try:
                return self.load_with_s3_select(bucket, key, fields_to_extract)
            except Exception as e:
                logger.warning(f"S3 Select failed, falling back to GetObject: {e}")
                return self.load_with_getobject(bucket, key, fields_to_extract)
        else:
            return self.load_with_getobject(bucket, key, fields_to_extract)
    
    def _get_or_refresh_metrics(self) -> Optional[S3SelectMetrics]:
        """
        CloudWatch ë©”íŠ¸ë¦­ ìºì‹œ ì¡°íšŒ ë˜ëŠ” ê°±ì‹ 
        
        Returns:
            ì„±ëŠ¥ ë©”íŠ¸ë¦­ (ì‹¤íŒ¨ ì‹œ None)
        """
        now = time.time()
        
        # ìºì‹œ ìœ íš¨í•˜ë©´ ë°˜í™˜
        if self._metrics_cache and now < self._metrics_cache_expiry:
            return self._metrics_cache
        
        # CloudWatchì—ì„œ ìµœê·¼ 1ì¼ ë©”íŠ¸ë¦­ ì¡°íšŒ
        try:
            end_time = datetime.utcnow()
            start_time = end_time - timedelta(days=1)
            
            # S3 Select í‰ê·  ë ˆì´í„´ì‹œ
            select_response = self.cloudwatch.get_metric_statistics(
                Namespace='Analemma/S3',
                MetricName='S3SelectLatency',
                Dimensions=[],
                StartTime=start_time,
                EndTime=end_time,
                Period=3600,  # 1ì‹œê°„ ë‹¨ìœ„
                Statistics=['Average']
            )
            
            # GetObject í‰ê·  ë ˆì´í„´ì‹œ
            getobject_response = self.cloudwatch.get_metric_statistics(
                Namespace='Analemma/S3',
                MetricName='GetObjectLatency',
                Dimensions=[],
                StartTime=start_time,
                EndTime=end_time,
                Period=3600,
                Statistics=['Average']
            )
            
            # í‰ê·  ê³„ì‚°
            select_latencies = [dp['Average'] for dp in select_response.get('Datapoints', [])]
            getobject_latencies = [dp['Average'] for dp in getobject_response.get('Datapoints', [])]
            
            if not select_latencies or not getobject_latencies:
                logger.debug("No CloudWatch metrics available yet")
                return None
            
            avg_select = sum(select_latencies) / len(select_latencies)
            avg_getobject = sum(getobject_latencies) / len(getobject_latencies)
            
            # ì¶”ì²œ ì„ê³„ê°’ ê³„ì‚°
            # S3 Selectê°€ 2ë°° ì´ìƒ ëŠë¦¬ë©´ ì„ê³„ê°’ ìƒí–¥ (50KB)
            if avg_select > avg_getobject * 2:
                recommended_threshold = 50 * 1024
            else:
                recommended_threshold = 10 * 1024
            
            # ë©”íŠ¸ë¦­ ìºì‹œ ì—…ë°ì´íŠ¸
            self._metrics_cache = S3SelectMetrics(
                avg_select_latency_ms=avg_select,
                avg_getobject_latency_ms=avg_getobject,
                select_success_rate=1.0,  # TODO: ì„±ê³µë¥  ë©”íŠ¸ë¦­ ì¶”ê°€
                recommended_threshold_kb=recommended_threshold // 1024,
                last_updated=datetime.utcnow()
            )
            
            # ì„ê³„ê°’ ìë™ ì¡°ì •
            self.size_threshold = recommended_threshold
            
            # ìºì‹œ ë§Œë£Œ ì‹œê°„ ì„¤ì • (5ë¶„)
            self._metrics_cache_expiry = now + 300
            
            logger.info(
                f"[CloudWatch] Metrics updated: Select={avg_select:.1f}ms, "
                f"GetObject={avg_getobject:.1f}ms, "
                f"New threshold={recommended_threshold // 1024}KB"
            )
            
            return self._metrics_cache
            
        except Exception as e:
            logger.warning(f"Failed to fetch CloudWatch metrics: {e}")
            return None
    
    def _record_metric(self, operation: str, latency_ms: float, success: bool):
        """
        CloudWatchì— ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
        
        Args:
            operation: 'S3Select' ë˜ëŠ” 'GetObject'
            latency_ms: ë ˆì´í„´ì‹œ (ë°€ë¦¬ì´ˆ)
            success: ì„±ê³µ ì—¬ë¶€
        """
        try:
            metric_name = f"{operation}Latency"
            
            self.cloudwatch.put_metric_data(
                Namespace='Analemma/S3',
                MetricData=[
                    {
                        'MetricName': metric_name,
                        'Value': latency_ms,
                        'Unit': 'Milliseconds',
                        'Timestamp': datetime.utcnow()
                    },
                    {
                        'MetricName': f"{operation}Success",
                        'Value': 1.0 if success else 0.0,
                        'Unit': 'None',
                        'Timestamp': datetime.utcnow()
                    }
                ]
            )
            
            logger.debug(f"[CloudWatch] Recorded {metric_name}={latency_ms:.1f}ms, success={success}")
            
        except Exception as e:
            # ë©”íŠ¸ë¦­ ê¸°ë¡ ì‹¤íŒ¨ëŠ” criticalí•˜ì§€ ì•ŠìŒ
            logger.debug(f"Failed to record CloudWatch metric: {e}")


# Singleton ì¸ìŠ¤í„´ìŠ¤
_optimizer = S3SelectOptimizer()


def smart_load_from_s3(
    bucket: str,
    key: str,
    fields: Optional[Set[str]] = None
) -> Dict[str, Any]:
    """
    S3ì—ì„œ ë°ì´í„° ìŠ¤ë§ˆíŠ¸ ë¡œë”© (í¸ì˜ í•¨ìˆ˜)
    
    Args:
        bucket: S3 ë²„í‚·
        key: S3 í‚¤
        fields: í•„ìš”í•œ í•„ë“œëª… ì§‘í•© (Noneì´ë©´ ì „ì²´)
    
    Returns:
        ë¡œë“œëœ ë°ì´í„°
    """
    return _optimizer.smart_load(bucket, key, fields)
