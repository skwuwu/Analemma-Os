"""
Segment Field Optimizer - ì„¸ê·¸ë¨¼íŠ¸ë³„ í•„ìš” í•„ë“œë§Œ ì „ë‹¬

ê° ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì…ì— ë”°ë¼ ì‹¤ì œ í•„ìš”í•œ í•„ë“œë§Œ ì¶”ì¶œí•˜ì—¬ 
payload í¬ê¸°ë¥¼ 70-80% ê°ì†Œì‹œí‚µë‹ˆë‹¤.
"""

from typing import Dict, Any, Set, Optional
import logging
import json

logger = logging.getLogger(__name__)

# [V3 Enhancement] ë™ì  Hydration ë©”íƒ€ë°ì´í„° í‚¤
DYNAMIC_INTENT_KEY = "__agent_intent__"  # ì—ì´ì „íŠ¸ê°€ ìš”ì²­í•˜ëŠ” ì¶”ê°€ í•„ë“œ

# ë…¸ë“œ íƒ€ì…ë³„ í•„ìˆ˜ í•„ë“œ ë§¤í•‘
NODE_REQUIRED_FIELDS = {
    "llm_chat": {
        "control": {"segment_id", "execution_id", "owner_id", "workflow_id"},
        "data": {"current_state"},  # workflow_config ë¶ˆí•„ìš”
        "config": {"node.config", "node.id", "node.type"}
    },
    "conditional": {
        "control": {"segment_id", "execution_id"},
        "data": {"current_state"},  # partition_map ë¶ˆí•„ìš”
        "config": {"node.condition", "node.id", "node.type"}
    },
    "data_transform": {
        "control": {"segment_id", "execution_id"},
        "data": {"current_state", "query_results"},
        "config": {"node.transform", "node.id", "node.type"}
    },
    "operator": {
        "control": {"segment_id", "execution_id"},
        "data": {"current_state"},
        "config": {"node.params", "node.strategy", "node.id", "node.type"}
    },
    "vision": {
        "control": {"segment_id", "execution_id"},
        "data": {"current_state", "image_inputs", "video_inputs"},
        "config": {"node.config", "node.id", "node.type"}
    },
    "api_call": {
        "control": {"segment_id", "execution_id"},
        "data": {"current_state"},
        "config": {"node.endpoint", "node.headers", "node.params", "node.id"}
    },
    "parallel_group": {
        "control": {"segment_id", "execution_id", "owner_id", "workflow_id"},
        "data": {"current_state", "branches"},
        "config": {"node.id", "node.type"}  # workflow_configëŠ” branchesì— í¬í•¨
    },
    "aggregator": {
        "control": {"segment_id", "execution_id"},
        "data": {"current_state", "branch_results", "parallel_results"},
        "config": {"node.aggregation_strategy", "node.id"}
    },
    "loop": {
        "control": {"segment_id", "execution_id", "loop_counter", "max_loop_iterations"},
        "data": {"current_state"},
        "config": {"node.loop_config", "node.id", "node.type"}
    },
    "trigger": {
        "control": {"execution_id"},
        "data": {"trigger_payload", "request_context"},
        "config": {"node.trigger_type"}
    }
}

# í•­ìƒ ì œì™¸í•  ëŒ€ìš©ëŸ‰ í•„ë“œ (í¬ì¸í„°ë¡œë§Œ ì „ë‹¬)
ALWAYS_EXCLUDE_FIELDS = {
    "workflow_config",     # ì „ì²´ ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ (ìˆ˜ë°± KB)
    "partition_map",       # ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ íŒŒí‹°ì…˜ (ìˆ˜ì‹­ KB)
    "segment_manifest",    # ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ (ìˆ˜ì‹­ KB)
    "state_history",       # ì´ì „ ìƒíƒœ íˆìŠ¤í† ë¦¬ (ë©”ê°€ë°”ì´íŠ¸ ê¸‰)
    "all_edges",           # ì „ì²´ ì—£ì§€ ì •ë³´ (ìˆ˜ì‹­ KB)
}

# íŠ¹ìˆ˜ ì¼€ì´ìŠ¤: ì´ íƒ€ì…ë“¤ë§Œ ì˜ˆì™¸ì ìœ¼ë¡œ í•„ìš”
SPECIAL_CASE_NEEDS = {
    "parallel_group": {"workflow_config"},  # ë¸Œëœì¹˜ ìƒì„± ì‹œ í•„ìš”
    "aggregator": {},  # ì§‘ê³„ë§Œ í•˜ë¯€ë¡œ ë¶ˆí•„ìš”
}


class SegmentFieldOptimizer:
    """ì„¸ê·¸ë¨¼íŠ¸ë³„ í•„ìš” í•„ë“œë§Œ ì¶”ì¶œí•˜ëŠ” ìµœì í™” ë„êµ¬"""
    
    @staticmethod
    def get_required_fields(
        segment_type: str,
        agent_intent: Optional[Dict[str, Any]] = None
    ) -> Set[str]:
        """
        ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì…ì— ë”°ë¼ í•„ìš”í•œ í•„ë“œ ì§‘í•© ë°˜í™˜
        
        [V3 Enhancement] ë™ì  Hydration ì§€ì›:
        - agent_intentë¡œ ëŸ°íƒ€ì„ì— ì¶”ê°€ í•„ë“œ ìš”ì²­ ê°€ëŠ¥
        - ì˜ˆ: {"additional_fields": ["state_history"], "range": "recent_10"}
        
        Args:
            segment_type: ë…¸ë“œ íƒ€ì… (llm_chat, conditional, etc.)
            agent_intent: ì—ì´ì „íŠ¸ì˜ ë™ì  ìš”ì²­ ë©”íƒ€ë°ì´í„°
        
        Returns:
            í•„ìš”í•œ í•„ë“œëª… ì§‘í•©
        """
        required = NODE_REQUIRED_FIELDS.get(segment_type, {})
        
        all_fields = set()
        all_fields.update(required.get("control", set()))
        all_fields.update(required.get("data", set()))
        all_fields.update(required.get("config", set()))
        
        # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì¶”ê°€
        special = SPECIAL_CASE_NEEDS.get(segment_type, set())
        all_fields.update(special)
        
        # [V3] ë™ì  Hydration: ì—ì´ì „íŠ¸ Intent ë°˜ì˜
        if agent_intent:
            additional = agent_intent.get("additional_fields", [])
            all_fields.update(additional)
            
            # ë¡œê·¸: ì–´ë–¤ í•„ë“œê°€ ë™ì ìœ¼ë¡œ ì¶”ê°€ë˜ì—ˆëŠ”ì§€ ì¶”ì 
            if additional:
                logger.info(
                    f"[DynamicHydration] {segment_type} requested: {additional}"
                )
        
        return all_fields
    
    @staticmethod
    def filter_event_payload(
        event: Dict[str, Any],
        segment_type: str,
        preserve_control_plane: bool = True,
        strict_mode: bool = False
    ) -> Dict[str, Any]:
        """
        ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì…ì— ë”°ë¼ ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±°
        
        [V3 Enhancements]:
        - strict_mode: ì™„ì „ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ëª¨ë“œ (ê°€ì°¨ì—†ì´ ì œê±°)
        - ë™ì  Intent ì§€ì› (__agent_intent__ í‚¤)
        - ì¡°ê±´ë¶€ ë¡œê¹… (ì§ë ¬í™” ë¹„ìš© ìµœì†Œí™”)
        
        Args:
            event: ì›ë³¸ ì´ë²¤íŠ¸ í˜ì´ë¡œë“œ
            segment_type: ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì…
            preserve_control_plane: Control Plane í•„ë“œ ë³´ì¡´ ì—¬ë¶€
            strict_mode: Trueë©´ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì™¸ ëª¨ë‘ ì œê±°
        
        Returns:
            ìµœì í™”ëœ ì´ë²¤íŠ¸ í˜ì´ë¡œë“œ
        """
        # [V3] ë™ì  Intent ì¶”ì¶œ
        agent_intent = event.get(DYNAMIC_INTENT_KEY)
        
        required_fields = SegmentFieldOptimizer.get_required_fields(
            segment_type,
            agent_intent=agent_intent
        )
        
        # Control Plane í•„ë“œ (í•­ìƒ ìœ ì§€)
        control_plane_fields = {
            "segment_id", "execution_id", "owner_id", "workflow_id",
            "idempotency_key", "loop_counter", "segment_to_run",
            "next_action", "status", "total_segments"
        }
        
        # í•„í„°ë§ëœ í˜ì´ë¡œë“œ ìƒì„±
        filtered = {}
        
        # [V3] Strict Mode: í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì™¸ ëª¨ë‘ ì œê±°
        if strict_mode:
            allowed = required_fields | control_plane_fields if preserve_control_plane else required_fields
            
            for key, value in event.items():
                if key in allowed:
                    filtered[key] = value
                elif key in ALWAYS_EXCLUDE_FIELDS:
                    # í¬ì¸í„°ë¡œ ë³€í™˜
                    filtered[key] = SegmentFieldOptimizer._to_pointer(key, value)
            
        else:
            # Legacy Mode: ê¸°ì¡´ í˜¼í•© ë°©ì‹
            for key, value in event.items():
                # 1. Control Plane í•„ë“œëŠ” í•­ìƒ ìœ ì§€
                if preserve_control_plane and key in control_plane_fields:
                    filtered[key] = value
                    continue
                
                # 2. í•„ìˆ˜ í•„ë“œëŠ” ìœ ì§€
                if key in required_fields:
                    filtered[key] = value
                    continue
                
                # 3. ì œì™¸ ëŒ€ìƒì´ë©´ í¬ì¸í„°ë§Œ ìœ ì§€
                if key in ALWAYS_EXCLUDE_FIELDS:
                    filtered[key] = SegmentFieldOptimizer._to_pointer(key, value)
                    continue
                
                # 4. ë‚˜ë¨¸ì§€ëŠ” í˜„ì¬ ë¡œì§ ìœ ì§€ (ë‹¹ë¶„ê°„)
                # TODO: ì¶”í›„ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë°©ì‹ìœ¼ë¡œ ì „í™˜
                filtered[key] = value
        
        # [Fix #1] ì¡°ê±´ë¶€ ë¡œê¹…: INFO ë ˆë²¨ì¼ ë•Œë§Œ í¬ê¸° ê³„ì‚°
        # í”¼ë“œë°±: len(str(event))ëŠ” ì§ë ¬í™” ë¹„ìš© 2ë²ˆ ë°œìƒ
        if logger.isEnabledFor(logging.INFO):
            # json.dumps ê²°ê³¼ë¥¼ í•œ ë²ˆë§Œ ê³„ì‚°
            original_json = json.dumps(event, default=str)
            filtered_json = json.dumps(filtered, default=str)
            
            original_size = len(original_json)
            filtered_size = len(filtered_json)
            reduction = (1 - filtered_size / original_size) * 100 if original_size > 0 else 0
            
            mode_label = "Strict" if strict_mode else "Legacy"
            logger.info(
                f"[FieldOptimizer/{mode_label}] {segment_type}: "
                f"{original_size} â†’ {filtered_size} bytes "
                f"({reduction:.1f}% reduction)"
            )
        
        return filtered
    
    @staticmethod
    def _to_pointer(field_name: str, value: Any) -> Dict[str, Any]:
        """
        í•„ë“œë¥¼ S3 í¬ì¸í„°ë¡œ ë³€í™˜
        
        Returns:
            S3Pointer í˜•íƒœ ë˜ëŠ” ê¸°ì¡´ í¬ì¸í„° ìœ ì§€
        """
        if isinstance(value, dict):
            if value.get("__s3_pointer__"):
                # ì´ë¯¸ í¬ì¸í„°ë©´ ê·¸ëŒ€ë¡œ
                return value
            elif value.get("bucket") and value.get("key"):
                # S3Pointer í˜•íƒœ
                return {
                    "__s3_pointer__": True,
                    "bucket": value["bucket"],
                    "key": value["key"],
                    "checksum": value.get("checksum", "")
                }
        
        # í¬ì¸í„°ë¡œ ë³€í™˜ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° None ë°˜í™˜ (ì œì™¸)
        return {"__excluded__": True, "field": field_name}
    
    @staticmethod
    def should_offload_to_s3(field_name: str, segment_type: str) -> bool:
        """
        íŠ¹ì • í•„ë“œë¥¼ S3ë¡œ ì˜¤í”„ë¡œë“œí•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨
        
        Args:
            field_name: í•„ë“œ ì´ë¦„
            segment_type: ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì…
        
        Returns:
            Trueë©´ S3 ì˜¤í”„ë¡œë“œ í•„ìš”
        """
        # í•­ìƒ ì˜¤í”„ë¡œë“œ
        if field_name in ALWAYS_EXCLUDE_FIELDS:
            return True
        
        # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤: parallel_groupì€ workflow_config í•„ìš”í•˜ë¯€ë¡œ ì˜¤í”„ë¡œë“œ ì•ˆ í•¨
        if segment_type in SPECIAL_CASE_NEEDS:
            if field_name in SPECIAL_CASE_NEEDS[segment_type]:
                return False
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° í•„ë“œ
        large_data_fields = {
            "final_state", "current_state", "branches",
            "parallel_results", "branch_results"
        }
        
        return field_name in large_data_fields
    
    @staticmethod
    def load_pointer_with_select(
        s3_client,
        pointer: Dict[str, Any],
        fields_to_extract: Optional[Set[str]] = None
    ) -> Dict[str, Any]:
        """
        [Fix #3] S3 Selectë¡œ í¬ì¸í„° ë¡œë“œ (ë¶€ë¶„ í•„ë“œë§Œ ì¶”ì¶œ)
        
        í”¼ë“œë°± ë°˜ì˜:
        - ì „ì²´ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ëŒ€ì‹  SQL-on-JSON ì‚¬ìš©
        - Lambda ë©”ëª¨ë¦¬/ë„¤íŠ¸ì›Œí¬ ë¹„ìš© íšê¸°ì  ê°ì†Œ
        
        Args:
            s3_client: boto3 S3 í´ë¼ì´ì–¸íŠ¸
            pointer: S3Pointer ê°ì²´ ({"bucket": ..., "key": ...})
            fields_to_extract: ì¶”ì¶œí•  í•„ë“œëª… ì§‘í•© (Noneì´ë©´ ì „ì²´)
        
        Returns:
            ë¡œë“œëœ ë°ì´í„° (í•„ë“œ í•„í„°ë§ ì ìš©)
        """
        if not pointer.get("__s3_pointer__"):
            # í¬ì¸í„°ê°€ ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            return pointer
        
        bucket = pointer.get("bucket")
        key = pointer.get("key")
        
        if not bucket or not key:
            logger.warning("Invalid S3 pointer: missing bucket or key")
            return {}
        
        try:
            # í•„ë“œ ì¶”ì¶œ ì—†ìœ¼ë©´ GetObject
            if not fields_to_extract:
                response = s3_client.get_object(Bucket=bucket, Key=key)
                data = json.loads(response['Body'].read().decode('utf-8'))
                logger.info(f"[S3 GetObject] Loaded full data from {key}")
                return data
            
            # S3 Selectë¡œ í•„ë“œë³„ ì¶”ì¶œ
            # SQL: SELECT s.field1, s.field2 FROM S3Object[*] s
            select_fields = ", ".join([f"s.{field}" for field in fields_to_extract])
            sql_query = f"SELECT {select_fields} FROM S3Object[*] s"
            
            response = s3_client.select_object_content(
                Bucket=bucket,
                Key=key,
                ExpressionType='SQL',
                Expression=sql_query,
                InputSerialization={
                    'JSON': {'Type': 'DOCUMENT'},
                    'CompressionType': 'GZIP'  # ğŸ”„ v3.3 KernelStateManager í˜¸í™˜
                },
                OutputSerialization={'JSON': {'RecordDelimiter': '\n'}}
            )
            
            # ìŠ¤íŠ¸ë¦¼ì—ì„œ ê²°ê³¼ ìˆ˜ì§‘
            result_data = {}
            for event in response['Payload']:
                if 'Records' in event:
                    records = event['Records']['Payload'].decode('utf-8')
                    for line in records.strip().split('\n'):
                        if line:
                            result_data.update(json.loads(line))
            
            logger.info(
                f"[S3 Select] Extracted {len(fields_to_extract)} fields from {key} "
                f"(~{len(str(result_data))} bytes)"
            )
            
            return result_data
            
        except Exception as e:
            logger.error(f"Failed to load S3 pointer: {e}", exc_info=True)
            # Fallback: ì „ì²´ ë¡œë“œ
            try:
                response = s3_client.get_object(Bucket=bucket, Key=key)
                return json.loads(response['Body'].read().decode('utf-8'))
            except Exception as fallback_error:
                logger.error(f"Fallback GetObject also failed: {fallback_error}")
                return {}


# Singleton ì¸ìŠ¤í„´ìŠ¤
_optimizer = SegmentFieldOptimizer()


def optimize_segment_payload(
    event: Dict[str, Any],
    segment_config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    ì„¸ê·¸ë¨¼íŠ¸ ì‹¤í–‰ ì „ í˜ì´ë¡œë“œ ìµœì í™” (í¸ì˜ í•¨ìˆ˜)
    
    Args:
        event: ì›ë³¸ ì´ë²¤íŠ¸
        segment_config: ì„¸ê·¸ë¨¼íŠ¸ ì„¤ì • (type í¬í•¨)
    
    Returns:
        ìµœì í™”ëœ ì´ë²¤íŠ¸
    """
    segment_type = segment_config.get("type", "unknown")
    return _optimizer.filter_event_payload(event, segment_type)


def get_offload_fields(segment_type: str) -> Set[str]:
    """
    S3 ì˜¤í”„ë¡œë“œê°€ í•„ìš”í•œ í•„ë“œ ëª©ë¡ ë°˜í™˜ (í¸ì˜ í•¨ìˆ˜)
    
    Args:
        segment_type: ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì…
    
    Returns:
        ì˜¤í”„ë¡œë“œ ëŒ€ìƒ í•„ë“œëª… ì§‘í•©
    """
    offload_fields = set()
    
    # ê¸°ë³¸ ì˜¤í”„ë¡œë“œ í•„ë“œ
    for field in ["final_state", "current_state", "branches", "parallel_results"]:
        if _optimizer.should_offload_to_s3(field, segment_type):
            offload_fields.add(field)
    
    # í•­ìƒ ì œì™¸ í•„ë“œ
    offload_fields.update(ALWAYS_EXCLUDE_FIELDS)
    
    # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì œì™¸
    if segment_type in SPECIAL_CASE_NEEDS:
        offload_fields -= SPECIAL_CASE_NEEDS[segment_type]
    
    return offload_fields
