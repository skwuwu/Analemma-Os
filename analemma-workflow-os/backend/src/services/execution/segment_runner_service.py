import logging
import os
import time
import json
from typing import Dict, Any, Optional, List, Tuple

# [v2.1] ì¤‘ì•™ ì§‘ì¤‘ì‹ ì¬ì‹œë„ ìœ í‹¸ë¦¬í‹°
try:
    from src.common.retry_utils import retry_call, retry_stepfunctions, retry_s3
    RETRY_UTILS_AVAILABLE = True
except ImportError:
    RETRY_UTILS_AVAILABLE = False

# Services
from src.services.state.state_manager import StateManager
from src.services.recovery.self_healing_service import SelfHealingService
# Legacy Imports (for now, until further refactoring)
from src.services.workflow.repository import WorkflowRepository
# Using generic imports from main handler file as source of truth
from src.handlers.core.main import run_workflow, partition_workflow as _partition_workflow_dynamically, _build_segment_config
from src.common.statebag import normalize_inplace


logger = logging.getLogger(__name__)

# ============================================================================
# ğŸ›¡ï¸ [Kernel] Dynamic Scheduling Constants
# ============================================================================
# ë©”ëª¨ë¦¬ ì•ˆì „ ë§ˆì§„ (80% ì‚¬ìš© ì‹œ ë¶„í•  íŠ¸ë¦¬ê±°)
MEMORY_SAFETY_THRESHOLD = 0.8
# ì„¸ê·¸ë¨¼íŠ¸ ë¶„í•  ì‹œ ìµœì†Œ ë…¸ë“œ ìˆ˜
MIN_NODES_PER_SUB_SEGMENT = 2
# ìµœëŒ€ ë¶„í•  ê¹Šì´ (ë¬´í•œ ë¶„í•  ë°©ì§€)
MAX_SPLIT_DEPTH = 3
# ì„¸ê·¸ë¨¼íŠ¸ ìƒíƒœ ê°’
SEGMENT_STATUS_PENDING = "PENDING"
SEGMENT_STATUS_RUNNING = "RUNNING"
SEGMENT_STATUS_COMPLETED = "COMPLETED"
SEGMENT_STATUS_SKIPPED = "SKIPPED"
SEGMENT_STATUS_FAILED = "FAILED"

# ============================================================================
# ğŸ”€ [Kernel] Parallel Scheduler Constants
# ============================================================================
# ê¸°ë³¸ ë™ì‹œì„± ì œí•œ (Lambda ê³„ì • ìˆ˜ì¤€)
DEFAULT_MAX_CONCURRENT_MEMORY_MB = 3072  # 3GB (Lambda 3ê°œ ë™ì‹œ ì‹¤í–‰ ê°€ì •)
DEFAULT_MAX_CONCURRENT_TOKENS = 100000   # ë¶„ë‹¹ í† í° ì œí•œ
DEFAULT_MAX_CONCURRENT_BRANCHES = 10     # ìµœëŒ€ ë™ì‹œ ë¸Œëœì¹˜ ìˆ˜

# ìŠ¤ì¼€ì¤„ë§ ì „ëµ
STRATEGY_SPEED_OPTIMIZED = "SPEED_OPTIMIZED"      # ìµœëŒ€í•œ ë³‘ë ¬ ì‹¤í–‰
STRATEGY_RESOURCE_OPTIMIZED = "RESOURCE_OPTIMIZED" # ìì› íš¨ìœ¨ ìš°ì„ 
STRATEGY_COST_OPTIMIZED = "COST_OPTIMIZED"        # ë¹„ìš© ìµœì†Œí™”

# ë¸Œëœì¹˜ ì˜ˆìƒ ìì› ê¸°ë³¸ê°’
DEFAULT_BRANCH_MEMORY_MB = 256
DEFAULT_BRANCH_TOKENS = 5000

# ê³„ì • ìˆ˜ì¤€ í•˜ë“œ ë¦¬ë°‹ (SPEED_OPTIMIZEDì—ì„œë„ ì²´í¬)
ACCOUNT_LAMBDA_CONCURRENCY_LIMIT = 100  # AWS ê¸°ë³¸ ë™ì‹œì„± ì œí•œ
ACCOUNT_MEMORY_HARD_LIMIT_MB = 10240    # 10GB í•˜ë“œ ë¦¬ë°‹

# ìƒíƒœ ë³‘í•© ì •ì±…
MERGE_POLICY_OVERWRITE = "OVERWRITE"      # ë‚˜ì¤‘ ê°’ì´ ë®ì–´ì”€ (ê¸°ë³¸)
MERGE_POLICY_APPEND_LIST = "APPEND_LIST"  # ë¦¬ìŠ¤íŠ¸ëŠ” í•©ì¹¨
MERGE_POLICY_KEEP_FIRST = "KEEP_FIRST"    # ì²« ë²ˆì§¸ ê°’ ìœ ì§€
MERGE_POLICY_CONFLICT_ERROR = "ERROR"     # ì¶©ëŒ ì‹œ ì—ëŸ¬

# ë¦¬ìŠ¤íŠ¸ ë³‘í•©ì´ í•„ìš”í•œ í‚¤ íŒ¨í„´
LIST_MERGE_KEY_PATTERNS = [
    '__new_history_logs',
    '__kernel_actions', 
    '_results',
    '_items',
    '_outputs',
    'collected_',
    'aggregated_'
]


class SegmentRunnerService:
    def __init__(self):
        self.state_manager = StateManager()
        self.healer = SelfHealingService()
        self.repo = WorkflowRepository()
        self.threshold = int(os.environ.get("STATE_SIZE_THRESHOLD", 256000))
        
        # ğŸ›¡ï¸ [Kernel] S3 í´ë¼ì´ì–¸íŠ¸ (ì§€ì—° ì´ˆê¸°í™”)
        self._s3_client = None
    
    @property
    def s3_client(self):
        """Lazy S3 client initialization"""
        if self._s3_client is None:
            import boto3
            self._s3_client = boto3.client('s3')
        return self._s3_client

    # ========================================================================
    # ï¿½ [Utility] State Merge: ë¬´ê²°ì„± ë³´ì¥ ìƒíƒœ ë³‘í•©
    # ========================================================================
    def _should_merge_as_list(self, key: str) -> bool:
        """
        ì´ í‚¤ê°€ ë¦¬ìŠ¤íŠ¸ ë³‘í•© ëŒ€ìƒì¸ì§€ í™•ì¸
        """
        for pattern in LIST_MERGE_KEY_PATTERNS:
            if pattern in key or key.startswith(pattern):
                return True
        return False

    def _merge_states(
        self,
        base_state: Dict[str, Any],
        new_state: Dict[str, Any],
        merge_policy: str = MERGE_POLICY_APPEND_LIST
    ) -> Dict[str, Any]:
        """
        ğŸ”§ ë¬´ê²°ì„± ë³´ì¥ ìƒíƒœ ë³‘í•©
        
        ì •ì±…:
        - OVERWRITE: ë‹¨ìˆœ ë®ì–´ì“°ê¸° (ê¸°ì¡´ ë™ì‘)
        - APPEND_LIST: ë¦¬ìŠ¤íŠ¸ í‚¤ëŠ” í•©ì¹¨, ë‚˜ë¨¸ì§€ëŠ” ë®ì–´ì”€
        - KEEP_FIRST: ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í‚¤ëŠ” ìœ ì§€
        - ERROR: í‚¤ ì¶©ëŒ ì‹œ ì˜ˆì™¸ ë°œìƒ
        
        íŠ¹ë³„ ì²˜ë¦¬:
        - __new_history_logs, __kernel_actions ë“±ì€ í•­ìƒ ë¦¬ìŠ¤íŠ¸ ë³‘í•©
        - _ë¡œ ì‹œì‘í•˜ëŠ” ë‚´ë¶€ í‚¤ëŠ” íŠ¹ë³„ ì·¨ê¸‰
        """
        if merge_policy == MERGE_POLICY_OVERWRITE:
            result = base_state.copy()
            result.update(new_state)
            return result
        
        result = base_state.copy()
        conflicts = []
        
        for key, new_value in new_state.items():
            if key not in result:
                # ìƒˆ í‚¤: ê·¸ëƒ¥ ì¶”ê°€
                result[key] = new_value
                continue
            
            existing_value = result[key]
            
            # ë¦¬ìŠ¤íŠ¸ ë³‘í•© ëŒ€ìƒ í‚¤ í™•ì¸
            if self._should_merge_as_list(key):
                if isinstance(existing_value, list) and isinstance(new_value, list):
                    result[key] = existing_value + new_value
                elif isinstance(new_value, list):
                    result[key] = [existing_value] + new_value if existing_value else new_value
                elif isinstance(existing_value, list):
                    result[key] = existing_value + [new_value] if new_value else existing_value
                else:
                    result[key] = [existing_value, new_value]
                continue
            
            # ì •ì±…ì— ë”°ë¥¸ ì²˜ë¦¬
            if merge_policy == MERGE_POLICY_KEEP_FIRST:
                # ê¸°ì¡´ ê°’ ìœ ì§€
                continue
            elif merge_policy == MERGE_POLICY_CONFLICT_ERROR:
                if existing_value != new_value:
                    conflicts.append(key)
            else:
                # APPEND_LIST ê¸°ë³¸: ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©´ ë®ì–´ì”€
                result[key] = new_value
        
        if conflicts:
            logger.warning(f"[Merge] State conflicts detected on keys: {conflicts}")
            if merge_policy == MERGE_POLICY_CONFLICT_ERROR:
                raise ValueError(f"State merge conflict on keys: {conflicts}")
        
        return result

    # ========================================================================
    # ï¿½ğŸ›¡ï¸ [Pattern 1] Segment-Level Self-Healing: ì„¸ê·¸ë¨¼íŠ¸ ë‚´ë¶€ ë™ì  ë¶„í• 
    # ========================================================================
    def _estimate_segment_memory(self, segment_config: Dict[str, Any], state: Dict[str, Any]) -> int:
        """
        ì„¸ê·¸ë¨¼íŠ¸ ì‹¤í–‰ì— í•„ìš”í•œ ë©”ëª¨ë¦¬ ì¶”ì • (MB ë‹¨ìœ„)
        
        [ìµœì í™”] json.dumps ëŒ€ì‹  ë©”íƒ€ë°ì´í„° ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
        - ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ json.dumpsëŠ” ê·¸ ìì²´ë¡œ ë©”ëª¨ë¦¬ ë¶€ë‹´
        - ë¦¬ìŠ¤íŠ¸ ê¸¸ì´, ë¬¸ìì—´ í‚¤ ì¡´ì¬ ì—¬ë¶€ ë“±ìœ¼ë¡œ ê²½ëŸ‰ ì¶”ì •
        
        ì¶”ì • ê¸°ì¤€:
        - ë…¸ë“œ ìˆ˜ Ã— ê¸°ë³¸ ë©”ëª¨ë¦¬ (10MB)
        - LLM ë…¸ë“œ: ì¶”ê°€ 50MB
        - for_each ë…¸ë“œ: ì•„ì´í…œ ìˆ˜ Ã— 5MB
        - ìƒíƒœ í¬ê¸°: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¶”ì •
        """
        base_memory = 50  # ê¸°ë³¸ ì˜¤ë²„í—¤ë“œ
        
        nodes = segment_config.get('nodes', [])
        if not nodes:
            return base_memory
        
        node_memory = len(nodes) * 10  # ë…¸ë“œë‹¹ 10MB
        
        llm_memory = 0
        foreach_memory = 0
        
        for node in nodes:
            node_type = node.get('type', '')
            if node_type in ('llm_chat', 'aiModel'):
                llm_memory += 50  # LLM ë…¸ë“œëŠ” ì¶”ê°€ 50MB
            elif node_type == 'for_each':
                config = node.get('config', {})
                items_key = config.get('input_list_key', '')
                if items_key and items_key in state:
                    items = state.get(items_key, [])
                    if isinstance(items, list):
                        foreach_memory += len(items) * 5
        
        # [ìµœì í™”] ìƒíƒœ í¬ê¸° ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¶”ì • (json.dumps íšŒí”¼)
        state_size_mb = self._estimate_state_size_lightweight(state)
        
        total = base_memory + node_memory + llm_memory + foreach_memory + int(state_size_mb)
        
        logger.debug(f"[Kernel] Memory estimate: base={base_memory}, nodes={node_memory}, "
                    f"llm={llm_memory}, foreach={foreach_memory}, state={state_size_mb:.1f}MB, total={total}MB")
        
        return total

    def _estimate_state_size_lightweight(self, state: Dict[str, Any], max_sample_keys: int = 20) -> float:
        """
        [ìµœì í™”] json.dumps ì—†ì´ ìƒíƒœ í¬ê¸°ë¥¼ ê²½ëŸ‰ ì¶”ì •
        
        ì „ëµ:
        1. ìƒìœ„ Nê°œ í‚¤ë§Œ ìƒ˜í”Œë§í•˜ì—¬ í‰ê·  í¬ê¸° ê³„ì‚°
        2. ë¦¬ìŠ¤íŠ¸ëŠ” ê¸¸ì´ Ã— í‰ê·  ì•„ì´í…œ í¬ê¸°ë¡œ ì¶”ì •
        3. ë¬¸ìì—´ì€ len() ì‚¬ìš©
        4. ì¤‘ì²© dictëŠ” í‚¤ ìˆ˜ë¡œ ì¶”ì •
        
        Returns:
            ì¶”ì • í¬ê¸° (MB)
        """
        if not state or not isinstance(state, dict):
            return 0.1  # ìµœì†Œ 100KB
        
        total_bytes = 0
        keys = list(state.keys())[:max_sample_keys]
        
        for key in keys:
            value = state.get(key)
            total_bytes += self._estimate_value_size(value)
        
        # ìƒ˜í”Œë§ ë¹„ìœ¨ë¡œ ì „ì²´ í¬ê¸° ì¶”ì •
        if len(state) > max_sample_keys:
            sample_ratio = len(state) / max_sample_keys
            total_bytes = int(total_bytes * sample_ratio)
        
        return total_bytes / (1024 * 1024)  # bytes â†’ MB

    def _estimate_value_size(self, value: Any, depth: int = 0) -> int:
        """
        ê°’ì˜ í¬ê¸°ë¥¼ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì¶”ì • (bytes)
        
        ì¬ê·€ ê¹Šì´ ì œí•œìœ¼ë¡œ ë¬´í•œ ë£¨í”„ ë°©ì§€
        """
        if depth > 3:  # ê¹Šì´ ì œí•œ
            return 100  # ëŒ€ëµì  ì¶”ì •
        
        if value is None:
            return 4
        elif isinstance(value, bool):
            return 4
        elif isinstance(value, (int, float)):
            return 8
        elif isinstance(value, str):
            return len(value.encode('utf-8', errors='ignore'))
        elif isinstance(value, bytes):
            return len(value)
        elif isinstance(value, list):
            if not value:
                return 2
            # ì²« 3ê°œ ì•„ì´í…œë§Œ ìƒ˜í”Œë§í•˜ì—¬ í‰ê·  ê³„ì‚°
            sample = value[:3]
            avg_size = sum(self._estimate_value_size(v, depth + 1) for v in sample) / len(sample)
            return int(avg_size * len(value))
        elif isinstance(value, dict):
            if not value:
                return 2
            # ì²« 5ê°œ í‚¤ë§Œ ìƒ˜í”Œë§
            sample_keys = list(value.keys())[:5]
            sample_size = sum(
                len(str(k)) + self._estimate_value_size(value[k], depth + 1) 
                for k in sample_keys
            )
            if len(value) > 5:
                return int(sample_size * len(value) / 5)
            return sample_size
        else:
            # ê¸°íƒ€ íƒ€ì…: ëŒ€ëµì  ì¶”ì •
            return 100

    def _split_segment(self, segment_config: Dict[str, Any], split_depth: int = 0) -> List[Dict[str, Any]]:
        """
        ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë” ì‘ì€ ì„œë¸Œ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„í• 
        
        ë¶„í•  ì „ëµ:
        1. ë…¸ë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜ìœ¼ë¡œ ë‚˜ëˆ”
        2. ì˜ì¡´ì„± ìœ ì§€: ì—£ì§€ ì—°ê²° ë³´ì¡´
        3. ìµœì†Œ ë…¸ë“œ ìˆ˜ ë³´ì¥
        """
        if split_depth >= MAX_SPLIT_DEPTH:
            logger.warning(f"[Kernel] Max split depth ({MAX_SPLIT_DEPTH}) reached, returning original segment")
            return [segment_config]
        
        nodes = segment_config.get('nodes', [])
        edges = segment_config.get('edges', [])
        
        if len(nodes) < MIN_NODES_PER_SUB_SEGMENT * 2:
            logger.info(f"[Kernel] Segment too small to split ({len(nodes)} nodes)")
            return [segment_config]
        
        # ë…¸ë“œë¥¼ ë°˜ìœ¼ë¡œ ë¶„í• 
        mid = len(nodes) // 2
        first_nodes = nodes[:mid]
        second_nodes = nodes[mid:]
        
        first_node_ids = {n.get('id') for n in first_nodes}
        second_node_ids = {n.get('id') for n in second_nodes}
        
        # ì—£ì§€ ë¶„ë¦¬: ê° ì„œë¸Œ ì„¸ê·¸ë¨¼íŠ¸ ë‚´ë¶€ ì—£ì§€ë§Œ ìœ ì§€
        first_edges = [e for e in edges 
                      if e.get('source') in first_node_ids and e.get('target') in first_node_ids]
        second_edges = [e for e in edges 
                       if e.get('source') in second_node_ids and e.get('target') in second_node_ids]
        
        # ì„œë¸Œ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
        original_id = segment_config.get('id', 'segment')
        
        sub_segment_1 = {
            **segment_config,
            'id': f"{original_id}_sub_1",
            'nodes': first_nodes,
            'edges': first_edges,
            '_kernel_split': True,
            '_split_depth': split_depth + 1,
            '_parent_segment_id': original_id
        }
        
        sub_segment_2 = {
            **segment_config,
            'id': f"{original_id}_sub_2",
            'nodes': second_nodes,
            'edges': second_edges,
            '_kernel_split': True,
            '_split_depth': split_depth + 1,
            '_parent_segment_id': original_id
        }
        
        logger.info(f"[Kernel] ğŸ”§ Segment '{original_id}' split into 2 sub-segments: "
                   f"{len(first_nodes)} + {len(second_nodes)} nodes")
        
        return [sub_segment_1, sub_segment_2]

    def _execute_with_auto_split(
        self, 
        segment_config: Dict[str, Any], 
        initial_state: Dict[str, Any],
        auth_user_id: str,
        split_depth: int = 0
    ) -> Dict[str, Any]:
        """
        ğŸ›¡ï¸ [Pattern 1] ë©”ëª¨ë¦¬ ê¸°ë°˜ ìë™ ë¶„í•  ì‹¤í–‰
        
        ë©”ëª¨ë¦¬ ë¶€ì¡±ì´ ì˜ˆìƒë˜ë©´ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë¶„í• í•˜ì—¬ ìˆœì°¨ ì‹¤í–‰
        """
        # ì‚¬ìš© ê°€ëŠ¥í•œ Lambda ë©”ëª¨ë¦¬
        available_memory = int(os.environ.get('AWS_LAMBDA_FUNCTION_MEMORY_SIZE', 512))
        
        # ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ ì¶”ì •
        estimated_memory = self._estimate_segment_memory(segment_config, initial_state)
        
        # ì•ˆì „ ì„ê³„ê°’ ì²´í¬
        if estimated_memory > available_memory * MEMORY_SAFETY_THRESHOLD:
            logger.info(f"[Kernel] âš ï¸ Memory pressure detected: {estimated_memory}MB estimated, "
                       f"{available_memory}MB available (threshold: {MEMORY_SAFETY_THRESHOLD*100}%)")
            
            # ë¶„í•  ì‹œë„
            sub_segments = self._split_segment(segment_config, split_depth)
            
            if len(sub_segments) > 1:
                logger.info(f"[Kernel] ğŸ”§ Executing {len(sub_segments)} sub-segments sequentially")
                
                # ì„œë¸Œ ì„¸ê·¸ë¨¼íŠ¸ ìˆœì°¨ ì‹¤í–‰
                current_state = initial_state.copy()
                all_logs = []
                kernel_actions = []
                
                for i, sub_seg in enumerate(sub_segments):
                    logger.info(f"[Kernel] Executing sub-segment {i+1}/{len(sub_segments)}: {sub_seg.get('id')}")
                    
                    # ì¬ê·€ì ìœ¼ë¡œ ìë™ ë¶„í•  ì ìš©
                    sub_result = self._execute_with_auto_split(
                        sub_seg, current_state, auth_user_id, split_depth + 1
                    )
                    
                    # ğŸ”§ ë¬´ê²°ì„± ë³´ì¥ ìƒíƒœ ë³‘í•© (ë¦¬ìŠ¤íŠ¸ í‚¤ëŠ” í•©ì¹¨)
                    if isinstance(sub_result, dict):
                        current_state = self._merge_states(
                            current_state, 
                            sub_result,
                            merge_policy=MERGE_POLICY_APPEND_LIST
                        )
                        # all_logsëŠ” ì´ë¯¸ _merge_statesì—ì„œ ì²˜ë¦¬ë¨
                    
                    kernel_actions.append({
                        'action': 'SPLIT_EXECUTE',
                        'sub_segment_id': sub_seg.get('id'),
                        'index': i,
                        'timestamp': time.time()
                    })
                
                # ì»¤ë„ ë©”íƒ€ë°ì´í„° ì¶”ê°€
                current_state['__kernel_actions'] = kernel_actions
                current_state['__new_history_logs'] = all_logs
                
                return current_state
        
        # ì •ìƒ ì‹¤í–‰ (ë¶„í•  ë¶ˆí•„ìš”)
        return run_workflow(
            config_json=segment_config,
            initial_state=initial_state,
            ddb_table_name=os.environ.get("JOB_TABLE"),
            user_api_keys={},
            run_config={"user_id": auth_user_id}
        )

    # ========================================================================
    # ğŸ›¡ï¸ [Pattern 2] Manifest Mutation: S3 Manifest ë™ì  ìˆ˜ì •
    # ========================================================================
    def _load_manifest_from_s3(self, manifest_s3_path: str) -> Optional[List[Dict[str, Any]]]:
        """S3ì—ì„œ segment_manifest ë¡œë“œ"""
        if not manifest_s3_path or not manifest_s3_path.startswith('s3://'):
            return None
        
        try:
            parts = manifest_s3_path.replace('s3://', '').split('/', 1)
            bucket = parts[0]
            key = parts[1] if len(parts) > 1 else ''
            
            response = self.s3_client.get_object(Bucket=bucket, Key=key)
            manifest = json.loads(response['Body'].read().decode('utf-8'))
            
            logger.info(f"[Kernel] Loaded manifest from S3: {len(manifest)} segments")
            return manifest
            
        except Exception as e:
            logger.error(f"[Kernel] Failed to load manifest from S3: {e}")
            return None

    def _save_manifest_to_s3(self, manifest: List[Dict[str, Any]], manifest_s3_path: str) -> bool:
        """ìˆ˜ì •ëœ segment_manifestë¥¼ S3ì— ì €ì¥"""
        if not manifest_s3_path or not manifest_s3_path.startswith('s3://'):
            return False
        
        try:
            parts = manifest_s3_path.replace('s3://', '').split('/', 1)
            bucket = parts[0]
            key = parts[1] if len(parts) > 1 else ''
            
            self.s3_client.put_object(
                Bucket=bucket,
                Key=key,
                Body=json.dumps(manifest, ensure_ascii=False).encode('utf-8'),
                ContentType='application/json',
                Metadata={
                    'kernel_modified': 'true',
                    'modified_at': str(int(time.time()))
                }
            )
            
            logger.info(f"[Kernel] Saved modified manifest to S3: {len(manifest)} segments")
            return True
            
        except Exception as e:
            logger.error(f"[Kernel] Failed to save manifest to S3: {e}")
            return False

    def _check_segment_status(self, segment_config: Dict[str, Any]) -> str:
        """ì„¸ê·¸ë¨¼íŠ¸ ìƒíƒœ í™•ì¸ (SKIPPED ë“±)"""
        return segment_config.get('status', SEGMENT_STATUS_PENDING)

    def _mark_segments_for_skip(
        self, 
        manifest_s3_path: str, 
        segment_ids_to_skip: List[int], 
        reason: str
    ) -> bool:
        """
        ğŸ›¡ï¸ [Pattern 2] íŠ¹ì • ì„¸ê·¸ë¨¼íŠ¸ë¥¼ SKIPìœ¼ë¡œ ë§ˆí‚¹
        
        ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
        - ì¡°ê±´ ë¶„ê¸°ì—ì„œ íŠ¹ì • ê²½ë¡œ ë¶ˆí•„ìš”
        - ì„ í–‰ ì„¸ê·¸ë¨¼íŠ¸ ì‹¤íŒ¨ë¡œ í›„ì† ì„¸ê·¸ë¨¼íŠ¸ ì‹¤í–‰ ë¶ˆê°€
        """
        manifest = self._load_manifest_from_s3(manifest_s3_path)
        if not manifest:
            return False
        
        modified = False
        for segment in manifest:
            if segment.get('segment_id') in segment_ids_to_skip:
                segment['status'] = SEGMENT_STATUS_SKIPPED
                segment['skip_reason'] = reason
                segment['skipped_at'] = int(time.time())
                segment['skipped_by'] = 'kernel'
                modified = True
                logger.info(f"[Kernel] Marked segment {segment.get('segment_id')} for skip: {reason}")
        
        if modified:
            return self._save_manifest_to_s3(manifest, manifest_s3_path)
        
        return False

    def _inject_recovery_segments(
        self,
        manifest_s3_path: str,
        after_segment_id: int,
        recovery_segments: List[Dict[str, Any]],
        reason: str
    ) -> bool:
        """
        ğŸ›¡ï¸ [Pattern 2] ë³µêµ¬ ì„¸ê·¸ë¨¼íŠ¸ ì‚½ì…
        
        ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
        - API ì‹¤íŒ¨ í›„ ë°±ì—… ê²½ë¡œ ì‚½ì…
        - ì—ëŸ¬ í•¸ë“¤ë§ ì„¸ê·¸ë¨¼íŠ¸ ë™ì  ì¶”ê°€
        """
        manifest = self._load_manifest_from_s3(manifest_s3_path)
        if not manifest:
            return False
        
        # ì‚½ì… ìœ„ì¹˜ ì°¾ê¸°
        insert_index = None
        for i, segment in enumerate(manifest):
            if segment.get('segment_id') == after_segment_id:
                insert_index = i + 1
                break
        
        if insert_index is None:
            logger.warning(f"[Kernel] Could not find segment {after_segment_id} for recovery injection")
            return False
        
        # ë³µêµ¬ ì„¸ê·¸ë¨¼íŠ¸ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
        max_segment_id = max(s.get('segment_id', 0) for s in manifest)
        for i, rec_seg in enumerate(recovery_segments):
            rec_seg['segment_id'] = max_segment_id + i + 1
            rec_seg['status'] = SEGMENT_STATUS_PENDING
            rec_seg['injected_by'] = 'kernel'
            rec_seg['injection_reason'] = reason
            rec_seg['injected_at'] = int(time.time())
            rec_seg['type'] = rec_seg.get('type', 'recovery')
        
        # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ì— ì‚½ì…
        new_manifest = manifest[:insert_index] + recovery_segments + manifest[insert_index:]
        
        # í›„ì† ì„¸ê·¸ë¨¼íŠ¸ ID ì¬ì¡°ì •
        for i, segment in enumerate(new_manifest):
            segment['execution_order'] = i
        
        logger.info(f"[Kernel] ğŸ”§ Injected {len(recovery_segments)} recovery segments after segment {after_segment_id}")
        
        return self._save_manifest_to_s3(new_manifest, manifest_s3_path)

    # ========================================================================
    # ğŸ”€ [Pattern 3] Parallel Scheduler: ì¸í”„ë¼ ì¸ì§€í˜• ë³‘ë ¬ ìŠ¤ì¼€ì¤„ë§
    # ========================================================================
    def _estimate_branch_resources(self, branch: Dict[str, Any], state: Dict[str, Any]) -> Dict[str, int]:
        """
        ë¸Œëœì¹˜ì˜ ì˜ˆìƒ ìì› ìš”êµ¬ëŸ‰ ì¶”ì •
        
        Returns:
            {
                'memory_mb': ì˜ˆìƒ ë©”ëª¨ë¦¬ (MB),
                'tokens': ì˜ˆìƒ í† í° ìˆ˜,
                'llm_calls': LLM í˜¸ì¶œ íšŸìˆ˜,
                'has_shared_resource': ê³µìœ  ìì› ì ‘ê·¼ ì—¬ë¶€
            }
        """
        nodes = branch.get('nodes', [])
        if not nodes:
            return {
                'memory_mb': DEFAULT_BRANCH_MEMORY_MB,
                'tokens': 0,
                'llm_calls': 0,
                'has_shared_resource': False
            }
        
        memory_mb = 50  # ê¸°ë³¸ ì˜¤ë²„í—¤ë“œ
        tokens = 0
        llm_calls = 0
        has_shared_resource = False
        
        for node in nodes:
            node_type = node.get('type', '')
            config = node.get('config', {})
            
            # ë©”ëª¨ë¦¬ ì¶”ì •
            memory_mb += 10  # ë…¸ë“œë‹¹ ê¸°ë³¸ 10MB
            
            if node_type in ('llm_chat', 'aiModel'):
                memory_mb += 50  # LLM ë…¸ë“œ ì¶”ê°€ ë©”ëª¨ë¦¬
                llm_calls += 1
                # í† í° ì¶”ì •: í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ê¸°ë°˜
                prompt = config.get('prompt', '') or config.get('system_prompt', '')
                tokens += len(prompt) // 4 + 500  # ëŒ€ëµì  í† í° ì¶”ì • + ì‘ë‹µ ì˜ˆìƒ
                
            elif node_type == 'for_each':
                items_key = config.get('input_list_key', '')
                if items_key and items_key in state:
                    items = state.get(items_key, [])
                    if isinstance(items, list):
                        memory_mb += len(items) * 5
                        # for_each ë‚´ë¶€ì— LLMì´ ìˆìœ¼ë©´ í† í° í­ì¦
                        sub_nodes = config.get('sub_node_config', {}).get('nodes', [])
                        for sub_node in sub_nodes:
                            if sub_node.get('type') in ('llm_chat', 'aiModel'):
                                tokens += len(items) * 1000  # ì•„ì´í…œë‹¹ 1000 í† í° ì˜ˆìƒ
                                llm_calls += len(items)
            
            # ê³µìœ  ìì› ì ‘ê·¼ ê°ì§€
            if node_type in ('db_write', 's3_write', 'api_call'):
                has_shared_resource = True
            if config.get('write_to_db') or config.get('write_to_s3'):
                has_shared_resource = True
        
        return {
            'memory_mb': memory_mb,
            'tokens': tokens,
            'llm_calls': llm_calls,
            'has_shared_resource': has_shared_resource
        }

    def _bin_pack_branches(
        self,
        branches: List[Dict[str, Any]],
        resource_estimates: List[Dict[str, int]],
        resource_policy: Dict[str, Any]
    ) -> List[List[Dict[str, Any]]]:
        """
        ğŸ¯ Bin Packing ì•Œê³ ë¦¬ì¦˜: ë¸Œëœì¹˜ë¥¼ ì‹¤í–‰ ë°°ì¹˜ë¡œ ê·¸ë£¹í™”
        
        ì „ëµ:
        1. ë¬´ê±°ìš´ ë¸Œëœì¹˜ ë¨¼ì € ë°°ì¹˜ (First Fit Decreasing)
        2. ê° ë°°ì¹˜ì˜ ì´ ìì›ì´ ì œí•œì„ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ êµ¬ì„±
        3. ê³µìœ  ìì› ì ‘ê·¼ ë¸Œëœì¹˜ëŠ” ë³„ë„ ë°°ì¹˜
        
        Returns:
            [[batch1_branches], [batch2_branches], ...]
        """
        max_memory = resource_policy.get('max_concurrent_memory_mb', DEFAULT_MAX_CONCURRENT_MEMORY_MB)
        max_tokens = resource_policy.get('max_concurrent_tokens', DEFAULT_MAX_CONCURRENT_TOKENS)
        max_branches = resource_policy.get('max_concurrent_branches', DEFAULT_MAX_CONCURRENT_BRANCHES)
        strategy = resource_policy.get('strategy', STRATEGY_RESOURCE_OPTIMIZED)
        
        # ë¸Œëœì¹˜ì™€ ìì› ì¶”ì •ì¹˜ ê²°í•© í›„ í¬ê¸°ìˆœ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
        indexed_branches = list(zip(branches, resource_estimates, range(len(branches))))
        
        # ì „ëµì— ë”°ë¥¸ ì •ë ¬ ê¸°ì¤€
        if strategy == STRATEGY_COST_OPTIMIZED:
            # í† í° ë§ì€ ê²ƒ ë¨¼ì € (ë¹„ìš©ì´ í° ì‘ì—… ìˆœì°¨ ì²˜ë¦¬)
            indexed_branches.sort(key=lambda x: x[1]['tokens'], reverse=True)
        else:
            # ë©”ëª¨ë¦¬ ë§ì€ ê²ƒ ë¨¼ì € (ê¸°ë³¸)
            indexed_branches.sort(key=lambda x: x[1]['memory_mb'], reverse=True)
        
        # ê³µìœ  ìì› ì ‘ê·¼ ë¸Œëœì¹˜ ë¶„ë¦¬
        shared_resource_branches = []
        normal_branches = []
        
        for branch, estimate, idx in indexed_branches:
            if estimate['has_shared_resource']:
                shared_resource_branches.append((branch, estimate, idx))
            else:
                normal_branches.append((branch, estimate, idx))
        
        # Bin Packing (First Fit Decreasing)
        batches: List[List[Tuple]] = []
        batch_resources: List[Dict[str, int]] = []
        
        for branch, estimate, idx in normal_branches:
            placed = False
            
            for i, batch in enumerate(batches):
                current = batch_resources[i]
                
                # ì´ ë°°ì¹˜ì— ì¶”ê°€ ê°€ëŠ¥í•œì§€ í™•ì¸
                new_memory = current['memory_mb'] + estimate['memory_mb']
                new_tokens = current['tokens'] + estimate['tokens']
                new_count = len(batch) + 1
                
                if (new_memory <= max_memory and 
                    new_tokens <= max_tokens and 
                    new_count <= max_branches):
                    
                    batch.append((branch, estimate, idx))
                    batch_resources[i] = {
                        'memory_mb': new_memory,
                        'tokens': new_tokens
                    }
                    placed = True
                    break
            
            if not placed:
                # ìƒˆ ë°°ì¹˜ ìƒì„±
                batches.append([(branch, estimate, idx)])
                batch_resources.append({
                    'memory_mb': estimate['memory_mb'],
                    'tokens': estimate['tokens']
                })
        
        # ê³µìœ  ìì› ë¸Œëœì¹˜ëŠ” ê°ê° ë³„ë„ ë°°ì¹˜ (Race Condition ë°©ì§€)
        for branch, estimate, idx in shared_resource_branches:
            batches.append([(branch, estimate, idx)])
            batch_resources.append({
                'memory_mb': estimate['memory_mb'],
                'tokens': estimate['tokens']
            })
        
        # ê²°ê³¼ ë³€í™˜: ë¸Œëœì¹˜ë§Œ ì¶”ì¶œ
        result = []
        for batch in batches:
            result.append([item[0] for item in batch])
        
        return result

    def _schedule_parallel_group(
        self,
        segment_config: Dict[str, Any],
        state: Dict[str, Any],
        segment_id: int
    ) -> Dict[str, Any]:
        """
        ğŸ”€ ë³‘ë ¬ ê·¸ë£¹ ìŠ¤ì¼€ì¤„ë§: resource_policyì— ë”°ë¼ ì‹¤í–‰ ë°°ì¹˜ ê²°ì •
        
        Returns:
            {
                'status': 'PARALLEL_GROUP' | 'SCHEDULED_PARALLEL',
                'branches': [...] (ì›ë³¸ ë˜ëŠ” ìŠ¤ì¼€ì¤„ëœ ë°°ì¹˜),
                'execution_batches': [[...], [...]] (ë°°ì¹˜ êµ¬ì¡°),
                'scheduling_metadata': {...}
            }
        """
        branches = segment_config.get('branches', [])
        resource_policy = segment_config.get('resource_policy', {})
        
        # resource_policyê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë³‘ë ¬ ì‹¤í–‰
        if not resource_policy:
            logger.info(f"[Scheduler] No resource_policy, using default parallel execution for {len(branches)} branches")
            return {
                'status': 'PARALLEL_GROUP',
                'branches': branches,
                'execution_batches': [branches],  # ë‹¨ì¼ ë°°ì¹˜
                'scheduling_metadata': {
                    'strategy': 'DEFAULT',
                    'total_branches': len(branches),
                    'batch_count': 1
                }
            }
        
        strategy = resource_policy.get('strategy', STRATEGY_RESOURCE_OPTIMIZED)
        
        # SPEED_OPTIMIZED: ê°€ë“œë ˆì¼ ì²´í¬ í›„ ìµœëŒ€ ë³‘ë ¬ ì‹¤í–‰
        if strategy == STRATEGY_SPEED_OPTIMIZED:
            # ğŸ›¡ï¸ ê³„ì • ìˆ˜ì¤€ í•˜ë“œ ë¦¬ë°‹ ì²´í¬ (ì‹œìŠ¤í…œ íŒ¨ë‹‰ ë°©ì§€)
            if len(branches) > ACCOUNT_LAMBDA_CONCURRENCY_LIMIT:
                logger.warning(f"[Scheduler] âš ï¸ SPEED_OPTIMIZED but branch count ({len(branches)}) "
                              f"exceeds account concurrency limit ({ACCOUNT_LAMBDA_CONCURRENCY_LIMIT})")
                # í•˜ë“œ ë¦¬ë°‹ ì ìš©í•˜ì—¬ ë°°ì¹˜ ë¶„í• 
                forced_policy = {
                    'max_concurrent_branches': ACCOUNT_LAMBDA_CONCURRENCY_LIMIT,
                    'max_concurrent_memory_mb': ACCOUNT_MEMORY_HARD_LIMIT_MB,
                    'strategy': STRATEGY_SPEED_OPTIMIZED
                }
                # ìì› ì¶”ì • ë° ë°°ì¹˜ ë¶„í• 
                resource_estimates = [self._estimate_branch_resources(b, state) for b in branches]
                execution_batches = self._bin_pack_branches(branches, resource_estimates, forced_policy)
                
                logger.info(f"[Scheduler] ğŸ›¡ï¸ Guardrail applied: {len(execution_batches)} batches")
                return {
                    'status': 'SCHEDULED_PARALLEL',
                    'branches': branches,
                    'execution_batches': execution_batches,
                    'scheduling_metadata': {
                        'strategy': strategy,
                        'total_branches': len(branches),
                        'batch_count': len(execution_batches),
                        'guardrail_applied': True,
                        'reason': 'Account concurrency limit exceeded'
                    }
                }
            
            logger.info(f"[Scheduler] SPEED_OPTIMIZED: All {len(branches)} branches in parallel")
            return {
                'status': 'PARALLEL_GROUP',
                'branches': branches,
                'execution_batches': [branches],
                'scheduling_metadata': {
                    'strategy': strategy,
                    'total_branches': len(branches),
                    'batch_count': 1,
                    'guardrail_applied': False
                }
            }
        
        # ìì› ì¶”ì •
        resource_estimates = []
        total_memory = 0
        total_tokens = 0
        
        for branch in branches:
            estimate = self._estimate_branch_resources(branch, state)
            resource_estimates.append(estimate)
            total_memory += estimate['memory_mb']
            total_tokens += estimate['tokens']
        
        logger.info(f"[Scheduler] Resource estimates: {total_memory}MB memory, {total_tokens} tokens, "
                   f"{len(branches)} branches")
        
        # ì œí•œ í™•ì¸
        max_memory = resource_policy.get('max_concurrent_memory_mb', DEFAULT_MAX_CONCURRENT_MEMORY_MB)
        max_tokens = resource_policy.get('max_concurrent_tokens', DEFAULT_MAX_CONCURRENT_TOKENS)
        
        # ì œí•œ ë‚´ë¼ë©´ ë‹¨ì¼ ë°°ì¹˜
        if total_memory <= max_memory and total_tokens <= max_tokens:
            logger.info(f"[Scheduler] Resources within limits, single batch execution")
            return {
                'status': 'PARALLEL_GROUP',
                'branches': branches,
                'execution_batches': [branches],
                'scheduling_metadata': {
                    'strategy': strategy,
                    'total_branches': len(branches),
                    'batch_count': 1,
                    'total_memory_mb': total_memory,
                    'total_tokens': total_tokens
                }
            }
        
        # Bin Packingìœ¼ë¡œ ë°°ì¹˜ ìƒì„±
        execution_batches = self._bin_pack_branches(branches, resource_estimates, resource_policy)
        
        logger.info(f"[Scheduler] ğŸ”§ Created {len(execution_batches)} execution batches from {len(branches)} branches")
        for i, batch in enumerate(execution_batches):
            batch_memory = sum(self._estimate_branch_resources(b, state)['memory_mb'] for b in batch)
            logger.info(f"[Scheduler]   Batch {i+1}: {len(batch)} branches, ~{batch_memory}MB")
        
        return {
            'status': 'SCHEDULED_PARALLEL',
            'branches': branches,
            'execution_batches': execution_batches,
            'scheduling_metadata': {
                'strategy': strategy,
                'total_branches': len(branches),
                'batch_count': len(execution_batches),
                'total_memory_mb': total_memory,
                'total_tokens': total_tokens,
                'resource_policy': resource_policy
            }
        }

    def _trigger_child_workflow(self, event: Dict[str, Any], branch_config: Dict[str, Any], auth_user_id: str, quota_id: str) -> Optional[Dict[str, Any]]:
        """
        Triggers a Child Step Function (Standard Orchestrator) for complex branches.
        "Fire and Forget" pattern to avoid Lambda timeouts.
        """
        try:
            import boto3
            import json
            import time
            
            sfn_client = boto3.client('stepfunctions')
            
            # 1. Resolve Orchestrator ARN
            orchestrator_arn = os.environ.get('WORKFLOW_ORCHESTRATOR_ARN')
            if not orchestrator_arn:
                logger.error("WORKFLOW_ORCHESTRATOR_ARN not set. Cannot trigger child workflow.")
                return None

            # 2. Construct Payload (Full 23 fields injection)
            payload = event.copy()
            payload['workflow_config'] = branch_config
            
            parent_workflow_id = payload.get('workflowId') or payload.get('workflow_id', 'unknown')
            parent_idempotency_key = payload.get('idempotency_key', str(time.time()))
            
            # 3. Generate Child Idempotency Key
            branch_id = branch_config.get('id') or f"branch_{int(time.time()*1000)}"
            child_idempotency_key = f"{parent_idempotency_key}_{branch_id}"[:80]
            
            payload['idempotency_key'] = child_idempotency_key
            payload['parent_workflow_id'] = parent_workflow_id
            
            # 4. Start Execution with retry
            safe_exec_name = "".join(c for c in child_idempotency_key if c.isalnum() or c in "-_")
            
            logger.info(f"Triggering Child SFN: {safe_exec_name}")
            
            # [v2.1] Step Functions start_executionì— ì¬ì‹œë„ ì ìš©
            def _start_child_execution():
                return sfn_client.start_execution(
                    stateMachineArn=orchestrator_arn,
                    name=safe_exec_name,
                    input=json.dumps(payload)
                )
            
            if RETRY_UTILS_AVAILABLE:
                response = retry_call(
                    _start_child_execution,
                    max_retries=2,
                    base_delay=0.5,
                    max_delay=5.0
                )
            else:
                response = _start_child_execution()
            
            return {
                "status": "ASYNC_CHILD_WORKFLOW_STARTED",
                "executionArn": response['executionArn'],
                "startDate": response['startDate'].isoformat(),
                "executionName": safe_exec_name
            }
            
        except Exception as e:
            logger.error(f"Failed to trigger child workflow: {e}")
            return None

    def execute_segment(self, event: Dict[str, Any]) -> Dict[str, Any]:
        """
        Main execution logic for a workflow segment.
        """
        # [Fix] ì´ë²¤íŠ¸ì—ì„œ MOCK_MODEë¥¼ ì½ì–´ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì£¼ì…
        # ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë“  í•˜ìœ„ í•¨ìˆ˜ë“¤(invoke_bedrock_model ë“±)ì´ MOCK_MODEë¥¼ ì¸ì‹í•¨
        event_mock_mode = event.get('MOCK_MODE', '').lower()
        if event_mock_mode in ('true', '1', 'yes', 'on'):
            os.environ['MOCK_MODE'] = 'true'
            logger.info("ğŸ§ª MOCK_MODE enabled from event payload")
        
        # 0. Check for Branch Offloading
        branch_config = event.get('branch_config')
        if branch_config:
            force_child = os.environ.get('FORCE_CHILD_WORKFLOW', 'false').lower() == 'true'
            node_count = len(branch_config.get('nodes', [])) if isinstance(branch_config.get('nodes'), list) else 0
            has_hitp = branch_config.get('hitp', False) or any(n.get('hitp') for n in branch_config.get('nodes', []))
            
            should_offload = force_child or node_count > 20 or has_hitp
            
            if should_offload:
                auth_user_id = event.get('ownerId') or event.get('owner_id')
                quota_id = event.get('quota_reservation_id')
                
                child_result = self._trigger_child_workflow(event, branch_config, auth_user_id, quota_id)
                if child_result:
                    return child_result

        # 1. State Bag Normalization
        normalize_inplace(event, remove_state_data=True)
        
        # 2. Extract Context
        auth_user_id = event.get('ownerId') or event.get('owner_id') or event.get('user_id')
        workflow_id = event.get('workflowId') or event.get('workflow_id')
        # ğŸš€ [Hybrid Mode] Support both segment_id (hybrid) and segment_to_run (legacy)
        segment_id = event.get('segment_id') or event.get('segment_to_run', 0)
        s3_bucket = os.environ.get("S3_BUCKET") or os.environ.get("SKELETON_S3_BUCKET")
        
        # 3. Load State (Inline or S3)
        # [Critical Fix] Step Functions passes state as 'current_state', not 'state'
        state_s3_path = event.get('state_s3_path')
        initial_state = event.get('current_state') or event.get('state', {})
        
        if state_s3_path:
            initial_state = self.state_manager.download_state_from_s3(state_s3_path)
            
        # 4. Resolve Segment Config
        # [Critical Fix] Support both test_workflow_config (E2E tests) and workflow_config
        workflow_config = event.get('test_workflow_config') or event.get('workflow_config')
        partition_map = event.get('partition_map')
        partition_map_s3_path = event.get('partition_map_s3_path')
        
        # ğŸš€ [Hybrid Mode] Direct segment_config support for MAP_REDUCE/BATCHED modes
        direct_segment_config = event.get('segment_config')
        execution_mode = event.get('execution_mode')
        
        if direct_segment_config and execution_mode in ('MAP_REDUCE', 'BATCHED'):
            logger.info(f"[Hybrid Mode] Using direct segment_config for {execution_mode} mode")
            segment_config = direct_segment_config
        else:
            # [Critical Fix] Support S3 Offloaded Partition Map with retry
            if not partition_map and partition_map_s3_path:
                try:
                    import boto3
                    import json
                    s3 = boto3.client('s3')
                    bucket_name = partition_map_s3_path.replace("s3://", "").split("/")[0]
                    key_name = "/".join(partition_map_s3_path.replace("s3://", "").split("/")[1:])
                    
                    logger.info(f"Loading partition_map from S3: {partition_map_s3_path}")
                    
                    # [v2.1] S3 get_objectì— ì¬ì‹œë„ ì ìš©
                    def _get_partition_map():
                        obj = s3.get_object(Bucket=bucket_name, Key=key_name)
                        return json.loads(obj['Body'].read().decode('utf-8'))
                    
                    if RETRY_UTILS_AVAILABLE:
                        partition_map = retry_call(
                            _get_partition_map,
                            max_retries=2,
                            base_delay=0.5,
                            max_delay=5.0
                        )
                    else:
                        partition_map = _get_partition_map()
                        
                except Exception as e:
                    logger.error(f"Failed to load partition_map from S3 after retries: {e}")
                    # Fallback to dynamic partitioning (handled in _resolve_segment_config)
            
            segment_config = self._resolve_segment_config(workflow_config, partition_map, segment_id)
        
        # [Critical Fix] parallel_group íƒ€ì… ì„¸ê·¸ë¨¼íŠ¸ëŠ” ë°”ë¡œ PARALLEL_GROUP status ë°˜í™˜
        # ASLì˜ ProcessParallelSegmentsê°€ branchesë¥¼ ë°›ì•„ì„œ Mapìœ¼ë¡œ ë³‘ë ¬ ì‹¤í–‰í•¨
        # ğŸ”€ [Pattern 3] ë³‘ë ¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©
        segment_type = segment_config.get('type') if isinstance(segment_config, dict) else None
        if segment_type == 'parallel_group':
            branches = segment_config.get('branches', [])
            logger.info(f"ğŸ”€ Parallel group detected with {len(branches)} branches")
            
            # ë³‘ë ¬ ìŠ¤ì¼€ì¤„ëŸ¬ í˜¸ì¶œ
            schedule_result = self._schedule_parallel_group(
                segment_config=segment_config,
                state=initial_state,
                segment_id=segment_id
            )
            
            # SCHEDULED_PARALLEL: ë°°ì¹˜ë³„ ìˆœì°¨ ì‹¤í–‰ í•„ìš”
            if schedule_result['status'] == 'SCHEDULED_PARALLEL':
                execution_batches = schedule_result['execution_batches']
                metadata = schedule_result['scheduling_metadata']
                
                logger.info(f"[Scheduler] ğŸ”§ Scheduled {metadata['total_branches']} branches into "
                           f"{metadata['batch_count']} batches (strategy: {metadata['strategy']})")
                
                return {
                    "status": "SCHEDULED_PARALLEL",
                    "final_state": initial_state,
                    "final_state_s3_path": None,
                    "next_segment_to_run": segment_id + 1,
                    "new_history_logs": [],
                    "error_info": None,
                    "branches": branches,
                    "execution_batches": execution_batches,
                    "segment_type": "scheduled_parallel",
                    "scheduling_metadata": metadata,
                    "segment_id": segment_id
                }
            
            # PARALLEL_GROUP: ê¸°ë³¸ ë³‘ë ¬ ì‹¤í–‰
            return {
                "status": "PARALLEL_GROUP",
                "final_state": initial_state,
                "final_state_s3_path": None,
                "next_segment_to_run": segment_id + 1,
                "new_history_logs": [],
                "error_info": None,
                "branches": branches,
                "execution_batches": schedule_result.get('execution_batches', [branches]),
                "segment_type": "parallel_group",
                "scheduling_metadata": schedule_result.get('scheduling_metadata'),
                "segment_id": segment_id
            }
        
        # ğŸ›¡ï¸ [Pattern 2] ì»¤ë„ ê²€ì¦: ì´ ì„¸ê·¸ë¨¼íŠ¸ê°€ SKIPPED ìƒíƒœì¸ê°€?
        segment_status = self._check_segment_status(segment_config)
        if segment_status == SEGMENT_STATUS_SKIPPED:
            skip_reason = segment_config.get('skip_reason', 'Kernel decision')
            logger.info(f"[Kernel] â­ï¸ Segment {segment_id} SKIPPED: {skip_reason}")
            
            # ì»¤ë„ ì•¡ì…˜ ë¡œê·¸ ê¸°ë¡
            kernel_log = {
                'action': 'SKIP',
                'segment_id': segment_id,
                'reason': skip_reason,
                'skipped_by': segment_config.get('skipped_by', 'kernel'),
                'timestamp': time.time()
            }
            
            return {
                "status": "SKIPPED",
                "final_state": initial_state,
                "final_state_s3_path": None,
                "next_segment_to_run": segment_id + 1,
                "new_history_logs": [],
                "error_info": None,
                "branches": None,
                "segment_type": "skipped",
                "kernel_action": kernel_log,
                "segment_id": segment_id
            }
        
        # 5. Apply Self-Healing (Prompt Injection / Refinement)
        self.healer.apply_healing(segment_config, event.get("_self_healing_metadata"))
        
        # 6. Check User Quota / Secret Resolution (Repo access)
        # Note: In a full refactor, this should move to a UserService or AuthMiddleware
        # For now, we keep it simple.
        if auth_user_id:
            try:
                self.repo.get_user(auth_user_id) # Just validating access/existence
            except Exception as e:
                logger.warning("User check failed, but proceeding if possible: %s", e)

        # 7. Execute Workflow Segment
        # ğŸ›¡ï¸ [Pattern 1] ë©”ëª¨ë¦¬ ê¸°ë°˜ ìë™ ë¶„í•  ì ìš©
        user_api_keys = {} # Should be resolved from Secrets Manager or Repo
        
        start_time = time.time()
        
        # ì»¤ë„ ë™ì  ë¶„í•  í™œì„±í™” ì—¬ë¶€ í™•ì¸
        enable_kernel_split = os.environ.get('ENABLE_KERNEL_SPLIT', 'true').lower() == 'true'
        
        if enable_kernel_split and isinstance(segment_config, dict):
            # ğŸ›¡ï¸ [Pattern 1] ìë™ ë¶„í•  ì‹¤í–‰
            result_state = self._execute_with_auto_split(
                segment_config=segment_config,
                initial_state=initial_state,
                auth_user_id=auth_user_id,
                split_depth=segment_config.get('_split_depth', 0)
            )
        else:
            # ê¸°ì¡´ ë¡œì§: ì§ì ‘ ì‹¤í–‰
            result_state = run_workflow(
                config_json=segment_config,
                initial_state=initial_state,
                ddb_table_name=os.environ.get("JOB_TABLE"),
                user_api_keys=user_api_keys,
                run_config={"user_id": auth_user_id}
            )
        
        execution_time = time.time() - start_time
        
        # ğŸ›¡ï¸ [Pattern 2] ì¡°ê±´ë¶€ ìŠ¤í‚µ ê²°ì •
        # ì‹¤í–‰ ê²°ê³¼ì—ì„œ ìŠ¤í‚µí•  ì„¸ê·¸ë¨¼íŠ¸ê°€ ì§€ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
        manifest_s3_path = event.get('segment_manifest_s3_path')
        if manifest_s3_path and isinstance(result_state, dict):
            skip_next_segments = result_state.get('_kernel_skip_segments', [])
            if skip_next_segments:
                skip_reason = result_state.get('_kernel_skip_reason', 'Condition not met')
                self._mark_segments_for_skip(manifest_s3_path, skip_next_segments, skip_reason)
                logger.info(f"[Kernel] Marked {len(skip_next_segments)} segments for skip: {skip_reason}")
            
            # ë³µêµ¬ ì„¸ê·¸ë¨¼íŠ¸ ì‚½ì… ìš”ì²­ ì²˜ë¦¬
            recovery_request = result_state.get('_kernel_inject_recovery')
            if recovery_request:
                self._inject_recovery_segments(
                    manifest_s3_path=manifest_s3_path,
                    after_segment_id=segment_id,
                    recovery_segments=recovery_request.get('segments', []),
                    reason=recovery_request.get('reason', 'Recovery injection')
                )
        
        # 8. Handle Output State Storage
        final_state, output_s3_path = self.state_manager.handle_state_storage(
            state=result_state,
            auth_user_id=auth_user_id,
            workflow_id=workflow_id,
            segment_id=segment_id,
            bucket=s3_bucket,
            threshold=self.threshold
        )
        
        # Extract history logs from result_state if available
        new_history_logs = result_state.get('__new_history_logs', []) if isinstance(result_state, dict) else []
        
        # [Critical Fix] ì›Œí¬í”Œë¡œìš° ì™„ë£Œ ì—¬ë¶€ ê²°ì •
        # 1. test_workflow_configê°€ ì£¼ì…ëœ ê²½ìš° (E2E í…ŒìŠ¤íŠ¸): í•œ ë²ˆì— ì „ì²´ ì‹¤í–‰ í›„ ì™„ë£Œ
        # 2. partition_mapì´ ì—†ëŠ” ê²½ìš°: ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ í•œ ë²ˆì— ì‹¤í–‰í–ˆìœ¼ë¯€ë¡œ ì™„ë£Œ
        # 3. partition_mapì´ ìˆëŠ” ê²½ìš°: ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
        is_e2e_test = event.get('test_workflow_config') is not None
        has_partition_map = partition_map is not None and len(partition_map) > 0
        
        # ğŸ›¡ï¸ ì»¤ë„ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ìˆëŠ” ê²½ìš°)
        kernel_actions = result_state.get('__kernel_actions', []) if isinstance(result_state, dict) else []
        
        if is_e2e_test or not has_partition_map:
            # E2E í…ŒìŠ¤íŠ¸ ë˜ëŠ” íŒŒí‹°ì…˜ ì—†ëŠ” ë‹¨ì¼ ì‹¤í–‰: ì›Œí¬í”Œë¡œìš° ì™„ë£Œ
            return {
                "status": "COMPLETE",  # ASLì´ ê¸°ëŒ€í•˜ëŠ” ìƒíƒœê°’
                "final_state": final_state,
                "final_state_s3_path": output_s3_path,
                "next_segment_to_run": None,  # ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì—†ìŒ
                "new_history_logs": new_history_logs,
                "error_info": None,
                "branches": None,
                "segment_type": "final",
                "state_s3_path": output_s3_path,
                "segment_id": segment_id,
                "execution_time": execution_time,
                "kernel_actions": kernel_actions if kernel_actions else None
            }
        
        # íŒŒí‹°ì…˜ ë§µì´ ìˆëŠ” ê²½ìš°: ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        total_segments = event.get('total_segments', len(partition_map))
        next_segment = segment_id + 1
        
        if next_segment >= total_segments:
            # ë§ˆì§€ë§‰ ì„¸ê·¸ë¨¼íŠ¸ ì™„ë£Œ
            return {
                "status": "COMPLETE",
                "final_state": final_state,
                "final_state_s3_path": output_s3_path,
                "next_segment_to_run": None,
                "new_history_logs": new_history_logs,
                "error_info": None,
                "branches": None,
                "segment_type": "final",
                "state_s3_path": output_s3_path,
                "segment_id": segment_id,
                "execution_time": execution_time,
                "kernel_actions": kernel_actions if kernel_actions else None
            }
        
        # ì•„ì§ ì‹¤í–‰í•  ì„¸ê·¸ë¨¼íŠ¸ê°€ ë‚¨ì•„ìˆìŒ
        return {
            "status": "SUCCEEDED",
            "final_state": final_state,
            "final_state_s3_path": output_s3_path,
            "next_segment_to_run": next_segment,
            "new_history_logs": new_history_logs,
            "error_info": None,
            "branches": None,
            "segment_type": "normal",
            "state_s3_path": output_s3_path,
            "segment_id": segment_id,
            "execution_time": execution_time,
            "kernel_actions": kernel_actions if kernel_actions else None
        }

    def _resolve_segment_config(self, workflow_config, partition_map, segment_id):
        """
        Identical logic to original handler for partitioning.
        """
        if workflow_config:
             # Basic full workflow or pre-chunked
             # If we are strictly running a segment, we might need to simulate partitioning if map is missing
             # For simplicity, we assume workflow_config IS the segment config if partition_map is missing
             # OR we call the dynamic partitioner.
             if not partition_map:
                 # Fallback to dynamic partitioning logic
                 parts = _partition_workflow_dynamically(workflow_config) # arbitrary chunks removed
                 if 0 <= segment_id < len(parts):
                     return parts[segment_id]
                 return workflow_config # Fallback

        # ğŸš¨ [Critical Fix] partition_mapì´ list ë˜ëŠ” dictì¼ ìˆ˜ ìˆìŒ
        if partition_map:
            if isinstance(partition_map, list):
                # listì¸ ê²½ìš°: ì¸ë±ìŠ¤ë¡œ ì ‘ê·¼
                if 0 <= segment_id < len(partition_map):
                    return partition_map[segment_id]
            elif isinstance(partition_map, dict):
                # dictì¸ ê²½ìš°: ë¬¸ìì—´ í‚¤ë¡œ ì ‘ê·¼
                if str(segment_id) in partition_map:
                    return partition_map[str(segment_id)]
            
        # Simplified fallback for readability in pilot
        return workflow_config
