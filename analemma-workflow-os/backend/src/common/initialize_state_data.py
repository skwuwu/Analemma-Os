"""
Initialize State Data - ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì´ˆê¸°í™”

v3.3 - Unified Pipe í†µí•©
    "íƒ„ìƒ(Init)ë¶€í„° ì†Œë©¸ê¹Œì§€ ë‹¨ì¼ íŒŒì´í”„"
    
    - Universal Sync Core(action='init')ë¥¼ í†µí•œ í‘œì¤€í™”ëœ ìƒíƒœ ìƒì„±
    - T=0 ê°€ë“œë ˆì¼: Dirty Input ìë™ ë°©ì–´ (256KB ì´ˆê³¼ ì‹œ ìë™ ì˜¤í”„ë¡œë”©)
    - í•„ìˆ˜ ë©”íƒ€ë°ì´í„° ê°•ì œ ì£¼ì… (Semantic Integrity)
    - íŒŒí‹°ì…”ë‹ ë¡œì§ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì— ì§‘ì¤‘, ì €ì¥ì€ USCì— ìœ„ì„
"""

import os
import json
import time
import logging
import boto3

# Import DecimalEncoder for JSON serialization
try:
    from src.common.json_utils import DecimalEncoder
except ImportError:
    DecimalEncoder = None

# v3.3: Universal Sync Core import (Unified Pipe)
try:
    from src.handlers.utils.universal_sync_core import universal_sync_core
    _HAS_USC = True
except ImportError:
    try:
        # Lambda í™˜ê²½ ëŒ€ì²´ ê²½ë¡œ
        from handlers.utils.universal_sync_core import universal_sync_core
        _HAS_USC = True
    except ImportError:
        _HAS_USC = False
        universal_sync_core = None

# [Priority 1 Optimization] Pre-compilation: Load partition_map from DB
# Runtime partitioning used only as fallback
try:
    from src.services.workflow.partition_service import partition_workflow_advanced
    _HAS_PARTITION = True
except ImportError:
    try:
        from services.workflow.partition_service import partition_workflow_advanced
        _HAS_PARTITION = True
    except ImportError:
        _HAS_PARTITION = False
        partition_workflow_advanced = None

# DynamoDB client (warm start optimization)
try:
    from src.common.aws_clients import get_dynamodb_resource
    _dynamodb = get_dynamodb_resource()
except ImportError:
    _dynamodb = boto3.resource('dynamodb')

# ğŸš¨ [Critical Fix] Match default values with template.yaml
WORKFLOWS_TABLE = os.environ.get('WORKFLOWS_TABLE', 'WorkflowsTableV3')

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# [v3.11] Unified State Hydration Strategy
from src.common.state_hydrator import StateHydrator, SmartStateBag

# [Phase 1] Merkle DAG State Versioning
try:
    from src.services.state.state_versioning_service import StateVersioningService
    _HAS_VERSIONING = True
except ImportError as _versioning_import_err:
    _HAS_VERSIONING = False
    StateVersioningService = None
    logger.error(
        f"[initialize_state_data] StateVersioningService import FAILED: {_versioning_import_err}"
    )

# ğŸ›¡ï¸ [UnboundLocalError Prevention] Pre-import for hash verification
# StateVersioningServiceë¥¼ í•¨ìˆ˜ ë‚´ë¶€ try ë¸”ë¡ì—ì„œ ì„í¬íŠ¸í•˜ë©´
# ì˜ˆì™¸ ë°œìƒ ì‹œ UnboundLocalErrorê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì „ì—­ ì„í¬íŠ¸ ë³´ì¥
try:
    if StateVersioningService is None:
        from src.services.state.state_versioning_service import StateVersioningService as _StateVersioningService_Fallback
        StateVersioningService = _StateVersioningService_Fallback
except ImportError:
    pass  # Already handled above

# Startup diagnostics â€” all _HAS_* flags resolved, visible in CloudWatch cold-start logs
logger.info(
    f"[initialize_state_data] startup flags: "
    f"_HAS_USC={_HAS_USC}, _HAS_PARTITION={_HAS_PARTITION}, _HAS_VERSIONING={_HAS_VERSIONING}"
)

# Environment variables for Merkle DAG
MANIFESTS_TABLE = os.environ.get('MANIFESTS_TABLE', 'WorkflowManifests-v3-dev')
STATE_BUCKET = os.environ.get('WORKFLOW_STATE_BUCKET', 'analemma-workflow-state-dev')

def _calculate_distributed_strategy(
    total_segments: int,
    llm_segments: int,
    hitp_segments: int,
    partition_map: list
) -> dict:
    """
    ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì‚° ì‹¤í–‰ ì „ëµ ê²°ì •
    
    Returns:
        dict: {
            "strategy": "SAFE" | "BATCHED" | "MAP_REDUCE" | "RECURSIVE",
            "max_concurrency": int,
            "batch_size": int (for BATCHED mode),
            "reason": str
        }
    """
    # ğŸ” ì›Œí¬í”Œë¡œìš° íŠ¹ì„± ë¶„ì„
    llm_ratio = llm_segments / max(total_segments, 1)
    hitp_ratio = hitp_segments / max(total_segments, 1)
    
    # ğŸ”— ì˜ì¡´ì„± ë¶„ì„: ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥í•œ ì„¸ê·¸ë¨¼íŠ¸ ê·¸ë£¹ ê³„ì‚°
    independent_segments = 0
    max_dependency_depth = 0
    
    for segment in partition_map:
        deps = segment.get("dependencies", [])
        if not deps:
            independent_segments += 1
        max_dependency_depth = max(max_dependency_depth, len(deps))
    
    independence_ratio = independent_segments / max(total_segments, 1)
    
    logger.info(f"[Strategy Analysis] segments={total_segments}, llm_ratio={llm_ratio:.2f}, "
                f"hitp_ratio={hitp_ratio:.2f}, independence_ratio={independence_ratio:.2f}, "
                f"max_dep_depth={max_dependency_depth}")
    
    # ğŸ“Š ì „ëµ ê²°ì • ë¡œì§
    # 1. HITPê°€ í¬í•¨ëœ ê²½ìš°: ë°˜ë“œì‹œ SAFE ëª¨ë“œ (ì¸ê°„ ìŠ¹ì¸ í•„ìš”)
    if hitp_segments > 0:
        return {
            "strategy": "SAFE",
            "max_concurrency": 1,
            "batch_size": 1,
            "reason": f"HITP segments detected ({hitp_segments}), requires sequential human approval"
        }
    
    # 2. ì†Œê·œëª¨ ì›Œí¬í”Œë¡œìš°: SAFE ëª¨ë“œ
    if total_segments <= 10:
        return {
            "strategy": "SAFE",
            "max_concurrency": min(total_segments, 5),
            "batch_size": 1,
            "reason": f"Small workflow ({total_segments} segments), SAFE mode sufficient"
        }
    
    # 3. ëŒ€ê·œëª¨ + ë†’ì€ ë…ë¦½ì„± + LLM ë¹„ìœ¨ ë†’ìŒ: MAP_REDUCE ëª¨ë“œ
    if total_segments > 100 and independence_ratio > 0.5 and llm_ratio > 0.3:
        return {
            "strategy": "MAP_REDUCE",
            "max_concurrency": 100,  # ë†’ì€ ë³‘ë ¬ì„±
            "batch_size": 25,
            "reason": f"High independence ({independence_ratio:.1%}), LLM-heavy ({llm_ratio:.1%}), optimal for Map-Reduce"
        }
    
    # 4. ì¤‘ê°„ ê·œëª¨ ë˜ëŠ” í˜¼í•© ì›Œí¬í”Œë¡œìš°: BATCHED ëª¨ë“œ
    if total_segments > 10:
        # ë°°ì¹˜ í¬ê¸° ë™ì  ê²°ì •: LLM ë¹„ìœ¨ ë†’ìœ¼ë©´ ì‘ì€ ë°°ì¹˜
        if llm_ratio > 0.5:
            batch_size = 10
            max_concurrency = 10
        else:
            batch_size = 20
            max_concurrency = 20
            
        return {
            "strategy": "BATCHED",
            "max_concurrency": max_concurrency,
            "batch_size": batch_size,
            "reason": f"Medium workflow ({total_segments} segments), batched processing optimal"
        }
    
    # 5. ê¸°ë³¸: SAFE ëª¨ë“œ
    return {
        "strategy": "SAFE",
        "max_concurrency": 2,
        "batch_size": 1,
        "reason": "Default fallback to SAFE mode"
    }

def _calculate_dynamic_concurrency(
    total_segments: int,
    llm_segments: int,
    hitp_segments: int,
    partition_map: list,
    owner_id: str
) -> int:
    """
    Calculate dynamic concurrency based on workflow complexity and user tier
    
    ğŸš¨ [Critical] Specification clarification:
    The max_concurrency returned by this function determines **parallel processing capacity within chunks**.
    
    - Distributed Map's MaxConcurrency is fixed at 1 in ASL (ensures state continuity)
    - This value is used for parallel branch execution within each chunk
    - Applied to parallel group processing within ProcessSegmentChunk
    
    Args:
        total_segments: Total number of segments
        llm_segments: Number of LLM segments
        hitp_segments: Number of HITP segments
        partition_map: Partition map
        owner_id: User ID
        
    Returns:
        int: Calculated chunk-internal MaxConcurrency value (range 5-50)
             â€» Separate from Distributed Map's own MaxConcurrency
    """
    # Default value
    base_concurrency = 15
    
    # 1. Calculate number of parallel branches
    max_parallel_branches = 0
    if partition_map:
        for segment in partition_map:
            if segment.get('type') == 'parallel_group':
                branches = segment.get('branches', [])
                max_parallel_branches = max(max_parallel_branches, len(branches))
    
    # 2. Adjust based on workflow complexity
    calculated_concurrency = base_concurrency # [Safety] Initialize default
    
    if max_parallel_branches == 0:
        # Use default value if no parallel branches
        calculated_concurrency = base_concurrency
    elif max_parallel_branches <= 5:

        # Small-scale parallel (5 or fewer): Execute all concurrently
        calculated_concurrency = max_parallel_branches
    elif max_parallel_branches <= 10:
        # Medium-scale parallel (6-10): Execute 80% concurrently
        calculated_concurrency = int(max_parallel_branches * 0.8)
    elif max_parallel_branches <= 20:
        # Large-scale parallel (11-20): Execute 60% concurrently
        calculated_concurrency = int(max_parallel_branches * 0.6)
    else:
        # Ultra-large-scale parallel (21+): Limit to maximum 30
        calculated_concurrency = min(30, int(max_parallel_branches * 0.5))
    
    # 3. Adjust based on user tier (optional)
    tier_concurrency_multiplier = 1.0
    subscription_plan = 'free'
    
    try:
        user_table = _dynamodb.Table(os.environ.get('USERS_TABLE', 'UsersTableV3'))
        user_response = user_table.get_item(Key={'userId': owner_id})
        user_item = user_response.get('Item')
        
        if user_item:
            subscription_plan = user_item.get('subscription_plan', 'free')
            
            # Apply tier multipliers
            tier_multipliers = {
                'free': 0.5,      # 50% limit
                'basic': 0.75,    # 75%
                'pro': 1.0,       # 100%
                'enterprise': 1.5 # 150% (up to 50 max)
            }
            
            tier_concurrency_multiplier = tier_multipliers.get(subscription_plan, 1.0)
            calculated_concurrency = int(calculated_concurrency * tier_concurrency_multiplier)
            
            logger.info(f"User tier '{subscription_plan}' applied multiplier {tier_concurrency_multiplier}")
    except Exception as e:
        logger.warning(f"Failed to load user tier for concurrency calculation: {e}")
    
    # 4. ğŸ›¡ï¸ [Concurrency Protection] Dynamic OS-level limit by tier
    # Ensures system stability while allowing tier-based scaling
    # 
    # ğŸš¨ [Production Note] Tier-based limits:
    # - free/basic: MAX_OS_LIMIT = 5 (testing/small workflows)
    # - pro: MAX_OS_LIMIT = 20 (medium parallelism)
    # - enterprise: MAX_OS_LIMIT = 50 (high parallelism)
    # 
    # AWS Lambda concurrent execution limit: ~1000 (account-level)
    # Monitor CloudWatch metrics: ConcurrentExecutions, Throttles
    tier_limits = {
        'free': 5,
        'basic': 5,
        'pro': 20,
        'enterprise': 50
    }
    MAX_OS_LIMIT = tier_limits.get(subscription_plan, 5)
    clamped_concurrency = min(calculated_concurrency, MAX_OS_LIMIT)
    
    if calculated_concurrency > MAX_OS_LIMIT:
        logger.warning(
            f"[Concurrency Protection] Clamping requested concurrency {calculated_concurrency} "
            f"to OS limit {MAX_OS_LIMIT}"
        )
    
    # 5. Final range limit (1 ~ MAX_OS_LIMIT)
    final_concurrency = max(1, clamped_concurrency)
    
    logger.info(
        f"Chunk-internal concurrency calculated: {final_concurrency} "
        f"(branches: {max_parallel_branches}, segments: {total_segments}, "
        f"llm: {llm_segments}, hitp: {hitp_segments}, OS_LIMIT: {MAX_OS_LIMIT}) "
        f"â€» Distributed Map MaxConcurrency remains 1 for state continuity"
    )
    
    return final_concurrency


def _load_workflow_config(owner_id: str, workflow_id: str) -> dict:
    """
    Load workflow config from Workflows table.
    Retrieve full config including subgraphs.
    """
    if not owner_id or not workflow_id:
        return None
    
    try:
        table = _dynamodb.Table(WORKFLOWS_TABLE)
        response = table.get_item(
            Key={'ownerId': owner_id, 'workflowId': workflow_id},
            # ğŸ›¡ï¸ [P1 FIX] ì§€ëŠ¥í˜• ë£¨í”„ ì œí•œ í•„ë“œ ì¶”ê°€
            # estimated_executions, loop_analysis ëˆ„ë½ ì‹œ ì¬ì‹¤í–‰ ì‹œ LoopLimitExceeded ë°œìƒ ê°€ëŠ¥
            ProjectionExpression='config, partition_map, total_segments, llm_segments_count, hitp_segments_count, estimated_executions, loop_analysis'
        )
        item = response.get('Item')
        if item and item.get('config'):
            config = item.get('config')
            # Parse if JSON string
            if isinstance(config, str):
                import json
                config = json.loads(config)
            logger.info(f"Loaded workflow config from DB: {workflow_id}")
            return {
                'config': config,
                'partition_map': item.get('partition_map'),
                'total_segments': item.get('total_segments'),
                'llm_segments_count': item.get('llm_segments_count'),
                'hitp_segments_count': item.get('hitp_segments_count'),
                'estimated_executions': item.get('estimated_executions'),  # ğŸ”§ Added
                'loop_analysis': item.get('loop_analysis')  # ğŸ”§ Added
            }
        return None
    except Exception as e:
        logger.warning(f"Failed to load workflow config: {e}")
        return None


def _load_precompiled_partition(owner_id: str, workflow_id: str) -> dict:
    """
    Load pre-compiled partition_map from Workflows table.
    Retrieve partition_map calculated at save time.
    """
    if not owner_id or not workflow_id:
        return None
    
    try:
        table = _dynamodb.Table(WORKFLOWS_TABLE)
        response = table.get_item(
            Key={'ownerId': owner_id, 'workflowId': workflow_id},
            # ğŸ›¡ï¸ [P1 FIX] ì§€ëŠ¥í˜• ë£¨í”„ ì œí•œ í•„ë“œ ì¶”ê°€
            # DBì—ì„œ ë¶ˆëŸ¬ì˜¨ ì›Œí¬í”Œë¡œìš° ì¬ì‹¤í–‰ ì‹œ ë™ì  ì œí•œ ë³´ì¥
            ProjectionExpression='partition_map, total_segments, llm_segments_count, hitp_segments_count, estimated_executions, loop_analysis'
        )
        item = response.get('Item')
        if item and item.get('partition_map'):
            logger.info(f"Loaded pre-compiled partition_map from DB: {item.get('total_segments', 0)} segments")
            return item
        return None
    except Exception as e:
        logger.warning(f"Failed to load pre-compiled partition_map: {e}")
        return None


def lambda_handler(event, context):
    """
    Initializes state data using StateHydrator for strict "SFN Only Sees Pointers" strategy.
    
    Processing Steps:
    1. Resolve inputs (workflow_config, partition_map).
    2. Calculate strategy (distributed vs sequential).
    3. Initialize SmartStateBag.
    4. Dehydrate state (Offload large data to S3).
    5. Return safe payload to Step Functions.
    """
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # [Option B Enhanced] Hybrid Error Handling Strategy
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ì‹œìŠ¤í…œ ì—ëŸ¬ vs ë¹„ì¦ˆë‹ˆìŠ¤ ì—ëŸ¬ êµ¬ë¶„:
    # - ì‹œìŠ¤í…œ ì—ëŸ¬: Exception raise â†’ SFN Retry ì‘ë™ (ì¼ì‹œì  ì¥ì•  ë³µêµ¬)
    # - ë¹„ì¦ˆë‹ˆìŠ¤ ì—ëŸ¬: JSON ë°˜í™˜ â†’ ASL Choice State ê°ì§€ (êµ¬ì¡°ì  ë¬¸ì œ)
    # 
    # ì‹œìŠ¤í…œ ì—ëŸ¬ ì˜ˆì‹œ: ImportError, ConnectionError, S3/DynamoDB Timeout
    # ë¹„ì¦ˆë‹ˆìŠ¤ ì—ëŸ¬ ì˜ˆì‹œ: ì˜ëª»ëœ workflow_config, DAG cycle, validation ì‹¤íŒ¨
    
    # ğŸ›¡ï¸ [P0 FIX] ìµœìƒë‹¨ì—ì„œ ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ë³€ìˆ˜ ë¯¸ë¦¬ í™•ë³´ (UnboundLocalError ì›ì²œ ì°¨ë‹¨)
    # _execute_initialization ë‚´ë¶€ì—ì„œ ë³€ìˆ˜ ì •ì˜ ì „ì— ì˜ˆì™¸ ë°œìƒ ì‹œ
    # ì—ëŸ¬ í•¸ë“¤ëŸ¬ê°€ UnboundLocalErrorë¡œ ìí­í•˜ëŠ” ê²ƒì„ ë°©ì§€
    raw_input = event.get('input') or event.get('initial_state') or event
    if not isinstance(raw_input, dict):
        raw_input = {}
    
    # ì—ëŸ¬ í•¸ë“¤ëŸ¬ì—ì„œ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê¸°ë³¸ê°’ ë³´ì¥
    safe_owner_id = raw_input.get('ownerId', '') or event.get('ownerId', 'system')
    safe_workflow_id = raw_input.get('workflowId', '') or event.get('workflowId', 'unknown')
    safe_exec_id = (
        raw_input.get('idempotency_key') or event.get('idempotency_key') or
        raw_input.get('execution_id') or event.get('execution_id') or 'unknown'
    )
    
    try:
        return _execute_initialization(event, context)
    
    except (ImportError, ConnectionError, TimeoutError) as system_error:
        # ğŸ”„ [System Error] Lambda Retry í™œì„±í™”
        # ì¼ì‹œì  ì¥ì•  ê°€ëŠ¥ì„± â†’ SFNì´ ìë™ ì¬ì‹œë„
        logger.error(
            f"ğŸ”„ [SYSTEM ERROR] Transient failure detected: {system_error}. "
            f"Allowing SFN to retry automatically.",
            exc_info=True
        )
        raise  # Re-raise to trigger SFN Retry mechanism
    
    except Exception as business_error:
        # ğŸ›¡ï¸ [Business Error] Soft-fail with explicit status
        # êµ¬ì¡°ì  ë¬¸ì œ â†’ ì¬ì‹œë„ ë¶ˆí•„ìš”, ASL Choice Stateë¡œ ì²˜ë¦¬
        logger.error(
            f"ğŸ›¡ï¸ [BUSINESS ERROR] Workflow initialization failed: {business_error}",
            exc_info=True
        )
        
        # ğŸ¯ [CRITICAL FIX] ASL ë§¤í•‘ í†µì¼ (Double-Bag ì¤‘ì²© ì œê±° + í•„ë“œëª… ì¼ì¹˜)
        # 
        # ASL êµ¬ì¡°:
        #   ResultSelector: { "bag.$": "$.Payload.state_data", ... }
        #   ResultPath: "$.state_data"
        #   HandleInitErrorResponse: Extract from $.state_data.bag
        #   NotifyAndFailInit: Expects $.init_error OR $.init_error_details
        # 
        # ìµœì¢… JSONPath: $.state_data.bag.error_type
        # 
        # âœ… ì˜¬ë°”ë¥¸ Lambda ë°˜í™˜:
        #   { "state_data": { "error_type": "...", ... }, "init_error": {...} }
        # 
        # âŒ ì˜ëª»ëœ Lambda ë°˜í™˜ (Double Bag ë°œìƒ):
        #   { "state_data": { "bag": { "error_type": "..." } } }
        # 
        # ASLì´ ìë™ìœ¼ë¡œ bag ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ë¯€ë¡œ LambdaëŠ” í‰íƒ„í•œ êµ¬ì¡°ë¡œ ë°˜í™˜!
        
        # ğŸ›¡ï¸ [P0 FIX] ìµœìƒë‹¨ì—ì„œ ë¯¸ë¦¬ ì„ ì–¸í•œ safe ë³€ìˆ˜ ì‚¬ìš©
        # UnboundLocalError ì›ì²œ ì°¨ë‹¨: _execute_initialization ë‚´ë¶€ì—ì„œ
        # ë³€ìˆ˜ ì •ì˜ ì „ì— ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ ì—ëŸ¬ í•¸ë“¤ëŸ¬ê°€ ì•ˆì „í•˜ê²Œ ë™ì‘
        error_payload = {
            'ownerId': safe_owner_id,
            'workflowId': safe_workflow_id,
            'execution_id': safe_exec_id,
            'error_type': type(business_error).__name__,
            'error_message': str(business_error),
            'is_retryable': False
        }
        
        # ğŸ¯ [Final Fix] ASL ëª¨ë“  ê²½ë¡œ ëŒ€ì‘ (Quadruple Mapping)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ASL States that reference error data:
        #   1. ResultSelector: $.Payload.state_data â†’ $.state_data.bag
        #   2. Catch block: $.error (standard AWS convention)
        #   3. NotifyAndFailInit: $.init_error (v3.3+ custom field)
        #   4. Legacy ASL: $.init_error_details (backward compatibility)
        # 
        # Solution: Provide ALL four fields to ensure complete ASL compatibility
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        return {
            'status': 'error',
            'next_action': 'FAILED',
            'state_data': error_payload,         # â‘  $.state_data.bag path
            'init_error': error_payload,         # â‘¡ $.init_error path (v3.3+)
            'init_error_details': error_payload, # â‘¢ $.init_error_details (legacy)
            'error': str(business_error)         # â‘£ $.error path (standard Catch)
        }


def _execute_initialization(event, context):
    """
    Internal implementation of initialization logic.
    Extracted to allow error handling wrapper.
    """
    logger.info("Initializing state data with StateHydrator strategy")
    
    # â”€â”€â”€ [P0 FIX] Safe-Init Pattern: ëª…ì‹œì  ìµœìƒë‹¨ ì´ˆê¸°í™” (UnboundLocalError ë°©ì§€) â”€â”€â”€
    # íŒŒì´ì¬ ìŠ¤ì½”í”„ ì—”ì§„ì´ í•¨ìˆ˜ ë‚´ í• ë‹¹ë¬¸ì„ ë°œê²¬í•˜ë©´ ë¡œì»¬ ë³€ìˆ˜ë¡œ ê°„ì£¼í•˜ë¯€ë¡œ,
    # í• ë‹¹ ì „ ì°¸ì¡° ì‹œ UnboundLocalError ë°œìƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ëª…ì‹œì ìœ¼ë¡œ ì´ˆê¸°í™”
    partition_map = []
    partition_result = {}
    total_segments = 0
    llm_segments = 0
    hitp_segments = 0
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    # 1. State Hydrator Initialization
    # ğŸ›¡ï¸ [P0 FIX] Environment variable consistency: Always use STATE_BUCKET
    # Prevents "file created in bucketA but verified in bucketB" configuration drift
    bucket = STATE_BUCKET  # Use pre-defined constant for consistency
    if not bucket:
        logger.warning(
            "[Configuration Error] STATE_BUCKET is not set. "
            "This will cause S3 operation failures. Check template.yaml environment variables."
        )
    
    hydrator = StateHydrator(bucket_name=bucket)
    
    # 2. Input Resolution
    # SFN ì…ë ¥ì´ {'input': {...}} í˜•íƒœì´ë©´ í•˜ìœ„ dict ì‚¬ìš©.
    # 'initial_state' í‚¤ë„ ì¸ì‹ (v3+ SFN ì…ë ¥ ìŠ¤í‚¤ë§ˆ).
    # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ event ì „ì²´ë¥¼ í´ë°± (ownerId/workflowIdê°€ top-levelì— ìˆëŠ” ë ˆê±°ì‹œ í˜¸ì¶œ í˜¸í™˜).
    # [M-03 Note] event ì „ì²´ í´ë°± ì‹œ orchestrator_selection ë“± SFN ë‚´ë¶€ í•„ë“œê°€ í¬í•¨ë  ìˆ˜ ìˆìŒ.
    #   í•´ë‹¹ í•„ë“œë“¤ì€ force_offload={'input'} ê²½ë¡œë¥¼ í†µí•´ S3ë¡œ ì˜¤í”„ë¡œë“œë˜ì–´ SFN í˜ì´ë¡œë“œì— ì”ë¥˜í•˜ì§€ ì•ŠìŒ.
    raw_input = event.get('input') or event.get('initial_state') or event
    if not isinstance(raw_input, dict):
        raw_input = {}
        
    # [FIX] When raw_input resolves to event['initial_state'], top-level fields
    # (workflowId, ownerId, test_workflow_config) are in event, not in raw_input.
    # Always fall back to event for fields that callers place at the top level.
    owner_id = raw_input.get('ownerId', "") or event.get('ownerId', "")
    workflow_id = raw_input.get('workflowId', "") or event.get('workflowId', "")
    current_time = int(time.time())

    # Generate execution_id for state tracking
    # Use idempotency_key if provided, otherwise generate unique ID
    execution_id = (raw_input.get('idempotency_key') or event.get('idempotency_key')
                    or raw_input.get('execution_id') or event.get('execution_id'))
    if not execution_id:
        import uuid
        execution_id = f"init-{workflow_id}-{int(time.time())}-{str(uuid.uuid4())[:8]}"

    # 2.1 Load Config & Partition Map
    # Priority: Input > DB (Precompiled) > Runtime Calc

    workflow_config = (raw_input.get('test_workflow_config') or raw_input.get('workflow_config')
                       or event.get('test_workflow_config') or event.get('workflow_config'))
    
    # ì…ë ¥ì—ì„œ partition_map ë¡œë“œ ì‹œë„ (Safe-Init íŒ¨í„´ ì ìš©)
    # [v3.18.4 Fix] content partition_map vs workflow segment partition_map êµ¬ë³„
    # LLM_STAGE6 ë“±ì—ì„œ content partition ({ partition_id, items })ì´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ê²½ìš°,
    # initialize_state_dataê°€ workflow segment partitionìœ¼ë¡œ ì˜¤ì¸í•˜ë©´:
    #   - ëŸ°íƒ€ì„ íŒŒí‹°ì…”ë‹ SKIP â†’ total_segments = 3(content) â† í‹€ë¦¼
    #   - estimated_executions = floor(50) â†’ max_loop_iterations = 70 (ê¸°ì¡´ ë²„ê·¸ ì¬ë°œ)
    # íŒë³„ ê¸°ì¤€: workflow segment ì•„ì´í…œì€ ë°˜ë“œì‹œ 'nodes' ë˜ëŠ” 'id' í‚¤ë¥¼ ê°€ì§.
    #            content partition ì•„ì´í…œì€ 'partition_id' + 'items' êµ¬ì¡°.
    def _is_workflow_segment_partition(pm):
        """partition_mapì´ workflow segment êµ¬ì¡°ì¸ì§€ íŒë³„"""
        if not isinstance(pm, list) or len(pm) == 0:
            return False
        first = pm[0]
        if not isinstance(first, dict):
            return False
        # workflow segment: nodes ë˜ëŠ” id(ì •ìˆ˜)+type í‚¤ ì¡°ì¬
        # content partition: partition_id + items í‚¤ ì¡°í•©
        has_content_keys = 'partition_id' in first or 'items' in first
        has_segment_keys = 'nodes' in first or ('id' in first and 'type' in first)
        if has_content_keys and not has_segment_keys:
            return False  # content partition â†’ workflow segmentë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        return True

    input_partition = raw_input.get('partition_map') or event.get('partition_map')
    if input_partition and _is_workflow_segment_partition(input_partition):
        partition_map = input_partition
    elif input_partition:
        logger.info(
            "[v3.18.4] input partition_map detected as CONTENT partition (partition_id/items structure), "
            "not using as workflow segment partition â†’ runtime partitioning will run"
        )
    
    # DB Loader Fallback
    if not workflow_config and workflow_id and owner_id:
        db_data = _load_workflow_config(owner_id, workflow_id)
        if db_data:
            workflow_config = db_data.get('config')
            # Only use DB partition map if not provided in input
            if not partition_map: 
                db_partition = db_data.get('partition_map')
                if db_partition:
                    partition_map = db_partition
            
            # ğŸ›¡ï¸ [P1 FIX] DBì—ì„œ ë£¨í”„ ë¶„ì„ ë°ì´í„° ì¶”ì¶œ (partition_resultì— ì €ì¥)
            # ëŸ°íƒ€ì„ íŒŒí‹°ì…”ë‹ì„ ê±´ë„ˆë›°ì—ˆì„ ë•Œë„ ë™ì  ë£¨í”„ ì œí•œ ê³„ì‚° ê°€ëŠ¥
            if db_data.get('estimated_executions') is not None:
                partition_result = {
                    'estimated_executions': db_data.get('estimated_executions'),
                    'loop_analysis': db_data.get('loop_analysis', {})
                }
                logger.info(
                    f"[DB Load] Restored loop analysis from DB: "
                    f"estimated_executions={partition_result['estimated_executions']}"
                )
    
    # Robustness: Ensure workflow_config is dict
    if not workflow_config:
        workflow_config = {}
        logger.warning("Proceeding with empty workflow_config")
    
    # Runtime Partitioning Fallback
    if not partition_map and _HAS_PARTITION:
        logger.info("Calculating partition_map at runtime...")
        try:
            partition_result = partition_workflow_advanced(workflow_config)
            
            # ğŸ›¡ï¸ [Type Validation] Ensure partition_result is dict
            # Prevents AttributeError if old version returns list
            if not isinstance(partition_result, dict):
                raise ValueError(
                    f"partition_workflow_advanced returned {type(partition_result).__name__}, "
                    f"expected dict. Check partition_service.py version sync."
                )
            
            partition_map = partition_result.get('partition_map', [])
            
            # Validate partition_map is list
            if not isinstance(partition_map, list):
                raise ValueError(
                    f"partition_map is {type(partition_map).__name__}, expected list"
                )
                
        except Exception as e:
            logger.error(f"Partitioning failed: {e}", exc_info=True)
            # [v3.18.3 Fix] Re-raise ALL exceptions from runtime partitioning.
            # An empty partition_map is non-recoverable: the workflow WILL fail later
            # during SFN execution with an obscure error. Failing fast here surfaces
            # the real cause (recursion error, import error, etc.) immediately
            # instead of silently degrading to a broken estimated_executions=floor(50).
            raise
            
    # Metadata Calculation (partition_mapì´ ì´ë¯¸ ìµœìƒë‹¨ì—ì„œ ì´ˆê¸°í™”ë˜ì–´ ì•ˆì „í•¨)
    total_segments = len(partition_map)
    llm_segments = sum(1 for seg in partition_map if seg.get('type') == 'llm')
    hitp_segments = sum(1 for seg in partition_map if seg.get('type') == 'hitp')
    
    # 3. Strategy Calculation
    try:
        distributed_strategy = _calculate_distributed_strategy(
            total_segments=total_segments,
            llm_segments=llm_segments,
            hitp_segments=hitp_segments,
            partition_map=partition_map or []
        )
    except Exception as e:
        logger.error(f"Strategy calculation failed: {e}")
        distributed_strategy = {
            "strategy": "SAFE",
            "max_concurrency": 1,
            "reason": "Strategy calculation failed"
        }
    
    # Concurrency Calculation
    max_concurrency = distributed_strategy.get('max_concurrency', 1)
    is_distributed_mode = distributed_strategy['strategy'] in ["MAP_REDUCE", "BATCHED"]
    
    # 4. [Phase 1] Merkle DAG Manifest Creation
    # ì´ì „: workflow_config + partition_map ì €ì¥ (ì „ì²´ StateBagì˜ 67%)
    # í˜„ì¬: Merkle manifest_id + hashë§Œ ì €ì¥ (ì°¸ì¡° í¬ì¸í„°)
    
    manifest_id = None
    manifest_hash = None
    config_hash = None
    
    if _HAS_VERSIONING:
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Merkle DAG ìƒì„±: ìµœì´ˆ 1íšŒ ì‹œë„ + ì‹¤íŒ¨ ì‹œ 1íšŒ ì¬ì‹œë„
        # ë¬´ê²°ì„± ì›ì¹™: ì¬ìƒì„±ë„ ì‹¤íŒ¨í•˜ë©´ ì›Œí¬í”Œë¡œìš°ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¢…ë£Œ.
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        # ğŸ›¡ï¸ [Infrastructure Validation] GC_DLQ_URL ì‚¬ì „ ê²€ì¦
        # use_2pc=Trueë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ GC_DLQ_URLì´ í•„ìˆ˜
        # ëŸ°íƒ€ì„ ì—ëŸ¬ ëŒ€ì‹  ëª…í™•í•œ ì„¤ì • ì˜¤ë¥˜ ë©”ì‹œì§€ ì œê³µ
        gc_dlq_url = os.environ.get('GC_DLQ_URL')
        if not gc_dlq_url:
            raise RuntimeError(
                "[Infrastructure Error] GC_DLQ_URL environment variable is required "
                "when using 2-Phase Commit (use_2pc=True). "
                "Please configure GC_DLQ_URL in template.yaml or environment settings."
            )
        
        _manifest_last_error = None
        for _attempt in range(1, 3):  # 1íšŒì°¨, 2íšŒì°¨
            try:
                # [BUG-INIT-01 FIX] gc_dlq_url ë° use_2pc ëª…ì‹œì  ì„¤ì •
                # segment_runner_service.pyì™€ ë™ì¼í•œ ì„¤ì • ì‚¬ìš©
                versioning_service = StateVersioningService(
                    dynamodb_table=MANIFESTS_TABLE,
                    s3_bucket=STATE_BUCKET,
                    use_2pc=True,
                    gc_dlq_url=gc_dlq_url
                )

                # segment_manifest ìƒì„± (ë¨¼ì € ê³„ì‚°)
                segment_manifest = []
                for idx, segment in enumerate(partition_map or []):
                    segment_manifest.append({
                        "segment_id": idx,
                        "segment_config": segment,
                        "execution_order": idx,
                        "dependencies": segment.get("dependencies", []),
                        "type": segment.get("type", "normal")
                    })

                # Merkle Manifest ìƒì„±
                manifest_pointer = versioning_service.create_manifest(
                    workflow_id=workflow_id,
                    workflow_config=workflow_config,
                    segment_manifest=segment_manifest,
                    parent_manifest_id=None  # ìµœì´ˆ ë²„ì „
                )

                manifest_id = manifest_pointer.manifest_id
                manifest_hash = manifest_pointer.manifest_hash
                config_hash = manifest_pointer.config_hash

                logger.info(
                    f"[Merkle DAG] Created manifest {manifest_id[:8]}... "
                    f"(attempt={_attempt}/2, hash={manifest_hash[:8]}..., {total_segments} segments)"
                )

                # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                # [Phase 8.1] Pre-flight Check: S3 Strong Consistency + Hash Integrity
                # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                # "ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì‹¤í–‰ì¡°ì°¨ í•˜ì§€ ì•ŠëŠ”ë‹¤"
                # - S3 Propagation Delay ë°©ì–´
                # - Manifest Hash Verification (ë¬´ê²°ì„± ë³´ì¥)
                # - Fail-fast: 700ms ë‚´ ê²€ì¦ ì™„ë£Œ (3íšŒ ì¬ì‹œë„)
                # - Trust Chainì˜ ì²« ë²ˆì§¸ ê²€ì¦ ì§€ì 
                try:
                    from src.services.state.async_commit_service import get_async_commit_service

                    async_commit = get_async_commit_service()
                    manifest_s3_key = f"manifests/{manifest_id}.json"

                    logger.info(
                        f"[Pre-flight Check] Verifying manifest availability: "
                        f"{manifest_id[:8]}..."
                    )

                    commit_status = async_commit.verify_commit_with_retry(
                        execution_id=execution_id,
                        s3_bucket=STATE_BUCKET,
                        s3_key=manifest_s3_key,
                        redis_key=None  # Redis ì—†ì´ S3ë§Œ ê²€ì¦
                    )

                    if not commit_status.s3_available:
                        raise RuntimeError(
                            f"[System Fault] Manifest S3 availability verification failed "
                            f"after {commit_status.retry_count} attempts "
                            f"(total_wait={commit_status.total_wait_ms:.1f}ms). "
                            f"S3 Strong Consistency violation! "
                            f"Manifest: {manifest_id[:8]}..."
                        )
                    
                    # ğŸ›¡ï¸ [Hash Integrity Verification] ì¶”ê°€ ë¬´ê²°ì„± ê²€ì¦
                    # S3 ì¡´ì¬ í™•ì¸ í›„ manifest ë‹¤ìš´ë¡œë“œí•˜ì—¬ í•´ì‹œ ëŒ€ì¡°
                    try:
                        import boto3
                        import hashlib
                        import gzip
                        
                        s3_client = boto3.client('s3')
                        response = s3_client.get_object(Bucket=STATE_BUCKET, Key=manifest_s3_key)
                        raw_content = response['Body'].read()
                        
                        # ğŸ”§ [Critical Fix] Gzip ì••ì¶• ì²˜ë¦¬
                        is_gzipped = raw_content.startswith(b'\x1f\x8b')
                        if is_gzipped:
                            logger.info("[Hash Verification] Gzip-compressed manifest detected, decompressing...")
                            manifest_content = gzip.decompress(raw_content)
                        else:
                            logger.info("[Hash Verification] Plain JSON manifest (expected format)")
                            manifest_content = raw_content
                        
                        # ğŸ”§ [CRITICAL FIX] Hash Recursion Prevention
                        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                        # [Root Cause] "Kernel Panic" from Hash Mismatch:
                        #   - create_manifest hashes ONLY {workflow_id, version, config_hash, segment_hashes}
                        #   - S3 marker includes manifest_hash, manifest_id, created_at, etc.
                        #   - Hashing the entire marker â†’ ALWAYS fails!
                        # 
                        # [Fix Strategy]:
                        #   1. Extract INVARIANT fields only (same as create_manifest)
                        #   2. Normalize types (version must be int, not string)
                        #   3. Re-compute hash from these exact fields
                        #   4. Compare with stored manifest_hash
                        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                        try:
                            # Step 1: Parse manifest marker JSON
                            manifest_obj = json.loads(manifest_content.decode('utf-8'))
                            stored_manifest_hash = manifest_obj.get('manifest_hash')
                            
                            if not stored_manifest_hash:
                                raise RuntimeError(
                                    "[Manifest Corruption] manifest_hash field missing from S3 marker. "
                                    f"Marker keys: {list(manifest_obj.keys())}"
                                )
                            
                            # Step 2: Extract INVARIANT data (matching create_manifest exactly)
                            # ğŸ›¡ï¸ [Critical] Type normalization to prevent hash mismatch:
                            #    - version: JSON deserializes as string, must convert to int
                            #    - segment_hashes: Must be dict (not list)
                            # ğŸ›¡ï¸ [P1 FIX] None-safe type casting: int(manifest_obj.get('version') or 0)
                            #    Prevents TypeError if version is None or ValueError if empty string
                            verification_target = {
                                'workflow_id': manifest_obj.get('workflow_id'),
                                'version': int(manifest_obj.get('version') or 0),  # ğŸ”§ None-safe normalization
                                'config_hash': manifest_obj.get('config_hash'),
                                'segment_hashes': manifest_obj.get('segment_hashes', {})  # ğŸ”§ Default to dict
                            }
                            
                            computed_hash = StateVersioningService.compute_hash(verification_target)
                            
                            logger.info(
                                f"[Hash Verification] Recomputed from invariant fields: "
                                f"{computed_hash[:16]}... (version={verification_target['version']}, "
                                f"segment_count={len(verification_target.get('segment_hashes', {}))})"
                            )
                            
                            # Step 4: Paranoid mode - compare with stored hash
                            if computed_hash != stored_manifest_hash:
                                logger.error(
                                    f"[Tampering Detected] Manifest hash mismatch!\n"
                                    f"Stored:     {stored_manifest_hash[:16]}...\n"
                                    f"Recomputed: {computed_hash[:16]}...\n"
                                    f"Verification target: {verification_target.keys()}"
                                )
                                raise RuntimeError("Manifest marker tampering detected")
                            
                            logger.info("[Hash Verification] âœ… Paranoid check passed (hash matches)")
                            
                        except json.JSONDecodeError as json_err:
                            logger.error(f"[Hash Verification] JSON parse failed: {json_err}")
                            # Fallback: cannot verify, use expected hash as-is
                            computed_hash = manifest_hash
                        
                        if computed_hash != manifest_hash:
                            raise RuntimeError(
                                f"[Integrity Violation] Manifest hash mismatch! "
                                f"Expected: {manifest_hash[:16]}..., "
                                f"Computed: {computed_hash[:16]}... "
                                f"Gzipped: {is_gzipped}, "
                                f"Size: raw={len(raw_content)}B, content={len(manifest_content)}B. "
                                f"This indicates data corruption, tampering, or JSON serialization mismatch. "
                                f"Verify StateVersioningService.compute_hash() consistency (static method)."
                            )
                        
                        logger.info(
                            f"[Hash Verification] âœ… Manifest integrity confirmed: "
                            f"{computed_hash[:16]}... (Gzipped: {is_gzipped}, "
                            f"Method: StateVersioningService.compute_hash [static])"
                        )
                    except s3_client.exceptions.NoSuchKey:
                        raise RuntimeError(
                            f"[System Fault] Manifest disappeared after availability check! "
                            f"Key: {manifest_s3_key}"
                        )

                    logger.info(
                        f"[Pre-flight Check] âœ… Manifest verified: {manifest_id[:8]}... "
                        f"(retries={commit_status.retry_count}, "
                        f"wait={commit_status.total_wait_ms:.1f}ms)"
                    )

                except ImportError:
                    logger.warning(
                        "[Pre-flight Check] AsyncCommitService not available, "
                        "skipping S3 verification (development mode)"
                    )
                except Exception as verify_error:
                    logger.error(
                        f"[Pre-flight Check] Manifest verification failed: {verify_error}",
                        exc_info=True
                    )
                    raise RuntimeError(
                        f"Pre-flight Check failed for manifest {manifest_id[:8]}...: "
                        f"{str(verify_error)}"
                    ) from verify_error

                break  # âœ… ìƒì„± + ê²€ì¦ ëª¨ë‘ ì„±ê³µ â†’ ë£¨í”„ íƒˆì¶œ

            except Exception as e:
                _manifest_last_error = e
                manifest_id = None
                if _attempt < 2:
                    logger.warning(
                        f"[Merkle DAG] Attempt {_attempt}/2 failed: {e}. "
                        f"Retrying manifest creation once more..."
                    )
                else:
                    logger.error(
                        f"[Merkle DAG] Attempt {_attempt}/2 failed: {e}",
                        exc_info=True
                    )

        if not manifest_id:
            raise RuntimeError(
                f"[System Fault] Merkle DAG manifest creation failed after 2 attempts. "
                f"Workflow state integrity cannot be guaranteed â€” aborting execution. "
                f"Last error: {_manifest_last_error}"
            ) from _manifest_last_error
    
    # State Bag Construction
    bag = SmartStateBag({}, hydrator=hydrator)
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # [Phase 2] Merkle DAG Content-Addressable Storage
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ğŸ›¡ï¸ [P1 FIX] manifest_id í•„ìˆ˜ ì¡°ê±´ ì™„í™” (Safe Fallback ë³´ì¥)
    # _HAS_VERSIONING=False í™˜ê²½(import ì‹¤íŒ¨ ë“±)ì—ì„œë„ Legacy ëª¨ë“œë¡œ ë™ì‘ ê°€ëŠ¥
    # manifest_idê°€ ì—†ìœ¼ë©´ ê²½ê³  ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê³„ì† ì§„í–‰ (Legacy ëª¨ë“œ)
    if not manifest_id:
        logger.warning(
            f"âš ï¸ [Legacy Mode] Merkle DAG manifest unavailable, falling back to legacy state storage. "
            f"Diagnostics: _HAS_VERSIONING={_HAS_VERSIONING}, "
            f"workflow_config={'present' if workflow_config else 'missing'}, "
            f"partition_map={'present({} segs)'.format(len(partition_map)) if partition_map else 'empty/None'}. "
            f"This reduces state integrity guarantees but allows workflow to proceed."
        )
        # Legacy ëª¨ë“œì—ì„œëŠ” manifest ê´€ë ¨ í•„ë“œë¥¼ nullë¡œ ì„¤ì •
        manifest_hash = None
        config_hash = None

    # âœ… Merkle DAG Mode: Content-Addressable Storage
    # - workflow_config/partition_map â†’ S3 ë¸”ë¡ìœ¼ë¡œ ì €ì¥ë¨
    # - StateBagì—ëŠ” manifest_id í¬ì¸í„°ë§Œ ì €ì¥ (93% í¬ê¸° ê°ì†Œ)
    # - segment_runnerëŠ” manifestì—ì„œ segment_config ë¡œë“œ
    if manifest_id:
        bag['manifest_id'] = manifest_id
        bag['manifest_hash'] = manifest_hash
        bag['config_hash'] = config_hash

        logger.info(
            f"[Merkle DAG] State storage optimized: "
            f"manifest_id={manifest_id[:8]}..., "
            f"hash={manifest_hash[:8]}..., "
            f"StateBag reduction: ~93%"
        )
    else:
        # Legacy mode: no manifest optimization
        logger.info("[Legacy Mode] No manifest optimization, using direct state storage")
        bag['manifest_id'] = None
        bag['manifest_hash'] = None
        bag['config_hash'] = None
    
    bag['current_state'] = workflow_config.get('initial_state', {})
    
    # Metadata
    bag['ownerId'] = owner_id
    bag['workflowId'] = workflow_id
    bag['idempotency_key'] = (raw_input.get('idempotency_key', "")
                              or event.get('idempotency_key', ""))
    bag['quota_reservation_id'] = (raw_input.get('quota_reservation_id', "")
                                   or event.get('quota_reservation_id', ""))
    bag['segment_to_run'] = 0
    bag['total_segments'] = max(1, total_segments)
    bag['distributed_mode'] = is_distributed_mode
    bag['max_concurrency'] = int(max_concurrency)
    bag['llm_segments'] = llm_segments
    bag['hitp_segments'] = hitp_segments
    # [M-02 FIX] execution_idë¥¼ bagì— ëª…ì‹œì ìœ¼ë¡œ ì €ì¥.
    # segment_runner_serviceì˜ save_state_delta ê°€ event.get('execution_id', 'unknown')ìœ¼ë¡œ
    # ì¡°íšŒí•˜ë¯€ë¡œ, bagì— ì—†ìœ¼ë©´ í•­ìƒ 'unknown'ì´ ê¸°ë¡ë¨.
    bag['execution_id'] = execution_id
    
    # [State Bag] Enforce Input Encapsulation
    # Embed raw input into the bag so it can be offloaded if large
    bag['input'] = raw_input
    bag['loop_counter'] = 0
    
    # ğŸ›¡ï¸ [Dynamic Loop Limit] Segment-based weighted execution counting
    # Formula: (TotalSegments + LoopWeightedSegments) Ã— 1.2 + 20
    # where LoopWeightedSegments = Î£(SegCount Ã— (MaxIter-1)) + 2Ã—ForEachCount
    # 
    # Example 1 - for_each with 30 iterations:
    #   - 3 base segments (prep, for_each, validator)
    #   - for_each adds 2 (parallel_group + aggregator)
    #   - Estimated: (3 + 2) Ã— 1.2 + 20 = 26
    #
    # Example 2 - Sequential loop with 5 iterations, 2 internal segments:
    #   - 4 base segments (prep, loop, seg1, seg2, validator)
    #   - loop adds 2 Ã— (5-1) = 8
    #   - Estimated: (4 + 8) Ã— 1.2 + 20 = 34.4 â‰ˆ 35
    #
    # This prevents LoopLimitExceeded errors while maintaining safety bounds.
    
    # Extract loop analysis from partition_result (if available)
    # ğŸ›¡ï¸ [Type Safety] Validate partition_result is dict before accessing
    # ğŸ›¡ï¸ [P1 FIX] DBì—ì„œ ë¶ˆëŸ¬ì˜¨ ê²½ìš°ë„ ê³ ë ¤ (db_dataì—ì„œ estimated_executions ì¶”ì¶œ)
    if partition_result and isinstance(partition_result, dict):
        # ğŸ›¡ï¸ [v3.18.1 Fix] estimated_executions ì•ˆì „ ì¶”ì¶œ
        # ë²„ê·¸: partition_result.get(key, default) ëŠ” í‚¤ê°€ ì—†ì„ ë•Œë§Œ default ì‚¬ìš©.
        #       DB ì¬ë¡œë“œ ì‹œ estimated_executions ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ í‚¤ ìì²´ê°€ ì—†ì–´
        #       total_segments(ì˜ˆ: 3)ë¡œ í´ë°± â†’ max_loop_iterations = 3+20 = 23
        # ìˆ˜ì •: ê°’ì´ Noneì´ê±°ë‚˜ floor(50) ë¯¸ë§Œì¸ ê²½ìš° ì•ˆì „í•œ ìµœì†Ÿê°’ìœ¼ë¡œ ëŒ€ì²´
        # ğŸ›¡ï¸ [v3.18.6 Fix] Import LOOP_LIMIT_FLOOR from partition_service to stay in sync
        _loop_floor = 100  # default fallback
        if _HAS_PARTITION:
            try:
                from src.services.workflow.partition_service import LOOP_LIMIT_FLOOR as _IMPORTED_FLOOR
                _loop_floor = _IMPORTED_FLOOR
            except Exception:
                pass
        _raw_est = partition_result.get("estimated_executions")
        if isinstance(_raw_est, (int, float)) and _raw_est >= _loop_floor:  # sync with partition_service LOOP_LIMIT_FLOOR
            estimated_executions = int(_raw_est)
        else:
            # estimated_executions ëˆ„ë½(DB êµ¬ ìŠ¤í‚¤ë§ˆ) ë˜ëŠ” ë¹„ì •ìƒ ê°’
            # total_segmentsë§Œìœ¼ë¡œ ê³„ì‚°í•˜ë©´ ë„ˆë¬´ ë‚®ì€ í•œë„ê°€ ì„¤ì •ë¨
            # â†’ max(total_segments * 10, LOOP_LIMIT_FLOOR) ìœ¼ë¡œ ìµœì†Œ floor ë³´ì¥
            estimated_executions = max(total_segments * 10, _loop_floor)  # ì ˆëŒ€ FLOOR ë¯¸ë§Œ ë¶ˆê°€
            logger.warning(
                f"[Dynamic Loop Limit] estimated_executions missing/low in partition_result "
                f"(raw={_raw_est}, floor={_loop_floor}). Using safe fallback: {estimated_executions} "
                f"(total_segments={total_segments} Ã— 10, min={_loop_floor})"
            )
        loop_analysis = partition_result.get("loop_analysis", {})
    else:
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # [v3.18.2 Fix] DB ì¬ë¡œë“œ ê²½ë¡œ â€” estimated_executions ë¯¸ì €ì¥ êµ¬í˜• ë ˆì½”ë“œ ëŒ€ì‘
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ë²„ê·¸ ê²½ë¡œ:
        #   1. save_workflow.pyê°€ v3.17 ì´ì „ì— estimated_executionsë¥¼ DBì— ì €ì¥ ì•ˆ í–ˆìŒ
        #   2. DB ì¬ë¡œë“œ ì‹œ estimated_executions=None â†’ partition_result={} (ë¹ˆ dict)
        #   3. {} is falsy â†’ ì´ else ë¸Œëœì¹˜ ì§„ì…
        #   4. ê¸°ì¡´ ì½”ë“œ: estimated_executions = total_segments (ì˜ˆ: 3)
        #      â†’ max_loop_iterations = 3 + 20 = 23 â†’ LoopLimitExceeded (23íšŒ)
        #
        # ìˆ˜ì •: workflow_configê°€ ìˆìœ¼ë©´ analyze_loop_structures ì¬ì‹¤í–‰ìœ¼ë¡œ ì •í™•í•œ ê°’ ì‚°ì¶œ
        # (full partition ì¬ì‹¤í–‰ ì—†ì´ loop ê°€ì¤‘ì¹˜ë§Œ ê³„ì‚° â€” ê°€ë³ê³  ì •í™•)
        if workflow_config and _HAS_PARTITION:
            try:
                from src.services.workflow.partition_service import (
                    analyze_loop_structures,
                    LOOP_LIMIT_SAFETY_MULTIPLIER,
                    LOOP_LIMIT_FLAT_BONUS,
                    LOOP_LIMIT_FLOOR,
                )
                _nodes = workflow_config.get("nodes", [])
                loop_analysis = analyze_loop_structures(_nodes)
                _weighted = loop_analysis["total_loop_weighted_segments"]
                _base = max(total_segments, 1)
                _raw = _base + _weighted
                estimated_executions = max(
                    int(_raw * LOOP_LIMIT_SAFETY_MULTIPLIER) + LOOP_LIMIT_FLAT_BONUS,
                    LOOP_LIMIT_FLOOR,
                )
                logger.info(
                    f"[Dynamic Loop Limit] Re-computed from workflow_config nodes "
                    f"(DB record missing estimated_executions): "
                    f"base={_base}, weighted={_weighted}, raw={_raw}, "
                    f"estimated_executions={estimated_executions}"
                )
            except Exception as _e:
                logger.warning(
                    f"[Dynamic Loop Limit] analyze_loop_structures fallback failed: {_e}. "
                    f"Using safe floor."
                )
                estimated_executions = max(total_segments * 10, LOOP_LIMIT_FLOOR)
                loop_analysis = {}
        else:
            # workflow_configë„ ì—†ëŠ” ìµœí›„ í´ë°±
            _fl2 = 100
            if _HAS_PARTITION:
                try:
                    from src.services.workflow.partition_service import LOOP_LIMIT_FLOOR as _fl2
                except Exception:
                    pass
            estimated_executions = max(total_segments * 10, _fl2)
            loop_analysis = {}
            logger.warning(
                f"[Dynamic Loop Limit] No partition_result and no workflow_config. "
                f"Using floor fallback: estimated_executions={estimated_executions} "
                f"(total_segments={total_segments} Ã— 10, min={_fl2})"
            )
    
    # Apply safety margin: 25% of estimate or minimum 20
    # [v3.18.5] 20% â†’ 25%: stage6 ë“± ê³ ë³µì¡ë„ ì›Œí¬í”Œë¡œì—ì„œ estimateê°€
    #   ì‹¤ì œ ì‹¤í–‰ íšŸìˆ˜ì— ê·¼ì†Œí•˜ê²Œ ëª» ë¯¸ì¹˜ëŠ” ì‚¬ë¡€ ë°©ì§€ (ì˜ˆ: estimate=297, ì‹¤í–‰=360)
    safety_margin = max(int(estimated_executions * 0.25), 20)
    default_loop_limit = estimated_executions + safety_margin
    
    # Branch loop limit: 50% of main limit or minimum 50
    default_branch_limit = max(int(default_loop_limit * 0.5), 50)
    
    # User can override via workflow_config, otherwise use dynamic calculation
    bag['max_loop_iterations'] = workflow_config.get('max_loop_iterations', default_loop_limit)
    bag['max_branch_iterations'] = workflow_config.get('max_branch_iterations', default_branch_limit)
    
    logger.info(
        f"[Dynamic Loop Limit] "
        f"estimated_executions={estimated_executions}, "
        f"safety_margin={safety_margin}, "
        f"loop_limit={bag['max_loop_iterations']}, "
        f"branch_limit={bag['max_branch_iterations']}, "
        f"loop_nodes={loop_analysis.get('loop_count', 0)}"
    )
    
    bag['start_time'] = current_time
    bag['last_update_time'] = current_time
    bag['state_durations'] = {}
    
    # [v3.23] ì‹œë®¬ë ˆì´í„° í”Œë˜ê·¸ë¥¼ bag ìµœìƒìœ„ë¡œ ë³µì‚¬
    # store_task_token.pyê°€ bag.get('AUTO_RESUME_HITP') ì¡°íšŒ
    auto_resume = raw_input.get('AUTO_RESUME_HITP') or event.get('AUTO_RESUME_HITP')
    if auto_resume:
        bag['AUTO_RESUME_HITP'] = auto_resume
    mock_mode = raw_input.get('MOCK_MODE') or event.get('MOCK_MODE')
    if mock_mode:
        bag['MOCK_MODE'] = mock_mode
    
    # 5. [Phase 1/2] Segment Manifest Strategy
    # Merkle DAG ëª¨ë“œ: segment_manifestëŠ” ì´ë¯¸ S3ì— ì €ì¥ë¨ (StateVersioningService)
    # Legacy ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ S3ì— ì—…ë¡œë“œ
    
    manifest_s3_path = ""
    
    if manifest_id:
        # Merkle DAG: ManifestëŠ” ì´ë¯¸ Content-Addressable ë¸”ë¡ìœ¼ë¡œ ì €ì¥ë¨
        # manifest_idë¡œ segment_manifest ì ‘ê·¼ ê°€ëŠ¥
        manifest_s3_path = f"s3://{STATE_BUCKET}/manifests/{manifest_id}.json"
        logger.info(f"[Merkle DAG] Using manifest: {manifest_s3_path}")
        
    elif partition_map and hydrator.s3_client and bucket:
        # Legacy ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ segment_manifest ìƒì„±/ì €ì¥
        segment_manifest = []
        for idx, segment in enumerate(partition_map or []):
            segment_manifest.append({
                "segment_id": idx,
                "segment_config": segment,
                "execution_order": idx,
                "dependencies": segment.get("dependencies", []),
                "type": segment.get("type", "normal")
            })
        
        # Legacy manifest upload
        try:
            manifest_key = f"workflow-manifests/{owner_id}/{workflow_id}/segment_manifest.json"
            hydrator.s3_client.put_object(
                Bucket=bucket,
                Key=manifest_key,
                Body=json.dumps(segment_manifest, default=str),
                ContentType='application/json'
            )
            manifest_s3_path = f"s3://{bucket}/{manifest_key}"
            logger.info(f"[Legacy] Segment manifest uploaded: {manifest_s3_path}")
        except Exception as e:
            logger.warning(f"Failed to upload manifest: {e}")
            
    # Create Pointers List for ItemProcessor (Map stateì—ì„œ ì‚¬ìš©)
    segment_manifest_pointers = []
    if manifest_s3_path:
        # S3 ê²½ë¡œë§Œ ì°¸ì¡°í•˜ëŠ” ê²½ëŸ‰ í¬ì¸í„°
        # Merkle DAG ëª¨ë“œì—ì„œëŠ” segment_indexë¡œ S3 Select ì¿¼ë¦¬
        if manifest_id:
            # Merkle DAG: manifest_id + segment_indexë¡œ ë¸”ë¡ ì ‘ê·¼
            for idx in range(total_segments):
                segment_manifest_pointers.append({
                    'segment_index': idx,
                    'segment_id': idx,
                    'segment_type': 'computed',  # Merkle DAGì—ì„œ íƒ€ì… ì¶”ë¡ 
                    'manifest_id': manifest_id,
                    'manifest_s3_path': manifest_s3_path,
                    'total_segments': total_segments
                })
        else:
            # Legacy: ê¸°ì¡´ ë°©ì‹
            segment_manifest = []
            if partition_map:
                for idx, segment in enumerate(partition_map or []):
                    segment_manifest.append({
                        "segment_id": idx,
                        "segment_config": segment,
                        "execution_order": idx,
                        "dependencies": segment.get("dependencies", []),
                        "type": segment.get("type", "normal")
                    })
            
            for idx, seg in enumerate(segment_manifest):
                segment_manifest_pointers.append({
                    'segment_index': idx,
                    'segment_id': seg.get('segment_id', idx),
                    'segment_type': seg.get('type', 'normal'),
                    'manifest_s3_path': manifest_s3_path,
                    'total_segments': len(segment_manifest)
                })
    else:
        # Fallback to inline (only if S3 failed/missing)
        if partition_map:
            segment_manifest_pointers = []
            for idx, segment in enumerate(partition_map or []):
                segment_manifest_pointers.append({
                    "segment_id": idx,
                    "segment_config": segment,
                    "execution_order": idx,
                    "dependencies": segment.get("dependencies", []),
                    "type": segment.get("type", "normal")
                })

    # Add to bag
    # [C-01 FIX] segment_manifest_pointersëŠ” ê²½ëŸ‰ í¬ì¸í„° ë°°ì—´ (~200B Ã— n ì„¸ê·¸ë¨¼íŠ¸).
    # ASL v3 Map stateì˜ ItemsPath: "$.state_data.bag.segment_manifest" ê°€ ì´ ë°°ì—´ì„ ì°¸ì¡°.
    # í¬ì¸í„°ê°€ ì—†ìœ¼ë©´ BATCHED/MAP_REDUCE ëª¨ë“œì—ì„œ Map stateê°€ 0íšŒ ì‹¤í–‰ë˜ì–´ ì „ë©´ ë¶ˆëŠ¥.
    # ì „ì²´ segment_configê°€ ì•„ë‹Œ í¬ì¸í„°ë§Œ ë‹´ìœ¼ë¯€ë¡œ 256KB SFN í•œë„ ë‚´ì— ìœ ì§€ë¨.
    bag['segment_manifest'] = segment_manifest_pointers
    bag['segment_manifest_s3_path'] = manifest_s3_path

    # 6. Dehydrate Final Payload
    # Force offload large fields to Ensure "SFN Only Sees Pointers"
    # Added 'input' to forced offload list to prevent Zombie Data
    # Note: 'segment_manifest' is intentionally NOT force-offloaded â€” it is a
    #   lightweight pointer array required inline by ASL Map state ItemsPath.
    force_offload = {'workflow_config', 'partition_map', 'current_state', 'input'}
    
    payload = hydrator.dehydrate(
        state=bag,
        owner_id=owner_id,
        workflow_id=workflow_id,
        execution_id=execution_id,
        return_delta=False,
        force_offload_fields=force_offload
    )
    
    # 7. Post-Processing & Compatibility
    # [C-01] segment_manifest_pointersëŠ” bag['segment_manifest']ì— ì´ë¯¸ ì €ì¥ë¨.
    # dehydrate() ì´í›„ì—ë„ í¬ì¸í„° ë°°ì—´ì´ payloadì— ìœ ì§€ë˜ì–´ì•¼ ASL Map stateê°€ ë™ì‘í•¨.
    
    # Ensure compatibility aliases exist if fields were offloaded
    # (ASL might reference _s3_path fields directly)
    for field in ['workflow_config', 'partition_map', 'current_state', 'input']:
        val = payload.get(field)
        if isinstance(val, dict) and val.get('__s3_offloaded'):
            payload[f"{field}_s3_path"] = val.get('__s3_path')
            
    # Explicitly set state_s3_path alias for current_state
    if payload.get('current_state_s3_path'):
        payload['state_s3_path'] = payload['current_state_s3_path']
        
    # [No Data at Root] REMOVED inline input copy to prevent payload explosion
    # payload['input'] = raw_input - DELETED
    
    logger.info(f"âœ… State Initialization Complete. Keys: {list(payload.keys())}")
    
    # ï¿½ [v3.13] Kernel Protocol - The Great Seal Pattern
    # seal_state_bagì„ ì‚¬ìš©í•˜ì—¬ í‘œì¤€ ì‘ë‹µ í¬ë§· ìƒì„±
    try:
        from src.common.kernel_protocol import seal_state_bag
        _HAS_KERNEL_PROTOCOL = True
    except ImportError:
        try:
            from common.kernel_protocol import seal_state_bag
            _HAS_KERNEL_PROTOCOL = True
        except ImportError:
            _HAS_KERNEL_PROTOCOL = False
    
    if _HAS_KERNEL_PROTOCOL:
        logger.info("ğŸ’ [v3.13] Using Kernel Protocol seal_state_bag")
        
        # Ensure idempotency_key is available
        idempotency_key = bag.get('idempotency_key') or raw_input.get('idempotency_key') or "unknown"
        
        # seal_state_bag: USC í˜¸ì¶œ + í‘œì¤€ í¬ë§· ë°˜í™˜
        response_data = seal_state_bag(
            base_state={},  # ë¹ˆ ìƒíƒœì—ì„œ ì‹œì‘
            result_delta=payload,
            action='init',
            context={
                'execution_id': idempotency_key,
                'idempotency_key': idempotency_key
            }
        )
        
        logger.info(f"âœ… [Kernel Protocol] Init complete: next_action={response_data.get('next_action')}")
    elif _HAS_USC and universal_sync_core:
        # Fallback to direct USC (if kernel_protocol not available)
        logger.warning("âš ï¸ Kernel Protocol not available, using direct USC")
        
        idempotency_key = bag.get('idempotency_key') or raw_input.get('idempotency_key') or "unknown"
        
        usc_result = universal_sync_core(
            base_state={},
            new_result=payload,
            context={
                'action': 'init',
                'execution_id': idempotency_key,
                'idempotency_key': idempotency_key
            }
        )
        
        # í‘œì¤€ í¬ë§·ìœ¼ë¡œ ë°˜í™˜
        response_data = {
            'state_data': usc_result.get('state_data', {}),
            'next_action': usc_result.get('next_action', 'STARTED')
        }
    else:
        # USCë„ ì—†ëŠ” ê²½ìš° í´ë°±
        logger.warning("âš ï¸ Universal Sync Core not available, using legacy initialization")
        
        payload['segment_to_run'] = 0
        payload['loop_counter'] = 0
        payload['state_history'] = []
        payload['last_update_time'] = current_time
        
        response_data = {
            'state_data': payload,
            'next_action': 'STARTED'
        }
    
    # ìµœì¢… í¬ê¸° ê²€ì¦
    response_json = json.dumps(response_data, default=str, ensure_ascii=False)
    response_size_kb = len(response_json.encode('utf-8')) / 1024
    
    logger.info(f"âœ… InitializeStateData response: {response_size_kb:.1f}KB")
    
    if response_size_kb > 250:
        logger.error(
            f"ğŸš¨ CRITICAL: Response exceeds 250KB ({response_size_kb:.1f}KB)! "
            f"Step Functions will reject with DataLimitExceeded."
        )
    elif response_size_kb > 200:
        logger.warning(f"âš ï¸ Response is {response_size_kb:.1f}KB (>200KB). Close to limit.")
    
    # [Option B] Add explicit success status for ASL Choice State validation
    response_data['status'] = 'success'
    
    return response_data

