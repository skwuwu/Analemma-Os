"""
Initialize State Data - ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì´ˆê¸°í™”

v3.3 - Unified Pipe í†µí•©
    "íƒ„ìƒ(Init)ë¶€í„° ì†Œë©¸ê¹Œì§€ ë‹¨ì¼ íŒŒì´í”„"
    
    - Universal Sync Core(action='init')ë¥¼ í†µí•œ í‘œì¤€í™”ëœ ìƒíƒœ ìƒì„±
    - T=0 ê°€ë“œë ˆì¼: Dirty Input ìë™ ë°©ì–´ (256KB ì´ˆê³¼ ì‹œ ìë™ ì˜¤í”„ë¡œë”©)
    - í•„ìˆ˜ ë©”íƒ€ë°ì´í„° ê°•ì œ ì£¼ì… (Semantic Integrity)
    - íŒŒí‹°ì…”ë‹ ë¡œì§ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì— ì§‘ì¤‘, ì €ì¥ì€ USCì— ìœ„ì„
"""

import os
import json
import time
import logging
import boto3

# Import DecimalEncoder for JSON serialization
try:
    from src.common.json_utils import DecimalEncoder
except ImportError:
    DecimalEncoder = None

# v3.3: Universal Sync Core import (Unified Pipe)
try:
    from src.handlers.utils.universal_sync_core import universal_sync_core
    _HAS_USC = True
except ImportError:
    try:
        # Lambda í™˜ê²½ ëŒ€ì²´ ê²½ë¡œ
        from handlers.utils.universal_sync_core import universal_sync_core
        _HAS_USC = True
    except ImportError:
        _HAS_USC = False
        universal_sync_core = None

# [Priority 1 Optimization] Pre-compilation: Load partition_map from DB
# Runtime partitioning used only as fallback
try:
    from src.services.workflow.partition_service import partition_workflow_advanced
    _HAS_PARTITION = True
except ImportError:
    try:
        from services.workflow.partition_service import partition_workflow_advanced
        _HAS_PARTITION = True
    except ImportError:
        _HAS_PARTITION = False
        partition_workflow_advanced = None

# DynamoDB client (warm start optimization)
try:
    from src.common.aws_clients import get_dynamodb_resource
    _dynamodb = get_dynamodb_resource()
except ImportError:
    _dynamodb = boto3.resource('dynamodb')

# ğŸš¨ [Critical Fix] Match default values with template.yaml
WORKFLOWS_TABLE = os.environ.get('WORKFLOWS_TABLE', 'WorkflowsTableV3')

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# [v3.11] Unified State Hydration Strategy
from src.common.state_hydrator import StateHydrator, SmartStateBag

# [Phase 1] Merkle DAG State Versioning
try:
    from src.services.state.state_versioning_service import StateVersioningService
    _HAS_VERSIONING = True
except ImportError as _versioning_import_err:
    _HAS_VERSIONING = False
    StateVersioningService = None
    logger.error(
        f"[initialize_state_data] StateVersioningService import FAILED: {_versioning_import_err}"
    )

# Startup diagnostics â€” all _HAS_* flags resolved, visible in CloudWatch cold-start logs
logger.info(
    f"[initialize_state_data] startup flags: "
    f"_HAS_USC={_HAS_USC}, _HAS_PARTITION={_HAS_PARTITION}, _HAS_VERSIONING={_HAS_VERSIONING}"
)

# Environment variables for Merkle DAG
MANIFESTS_TABLE = os.environ.get('MANIFESTS_TABLE', 'WorkflowManifests-v3-dev')
STATE_BUCKET = os.environ.get('WORKFLOW_STATE_BUCKET', 'analemma-workflow-state-dev')

def _calculate_distributed_strategy(
    total_segments: int,
    llm_segments: int,
    hitp_segments: int,
    partition_map: list
) -> dict:
    """
    ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì‚° ì‹¤í–‰ ì „ëµ ê²°ì •
    
    Returns:
        dict: {
            "strategy": "SAFE" | "BATCHED" | "MAP_REDUCE" | "RECURSIVE",
            "max_concurrency": int,
            "batch_size": int (for BATCHED mode),
            "reason": str
        }
    """
    # ğŸ” ì›Œí¬í”Œë¡œìš° íŠ¹ì„± ë¶„ì„
    llm_ratio = llm_segments / max(total_segments, 1)
    hitp_ratio = hitp_segments / max(total_segments, 1)
    
    # ğŸ”— ì˜ì¡´ì„± ë¶„ì„: ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥í•œ ì„¸ê·¸ë¨¼íŠ¸ ê·¸ë£¹ ê³„ì‚°
    independent_segments = 0
    max_dependency_depth = 0
    
    for segment in partition_map:
        deps = segment.get("dependencies", [])
        if not deps:
            independent_segments += 1
        max_dependency_depth = max(max_dependency_depth, len(deps))
    
    independence_ratio = independent_segments / max(total_segments, 1)
    
    logger.info(f"[Strategy Analysis] segments={total_segments}, llm_ratio={llm_ratio:.2f}, "
                f"hitp_ratio={hitp_ratio:.2f}, independence_ratio={independence_ratio:.2f}, "
                f"max_dep_depth={max_dependency_depth}")
    
    # ğŸ“Š ì „ëµ ê²°ì • ë¡œì§
    # 1. HITPê°€ í¬í•¨ëœ ê²½ìš°: ë°˜ë“œì‹œ SAFE ëª¨ë“œ (ì¸ê°„ ìŠ¹ì¸ í•„ìš”)
    if hitp_segments > 0:
        return {
            "strategy": "SAFE",
            "max_concurrency": 1,
            "batch_size": 1,
            "reason": f"HITP segments detected ({hitp_segments}), requires sequential human approval"
        }
    
    # 2. ì†Œê·œëª¨ ì›Œí¬í”Œë¡œìš°: SAFE ëª¨ë“œ
    if total_segments <= 10:
        return {
            "strategy": "SAFE",
            "max_concurrency": min(total_segments, 5),
            "batch_size": 1,
            "reason": f"Small workflow ({total_segments} segments), SAFE mode sufficient"
        }
    
    # 3. ëŒ€ê·œëª¨ + ë†’ì€ ë…ë¦½ì„± + LLM ë¹„ìœ¨ ë†’ìŒ: MAP_REDUCE ëª¨ë“œ
    if total_segments > 100 and independence_ratio > 0.5 and llm_ratio > 0.3:
        return {
            "strategy": "MAP_REDUCE",
            "max_concurrency": 100,  # ë†’ì€ ë³‘ë ¬ì„±
            "batch_size": 25,
            "reason": f"High independence ({independence_ratio:.1%}), LLM-heavy ({llm_ratio:.1%}), optimal for Map-Reduce"
        }
    
    # 4. ì¤‘ê°„ ê·œëª¨ ë˜ëŠ” í˜¼í•© ì›Œí¬í”Œë¡œìš°: BATCHED ëª¨ë“œ
    if total_segments > 10:
        # ë°°ì¹˜ í¬ê¸° ë™ì  ê²°ì •: LLM ë¹„ìœ¨ ë†’ìœ¼ë©´ ì‘ì€ ë°°ì¹˜
        if llm_ratio > 0.5:
            batch_size = 10
            max_concurrency = 10
        else:
            batch_size = 20
            max_concurrency = 20
            
        return {
            "strategy": "BATCHED",
            "max_concurrency": max_concurrency,
            "batch_size": batch_size,
            "reason": f"Medium workflow ({total_segments} segments), batched processing optimal"
        }
    
    # 5. ê¸°ë³¸: SAFE ëª¨ë“œ
    return {
        "strategy": "SAFE",
        "max_concurrency": 2,
        "batch_size": 1,
        "reason": "Default fallback to SAFE mode"
    }

def _calculate_dynamic_concurrency(
    total_segments: int,
    llm_segments: int,
    hitp_segments: int,
    partition_map: list,
    owner_id: str
) -> int:
    """
    Calculate dynamic concurrency based on workflow complexity and user tier
    
    ğŸš¨ [Critical] Specification clarification:
    The max_concurrency returned by this function determines **parallel processing capacity within chunks**.
    
    - Distributed Map's MaxConcurrency is fixed at 1 in ASL (ensures state continuity)
    - This value is used for parallel branch execution within each chunk
    - Applied to parallel group processing within ProcessSegmentChunk
    
    Args:
        total_segments: Total number of segments
        llm_segments: Number of LLM segments
        hitp_segments: Number of HITP segments
        partition_map: Partition map
        owner_id: User ID
        
    Returns:
        int: Calculated chunk-internal MaxConcurrency value (range 5-50)
             â€» Separate from Distributed Map's own MaxConcurrency
    """
    # Default value
    base_concurrency = 15
    
    # 1. Calculate number of parallel branches
    max_parallel_branches = 0
    if partition_map:
        for segment in partition_map:
            if segment.get('type') == 'parallel_group':
                branches = segment.get('branches', [])
                max_parallel_branches = max(max_parallel_branches, len(branches))
    
    # 2. Adjust based on workflow complexity
    calculated_concurrency = base_concurrency # [Safety] Initialize default
    
    if max_parallel_branches == 0:
        # Use default value if no parallel branches
        calculated_concurrency = base_concurrency
    elif max_parallel_branches <= 5:

        # Small-scale parallel (5 or fewer): Execute all concurrently
        calculated_concurrency = max_parallel_branches
    elif max_parallel_branches <= 10:
        # Medium-scale parallel (6-10): Execute 80% concurrently
        calculated_concurrency = int(max_parallel_branches * 0.8)
    elif max_parallel_branches <= 20:
        # Large-scale parallel (11-20): Execute 60% concurrently
        calculated_concurrency = int(max_parallel_branches * 0.6)
    else:
        # Ultra-large-scale parallel (21+): Limit to maximum 30
        calculated_concurrency = min(30, int(max_parallel_branches * 0.5))
    
    # 3. Adjust based on user tier (optional)
    try:
        user_table = _dynamodb.Table(os.environ.get('USERS_TABLE', 'UsersTableV3'))
        user_response = user_table.get_item(Key={'userId': owner_id})
        user_item = user_response.get('Item')
        
        if user_item:
            subscription_plan = user_item.get('subscription_plan', 'free')
            
            # Apply tier multipliers
            tier_multipliers = {
                'free': 0.5,      # 50% limit
                'basic': 0.75,    # 75%
                'pro': 1.0,       # 100%
                'enterprise': 1.5 # 150% (up to 50 max)
            }
            
            multiplier = tier_multipliers.get(subscription_plan, 1.0)
            calculated_concurrency = int(calculated_concurrency * multiplier)
            
            logger.info(f"User tier '{subscription_plan}' applied multiplier {multiplier}")
    except Exception as e:
        logger.warning(f"Failed to load user tier for concurrency calculation: {e}")
    
    # 4. ğŸ›¡ï¸ [Concurrency Protection] OS-level upper limit clamping
    # Ensure overall system stability according to account concurrency limit
    # Increased from 2 to 5 to enable proper testing of parallel scheduling strategies
    MAX_OS_LIMIT = 5  # Allows testing of batch splitting and speed guardrails
    clamped_concurrency = min(calculated_concurrency, MAX_OS_LIMIT)
    
    if calculated_concurrency > MAX_OS_LIMIT:
        logger.warning(
            f"[Concurrency Protection] Clamping requested concurrency {calculated_concurrency} "
            f"to OS limit {MAX_OS_LIMIT}"
        )
    
    # 5. Final range limit (1 ~ MAX_OS_LIMIT)
    final_concurrency = max(1, clamped_concurrency)
    
    logger.info(
        f"Chunk-internal concurrency calculated: {final_concurrency} "
        f"(branches: {max_parallel_branches}, segments: {total_segments}, "
        f"llm: {llm_segments}, hitp: {hitp_segments}, OS_LIMIT: {MAX_OS_LIMIT}) "
        f"â€» Distributed Map MaxConcurrency remains 1 for state continuity"
    )
    
    return final_concurrency


def _load_workflow_config(owner_id: str, workflow_id: str) -> dict:
    """
    Load workflow config from Workflows table.
    Retrieve full config including subgraphs.
    """
    if not owner_id or not workflow_id:
        return None
    
    try:
        table = _dynamodb.Table(WORKFLOWS_TABLE)
        response = table.get_item(
            Key={'ownerId': owner_id, 'workflowId': workflow_id},
            ProjectionExpression='config, partition_map, total_segments, llm_segments_count, hitp_segments_count'
        )
        item = response.get('Item')
        if item and item.get('config'):
            config = item.get('config')
            # Parse if JSON string
            if isinstance(config, str):
                import json
                config = json.loads(config)
            logger.info(f"Loaded workflow config from DB: {workflow_id}")
            return {
                'config': config,
                'partition_map': item.get('partition_map'),
                'total_segments': item.get('total_segments'),
                'llm_segments_count': item.get('llm_segments_count'),
                'hitp_segments_count': item.get('hitp_segments_count')
            }
        return None
    except Exception as e:
        logger.warning(f"Failed to load workflow config: {e}")
        return None


def _load_precompiled_partition(owner_id: str, workflow_id: str) -> dict:
    """
    Load pre-compiled partition_map from Workflows table.
    Retrieve partition_map calculated at save time.
    """
    if not owner_id or not workflow_id:
        return None
    
    try:
        table = _dynamodb.Table(WORKFLOWS_TABLE)
        response = table.get_item(
            Key={'ownerId': owner_id, 'workflowId': workflow_id},
            ProjectionExpression='partition_map, total_segments, llm_segments_count, hitp_segments_count'
        )
        item = response.get('Item')
        if item and item.get('partition_map'):
            logger.info(f"Loaded pre-compiled partition_map from DB: {item.get('total_segments', 0)} segments")
            return item
        return None
    except Exception as e:
        logger.warning(f"Failed to load pre-compiled partition_map: {e}")
        return None


def lambda_handler(event, context):
    """
    Initializes state data using StateHydrator for strict "SFN Only Sees Pointers" strategy.
    
    Processing Steps:
    1. Resolve inputs (workflow_config, partition_map).
    2. Calculate strategy (distributed vs sequential).
    3. Initialize SmartStateBag.
    4. Dehydrate state (Offload large data to S3).
    5. Return safe payload to Step Functions.
    """
    logger.info("Initializing state data with StateHydrator strategy")
    
    # 1. State Hydrator Initialization
    bucket = os.environ.get('WORKFLOW_STATE_BUCKET')
    # Validate bucket early
    if not bucket:
        raw_bucket = os.environ.get('S3_BUCKET') or os.environ.get('SKELETON_S3_BUCKET')
        bucket = raw_bucket.strip() if raw_bucket else None
    
    hydrator = StateHydrator(bucket_name=bucket)
    
    # 2. Input Resolution
    # SFN ì…ë ¥ì´ {'input': {...}} í˜•íƒœì´ë©´ í•˜ìœ„ dict ì‚¬ìš©.
    # 'initial_state' í‚¤ë„ ì¸ì‹ (v3+ SFN ì…ë ¥ ìŠ¤í‚¤ë§ˆ).
    # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ event ì „ì²´ë¥¼ í´ë°± (ownerId/workflowIdê°€ top-levelì— ìˆëŠ” ë ˆê±°ì‹œ í˜¸ì¶œ í˜¸í™˜).
    # [M-03 Note] event ì „ì²´ í´ë°± ì‹œ orchestrator_selection ë“± SFN ë‚´ë¶€ í•„ë“œê°€ í¬í•¨ë  ìˆ˜ ìˆìŒ.
    #   í•´ë‹¹ í•„ë“œë“¤ì€ force_offload={'input'} ê²½ë¡œë¥¼ í†µí•´ S3ë¡œ ì˜¤í”„ë¡œë“œë˜ì–´ SFN í˜ì´ë¡œë“œì— ì”ë¥˜í•˜ì§€ ì•ŠìŒ.
    raw_input = event.get('input') or event.get('initial_state') or event
    if not isinstance(raw_input, dict):
        raw_input = {}
        
    # [FIX] When raw_input resolves to event['initial_state'], top-level fields
    # (workflowId, ownerId, test_workflow_config) are in event, not in raw_input.
    # Always fall back to event for fields that callers place at the top level.
    owner_id = raw_input.get('ownerId', "") or event.get('ownerId', "")
    workflow_id = raw_input.get('workflowId', "") or event.get('workflowId', "")
    current_time = int(time.time())

    # Generate execution_id for state tracking
    # Use idempotency_key if provided, otherwise generate unique ID
    execution_id = (raw_input.get('idempotency_key') or event.get('idempotency_key')
                    or raw_input.get('execution_id') or event.get('execution_id'))
    if not execution_id:
        import uuid
        execution_id = f"init-{workflow_id}-{int(time.time())}-{str(uuid.uuid4())[:8]}"

    # 2.1 Load Config & Partition Map
    # Priority: Input > DB (Precompiled) > Runtime Calc

    workflow_config = (raw_input.get('test_workflow_config') or raw_input.get('workflow_config')
                       or event.get('test_workflow_config') or event.get('workflow_config'))
    partition_map = raw_input.get('partition_map') or event.get('partition_map')
    
    # DB Loader Fallback
    if not workflow_config and workflow_id and owner_id:
        db_data = _load_workflow_config(owner_id, workflow_id)
        if db_data:
            workflow_config = db_data.get('config')
            # Only use DB partition map if not provided in input
            if not partition_map: 
                partition_map = db_data.get('partition_map')
    
    # Robustness: Ensure workflow_config is dict
    if not workflow_config:
        workflow_config = {}
        logger.warning("Proceeding with empty workflow_config")
    
    # Runtime Partitioning Fallback
    partition_result = None  # Initialize to prevent UnboundLocalError
    if not partition_map and _HAS_PARTITION:
        logger.info("Calculating partition_map at runtime...")
        try:
            partition_result = partition_workflow_advanced(workflow_config)
            partition_map = partition_result.get('partition_map', [])
        except Exception as e:
            logger.error(f"Partitioning failed: {e}")
            partition_map = []
            partition_result = {}  # Set empty dict on failure
            
    # Metadata Calculation
    total_segments = len(partition_map) if partition_map else 0
    llm_segments = sum(1 for seg in partition_map if seg.get('type') == 'llm') if partition_map else 0
    hitp_segments = sum(1 for seg in partition_map if seg.get('type') == 'hitp') if partition_map else 0
    
    # 3. Strategy Calculation
    try:
        distributed_strategy = _calculate_distributed_strategy(
            total_segments=total_segments,
            llm_segments=llm_segments,
            hitp_segments=hitp_segments,
            partition_map=partition_map or []
        )
    except Exception as e:
        logger.error(f"Strategy calculation failed: {e}")
        distributed_strategy = {
            "strategy": "SAFE",
            "max_concurrency": 1,
            "reason": "Strategy calculation failed"
        }
    
    # Concurrency Calculation
    max_concurrency = distributed_strategy.get('max_concurrency', 1)
    is_distributed_mode = distributed_strategy['strategy'] in ["MAP_REDUCE", "BATCHED"]
    
    # 4. [Phase 1] Merkle DAG Manifest Creation
    # ì´ì „: workflow_config + partition_map ì €ì¥ (ì „ì²´ StateBagì˜ 67%)
    # í˜„ì¬: Merkle manifest_id + hashë§Œ ì €ì¥ (ì°¸ì¡° í¬ì¸í„°)
    
    manifest_id = None
    manifest_hash = None
    config_hash = None
    
    if _HAS_VERSIONING:
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Merkle DAG ìƒì„±: ìµœì´ˆ 1íšŒ ì‹œë„ + ì‹¤íŒ¨ ì‹œ 1íšŒ ì¬ì‹œë„
        # ë¬´ê²°ì„± ì›ì¹™: ì¬ìƒì„±ë„ ì‹¤íŒ¨í•˜ë©´ ì›Œí¬í”Œë¡œìš°ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¢…ë£Œ.
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        _manifest_last_error = None
        for _attempt in range(1, 3):  # 1íšŒì°¨, 2íšŒì°¨
            try:
                # [BUG-INIT-01 FIX] gc_dlq_url ë° use_2pc ëˆ„ë½ â†’ create_manifest()ê°€
                # "GC DLQ URL is required for 2-Phase Commit" RuntimeErrorë¥¼ í•­ìƒ ë°œìƒì‹œí‚´.
                # segment_runner_service.pyì™€ ë™ì¼í•œ ì„¤ì •ì„ ì‚¬ìš©í•´ì•¼ í•¨.
                versioning_service = StateVersioningService(
                    dynamodb_table=MANIFESTS_TABLE,
                    s3_bucket=STATE_BUCKET,
                    use_2pc=True,
                    gc_dlq_url=os.environ.get('GC_DLQ_URL')
                )

                # segment_manifest ìƒì„± (ë¨¼ì € ê³„ì‚°)
                segment_manifest = []
                for idx, segment in enumerate(partition_map or []):
                    segment_manifest.append({
                        "segment_id": idx,
                        "segment_config": segment,
                        "execution_order": idx,
                        "dependencies": segment.get("dependencies", []),
                        "type": segment.get("type", "normal")
                    })

                # Merkle Manifest ìƒì„±
                manifest_pointer = versioning_service.create_manifest(
                    workflow_id=workflow_id,
                    workflow_config=workflow_config,
                    segment_manifest=segment_manifest,
                    parent_manifest_id=None  # ìµœì´ˆ ë²„ì „
                )

                manifest_id = manifest_pointer.manifest_id
                manifest_hash = manifest_pointer.manifest_hash
                config_hash = manifest_pointer.config_hash

                logger.info(
                    f"[Merkle DAG] Created manifest {manifest_id[:8]}... "
                    f"(attempt={_attempt}/2, hash={manifest_hash[:8]}..., {total_segments} segments)"
                )

                # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                # [Phase 8.1] Pre-flight Check: S3 Strong Consistency ê²€ì¦
                # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                # "ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì‹¤í–‰ì¡°ì°¨ í•˜ì§€ ì•ŠëŠ”ë‹¤"
                # - S3 Propagation Delay ë°©ì–´
                # - Fail-fast: 700ms ë‚´ ê²€ì¦ ì™„ë£Œ (3íšŒ ì¬ì‹œë„)
                # - Trust Chainì˜ ì²« ë²ˆì§¸ ê²€ì¦ ì§€ì 
                try:
                    from src.services.state.async_commit_service import get_async_commit_service

                    async_commit = get_async_commit_service()
                    manifest_s3_key = f"manifests/{manifest_id}.json"

                    logger.info(
                        f"[Pre-flight Check] Verifying manifest availability: "
                        f"{manifest_id[:8]}..."
                    )

                    commit_status = async_commit.verify_commit_with_retry(
                        execution_id=execution_id,
                        s3_bucket=STATE_BUCKET,
                        s3_key=manifest_s3_key,
                        redis_key=None  # Redis ì—†ì´ S3ë§Œ ê²€ì¦
                    )

                    if not commit_status.s3_available:
                        raise RuntimeError(
                            f"[System Fault] Manifest S3 availability verification failed "
                            f"after {commit_status.retry_count} attempts "
                            f"(total_wait={commit_status.total_wait_ms:.1f}ms). "
                            f"S3 Strong Consistency violation! "
                            f"Manifest: {manifest_id[:8]}..."
                        )

                    logger.info(
                        f"[Pre-flight Check] âœ… Manifest verified: {manifest_id[:8]}... "
                        f"(retries={commit_status.retry_count}, "
                        f"wait={commit_status.total_wait_ms:.1f}ms)"
                    )

                except ImportError:
                    logger.warning(
                        "[Pre-flight Check] AsyncCommitService not available, "
                        "skipping S3 verification (development mode)"
                    )
                except Exception as verify_error:
                    logger.error(
                        f"[Pre-flight Check] Manifest verification failed: {verify_error}",
                        exc_info=True
                    )
                    raise RuntimeError(
                        f"Pre-flight Check failed for manifest {manifest_id[:8]}...: "
                        f"{str(verify_error)}"
                    ) from verify_error

                break  # âœ… ìƒì„± + ê²€ì¦ ëª¨ë‘ ì„±ê³µ â†’ ë£¨í”„ íƒˆì¶œ

            except Exception as e:
                _manifest_last_error = e
                manifest_id = None
                if _attempt < 2:
                    logger.warning(
                        f"[Merkle DAG] Attempt {_attempt}/2 failed: {e}. "
                        f"Retrying manifest creation once more..."
                    )
                else:
                    logger.error(
                        f"[Merkle DAG] Attempt {_attempt}/2 failed: {e}",
                        exc_info=True
                    )

        if not manifest_id:
            raise RuntimeError(
                f"[System Fault] Merkle DAG manifest creation failed after 2 attempts. "
                f"Workflow state integrity cannot be guaranteed â€” aborting execution. "
                f"Last error: {_manifest_last_error}"
            ) from _manifest_last_error
    
    # State Bag Construction
    bag = SmartStateBag({}, hydrator=hydrator)
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # [Phase 2] Merkle DAG Content-Addressable Storage
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    if not manifest_id:
        raise RuntimeError(
            f"Merkle DAG manifest is required for workflow state integrity. "
            f"Diagnostics: _HAS_VERSIONING={_HAS_VERSIONING}, "
            f"workflow_config={'present' if workflow_config else 'missing'}, "
            f"partition_map={'present({} segs)'.format(len(partition_map)) if partition_map else 'empty/None'}. "
            f"Check StateVersioningService import and dependencies."
        )

    # âœ… Merkle DAG Mode: Content-Addressable Storage
    # - workflow_config/partition_map â†’ S3 ë¸”ë¡ìœ¼ë¡œ ì €ì¥ë¨
    # - StateBagì—ëŠ” manifest_id í¬ì¸í„°ë§Œ ì €ì¥ (93% í¬ê¸° ê°ì†Œ)
    # - segment_runnerëŠ” manifestì—ì„œ segment_config ë¡œë“œ
    bag['manifest_id'] = manifest_id
    bag['manifest_hash'] = manifest_hash
    bag['config_hash'] = config_hash

    logger.info(
        f"[Merkle DAG] State storage optimized: "
        f"manifest_id={manifest_id[:8]}..., "
        f"hash={manifest_hash[:8]}..., "
        f"StateBag reduction: ~93%"
    )
    
    bag['current_state'] = workflow_config.get('initial_state', {})
    
    # Metadata
    bag['ownerId'] = owner_id
    bag['workflowId'] = workflow_id
    bag['idempotency_key'] = (raw_input.get('idempotency_key', "")
                              or event.get('idempotency_key', ""))
    bag['quota_reservation_id'] = (raw_input.get('quota_reservation_id', "")
                                   or event.get('quota_reservation_id', ""))
    bag['segment_to_run'] = 0
    bag['total_segments'] = max(1, total_segments)
    bag['distributed_mode'] = is_distributed_mode
    bag['max_concurrency'] = int(max_concurrency)
    bag['llm_segments'] = llm_segments
    bag['hitp_segments'] = hitp_segments
    # [M-02 FIX] execution_idë¥¼ bagì— ëª…ì‹œì ìœ¼ë¡œ ì €ì¥.
    # segment_runner_serviceì˜ save_state_delta ê°€ event.get('execution_id', 'unknown')ìœ¼ë¡œ
    # ì¡°íšŒí•˜ë¯€ë¡œ, bagì— ì—†ìœ¼ë©´ í•­ìƒ 'unknown'ì´ ê¸°ë¡ë¨.
    bag['execution_id'] = execution_id
    
    # [State Bag] Enforce Input Encapsulation
    # Embed raw input into the bag so it can be offloaded if large
    bag['input'] = raw_input
    bag['loop_counter'] = 0
    
    # ğŸ›¡ï¸ [Dynamic Loop Limit] Segment-based weighted execution counting
    # Formula: (TotalSegments + LoopWeightedSegments) Ã— 1.2 + 20
    # where LoopWeightedSegments = Î£(SegCount Ã— (MaxIter-1)) + 2Ã—ForEachCount
    # 
    # Example 1 - for_each with 30 iterations:
    #   - 3 base segments (prep, for_each, validator)
    #   - for_each adds 2 (parallel_group + aggregator)
    #   - Estimated: (3 + 2) Ã— 1.2 + 20 = 26
    #
    # Example 2 - Sequential loop with 5 iterations, 2 internal segments:
    #   - 4 base segments (prep, loop, seg1, seg2, validator)
    #   - loop adds 2 Ã— (5-1) = 8
    #   - Estimated: (4 + 8) Ã— 1.2 + 20 = 34.4 â‰ˆ 35
    #
    # This prevents LoopLimitExceeded errors while maintaining safety bounds.
    
    # Extract loop analysis from partition_result (if available)
    if partition_result:
        estimated_executions = partition_result.get("estimated_executions", total_segments)
        loop_analysis = partition_result.get("loop_analysis", {})
    else:
        # Fallback when partitioning failed
        estimated_executions = total_segments
        loop_analysis = {}
    
    # Apply safety margin: 20% of estimate or minimum 20
    safety_margin = max(int(estimated_executions * 0.2), 20)
    default_loop_limit = estimated_executions + safety_margin
    
    # Branch loop limit: 50% of main limit or minimum 50
    default_branch_limit = max(int(default_loop_limit * 0.5), 50)
    
    # User can override via workflow_config, otherwise use dynamic calculation
    bag['max_loop_iterations'] = workflow_config.get('max_loop_iterations', default_loop_limit)
    bag['max_branch_iterations'] = workflow_config.get('max_branch_iterations', default_branch_limit)
    
    logger.info(
        f"[Dynamic Loop Limit] "
        f"estimated_executions={estimated_executions}, "
        f"safety_margin={safety_margin}, "
        f"loop_limit={bag['max_loop_iterations']}, "
        f"branch_limit={bag['max_branch_iterations']}, "
        f"loop_nodes={loop_analysis.get('loop_count', 0)}"
    )
    
    bag['start_time'] = current_time
    bag['last_update_time'] = current_time
    bag['state_durations'] = {}
    
    # [v3.23] ì‹œë®¬ë ˆì´í„° í”Œë˜ê·¸ë¥¼ bag ìµœìƒìœ„ë¡œ ë³µì‚¬
    # store_task_token.pyê°€ bag.get('AUTO_RESUME_HITP') ì¡°íšŒ
    auto_resume = raw_input.get('AUTO_RESUME_HITP') or event.get('AUTO_RESUME_HITP')
    if auto_resume:
        bag['AUTO_RESUME_HITP'] = auto_resume
    mock_mode = raw_input.get('MOCK_MODE') or event.get('MOCK_MODE')
    if mock_mode:
        bag['MOCK_MODE'] = mock_mode
    
    # 5. [Phase 1/2] Segment Manifest Strategy
    # Merkle DAG ëª¨ë“œ: segment_manifestëŠ” ì´ë¯¸ S3ì— ì €ì¥ë¨ (StateVersioningService)
    # Legacy ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ S3ì— ì—…ë¡œë“œ
    
    manifest_s3_path = ""
    
    if manifest_id:
        # Merkle DAG: ManifestëŠ” ì´ë¯¸ Content-Addressable ë¸”ë¡ìœ¼ë¡œ ì €ì¥ë¨
        # manifest_idë¡œ segment_manifest ì ‘ê·¼ ê°€ëŠ¥
        manifest_s3_path = f"s3://{STATE_BUCKET}/manifests/{manifest_id}.json"
        logger.info(f"[Merkle DAG] Using manifest: {manifest_s3_path}")
        
    elif partition_map and hydrator.s3_client and bucket:
        # Legacy ëª¨ë“œ: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ segment_manifest ìƒì„±/ì €ì¥
        segment_manifest = []
        for idx, segment in enumerate(partition_map or []):
            segment_manifest.append({
                "segment_id": idx,
                "segment_config": segment,
                "execution_order": idx,
                "dependencies": segment.get("dependencies", []),
                "type": segment.get("type", "normal")
            })
        
        # Legacy manifest upload
        try:
            manifest_key = f"workflow-manifests/{owner_id}/{workflow_id}/segment_manifest.json"
            hydrator.s3_client.put_object(
                Bucket=bucket,
                Key=manifest_key,
                Body=json.dumps(segment_manifest, default=str),
                ContentType='application/json'
            )
            manifest_s3_path = f"s3://{bucket}/{manifest_key}"
            logger.info(f"[Legacy] Segment manifest uploaded: {manifest_s3_path}")
        except Exception as e:
            logger.warning(f"Failed to upload manifest: {e}")
            
    # Create Pointers List for ItemProcessor (Map stateì—ì„œ ì‚¬ìš©)
    segment_manifest_pointers = []
    if manifest_s3_path:
        # S3 ê²½ë¡œë§Œ ì°¸ì¡°í•˜ëŠ” ê²½ëŸ‰ í¬ì¸í„°
        # Merkle DAG ëª¨ë“œì—ì„œëŠ” segment_indexë¡œ S3 Select ì¿¼ë¦¬
        if manifest_id:
            # Merkle DAG: manifest_id + segment_indexë¡œ ë¸”ë¡ ì ‘ê·¼
            for idx in range(total_segments):
                segment_manifest_pointers.append({
                    'segment_index': idx,
                    'segment_id': idx,
                    'segment_type': 'computed',  # Merkle DAGì—ì„œ íƒ€ì… ì¶”ë¡ 
                    'manifest_id': manifest_id,
                    'manifest_s3_path': manifest_s3_path,
                    'total_segments': total_segments
                })
        else:
            # Legacy: ê¸°ì¡´ ë°©ì‹
            segment_manifest = []
            if partition_map:
                for idx, segment in enumerate(partition_map or []):
                    segment_manifest.append({
                        "segment_id": idx,
                        "segment_config": segment,
                        "execution_order": idx,
                        "dependencies": segment.get("dependencies", []),
                        "type": segment.get("type", "normal")
                    })
            
            for idx, seg in enumerate(segment_manifest):
                segment_manifest_pointers.append({
                    'segment_index': idx,
                    'segment_id': seg.get('segment_id', idx),
                    'segment_type': seg.get('type', 'normal'),
                    'manifest_s3_path': manifest_s3_path,
                    'total_segments': len(segment_manifest)
                })
    else:
        # Fallback to inline (only if S3 failed/missing)
        if partition_map:
            segment_manifest_pointers = []
            for idx, segment in enumerate(partition_map or []):
                segment_manifest_pointers.append({
                    "segment_id": idx,
                    "segment_config": segment,
                    "execution_order": idx,
                    "dependencies": segment.get("dependencies", []),
                    "type": segment.get("type", "normal")
                })

    # Add to bag
    # [C-01 FIX] segment_manifest_pointersëŠ” ê²½ëŸ‰ í¬ì¸í„° ë°°ì—´ (~200B Ã— n ì„¸ê·¸ë¨¼íŠ¸).
    # ASL v3 Map stateì˜ ItemsPath: "$.state_data.bag.segment_manifest" ê°€ ì´ ë°°ì—´ì„ ì°¸ì¡°.
    # í¬ì¸í„°ê°€ ì—†ìœ¼ë©´ BATCHED/MAP_REDUCE ëª¨ë“œì—ì„œ Map stateê°€ 0íšŒ ì‹¤í–‰ë˜ì–´ ì „ë©´ ë¶ˆëŠ¥.
    # ì „ì²´ segment_configê°€ ì•„ë‹Œ í¬ì¸í„°ë§Œ ë‹´ìœ¼ë¯€ë¡œ 256KB SFN í•œë„ ë‚´ì— ìœ ì§€ë¨.
    bag['segment_manifest'] = segment_manifest_pointers
    bag['segment_manifest_s3_path'] = manifest_s3_path

    # 6. Dehydrate Final Payload
    # Force offload large fields to Ensure "SFN Only Sees Pointers"
    # Added 'input' to forced offload list to prevent Zombie Data
    # Note: 'segment_manifest' is intentionally NOT force-offloaded â€” it is a
    #   lightweight pointer array required inline by ASL Map state ItemsPath.
    force_offload = {'workflow_config', 'partition_map', 'current_state', 'input'}
    
    payload = hydrator.dehydrate(
        state=bag,
        owner_id=owner_id,
        workflow_id=workflow_id,
        execution_id=execution_id,
        return_delta=False,
        force_offload_fields=force_offload
    )
    
    # 7. Post-Processing & Compatibility
    # [C-01] segment_manifest_pointersëŠ” bag['segment_manifest']ì— ì´ë¯¸ ì €ì¥ë¨.
    # dehydrate() ì´í›„ì—ë„ í¬ì¸í„° ë°°ì—´ì´ payloadì— ìœ ì§€ë˜ì–´ì•¼ ASL Map stateê°€ ë™ì‘í•¨.
    
    # Ensure compatibility aliases exist if fields were offloaded
    # (ASL might reference _s3_path fields directly)
    for field in ['workflow_config', 'partition_map', 'current_state', 'input']:
        val = payload.get(field)
        if isinstance(val, dict) and val.get('__s3_offloaded'):
            payload[f"{field}_s3_path"] = val.get('__s3_path')
            
    # Explicitly set state_s3_path alias for current_state
    if payload.get('current_state_s3_path'):
        payload['state_s3_path'] = payload['current_state_s3_path']
        
    # [No Data at Root] REMOVED inline input copy to prevent payload explosion
    # payload['input'] = raw_input - DELETED
    
    logger.info(f"âœ… State Initialization Complete. Keys: {list(payload.keys())}")
    
    # ï¿½ [v3.13] Kernel Protocol - The Great Seal Pattern
    # seal_state_bagì„ ì‚¬ìš©í•˜ì—¬ í‘œì¤€ ì‘ë‹µ í¬ë§· ìƒì„±
    try:
        from src.common.kernel_protocol import seal_state_bag
        _HAS_KERNEL_PROTOCOL = True
    except ImportError:
        try:
            from common.kernel_protocol import seal_state_bag
            _HAS_KERNEL_PROTOCOL = True
        except ImportError:
            _HAS_KERNEL_PROTOCOL = False
    
    if _HAS_KERNEL_PROTOCOL:
        logger.info("ğŸ’ [v3.13] Using Kernel Protocol seal_state_bag")
        
        # Ensure idempotency_key is available
        idempotency_key = bag.get('idempotency_key') or raw_input.get('idempotency_key') or "unknown"
        
        # seal_state_bag: USC í˜¸ì¶œ + í‘œì¤€ í¬ë§· ë°˜í™˜
        response_data = seal_state_bag(
            base_state={},  # ë¹ˆ ìƒíƒœì—ì„œ ì‹œì‘
            result_delta=payload,
            action='init',
            context={
                'execution_id': idempotency_key,
                'idempotency_key': idempotency_key
            }
        )
        
        logger.info(f"âœ… [Kernel Protocol] Init complete: next_action={response_data.get('next_action')}")
    elif _HAS_USC and universal_sync_core:
        # Fallback to direct USC (if kernel_protocol not available)
        logger.warning("âš ï¸ Kernel Protocol not available, using direct USC")
        
        idempotency_key = bag.get('idempotency_key') or raw_input.get('idempotency_key') or "unknown"
        
        usc_result = universal_sync_core(
            base_state={},
            new_result=payload,
            context={
                'action': 'init',
                'execution_id': idempotency_key,
                'idempotency_key': idempotency_key
            }
        )
        
        # í‘œì¤€ í¬ë§·ìœ¼ë¡œ ë°˜í™˜
        response_data = {
            'state_data': usc_result.get('state_data', {}),
            'next_action': usc_result.get('next_action', 'STARTED')
        }
    else:
        # USCë„ ì—†ëŠ” ê²½ìš° í´ë°±
        logger.warning("âš ï¸ Universal Sync Core not available, using legacy initialization")
        
        payload['segment_to_run'] = 0
        payload['loop_counter'] = 0
        payload['state_history'] = []
        payload['last_update_time'] = current_time
        
        response_data = {
            'state_data': payload,
            'next_action': 'STARTED'
        }
    
    # ìµœì¢… í¬ê¸° ê²€ì¦
    response_json = json.dumps(response_data, default=str, ensure_ascii=False)
    response_size_kb = len(response_json.encode('utf-8')) / 1024
    
    logger.info(f"âœ… InitializeStateData response: {response_size_kb:.1f}KB")
    
    if response_size_kb > 250:
        logger.error(
            f"ğŸš¨ CRITICAL: Response exceeds 250KB ({response_size_kb:.1f}KB)! "
            f"Step Functions will reject with DataLimitExceeded."
        )
    elif response_size_kb > 200:
        logger.warning(f"âš ï¸ Response is {response_size_kb:.1f}KB (>200KB). Close to limit.")
    
    return response_data

